{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/notebooks_0114_working/modeling_vip5.py\n"
     ]
    }
   ],
   "source": [
    "import modeling_vip5\n",
    "print(modeling_vip5.__file__)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import shutil\n",
    "import time\n",
    "from packaging import version\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gzip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from src.param import parse_args\n",
    "from src.utils import LossMeter\n",
    "from src.dist_utils import reduce_dict\n",
    "from transformers import T5Tokenizer\n",
    "from src.tokenization import P5Tokenizer\n",
    "from src.model import VIP5Tuning\n",
    "\n",
    "_use_native_amp = False\n",
    "_use_apex = False\n",
    "\n",
    "# Check if Pytorch version >= 1.6 to switch between Native AMP and Apex\n",
    "if version.parse(torch.__version__) < version.parse(\"1.6\"):\n",
    "    from transormers.file_utils import is_apex_available\n",
    "    if is_apex_available():\n",
    "        from apex import amp\n",
    "    _use_apex = True\n",
    "else:\n",
    "    _use_native_amp = True\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "from src.trainer_base import TrainerBase\n",
    "\n",
    "import pickle\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def save_pickle(data, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "import json\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def ReadLineFromFile(path):\n",
    "    lines = []\n",
    "    with open(path,'r') as fd:\n",
    "        for line in fd:\n",
    "            lines.append(line.rstrip('\\n'))\n",
    "    return lines\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sequential_loss', 'direct_loss', 'explanation_loss']\n",
      "Process Launching at GPU 0\n",
      "{'distributed': False, 'multiGPU': True, 'fp16': True, 'split': 'toys', 'train': 'toys', 'valid': 'toys', 'test': 'toys', 'batch_size': 16, 'optim': 'adamw', 'warmup_ratio': 0.1, 'lr': 0.001, 'num_workers': 4, 'clip_grad_norm': 5.0, 'losses': 'sequential,direct,explanation', 'backbone': 't5-small', 'image_feature_type': 'vitb32', 'image_feature_size_ratio': 2, 'use_adapter': True, 'reduction_factor': 8, 'use_single_adapter': True, 'use_vis_layer_norm': True, 'add_adapter_cross_attn': True, 'use_lm_head_adapter': True, 'epoch': 20, 'local_rank': 0, 'comment': '', 'train_topk': -1, 'valid_topk': -1, 'dropout': 0.1, 'tokenizer': 'p5', 'max_text_length': 1024, 'gen_max_length': 64, 'do_lower_case': False, 'weight_decay': 0.01, 'adam_eps': 1e-06, 'gradient_accumulation_steps': 1, 'seed': 2022, 'whole_word_embed': True, 'category_embed': True, 'world_size': 4, 'LOSSES_NAME': ['sequential_loss', 'direct_loss', 'explanation_loss', 'total_loss'], 'gpu': 0, 'rank': 0, 'run_name': 'Mar04_17-28_GPU4_toys_t5-small_sequentialdirectexplanation'}\n"
     ]
    }
   ],
   "source": [
    "class DotDict(dict):\n",
    "    def __init__(self, **kwds):\n",
    "        self.update(kwds)\n",
    "        self.__dict__ = self\n",
    "        \n",
    "args = DotDict()\n",
    "\n",
    "args.distributed = False\n",
    "args.multiGPU = True\n",
    "args.fp16 = True\n",
    "#####################\n",
    "args.split = \"toys\"\n",
    "#####################\n",
    "args.train = args.split\n",
    "args.valid = args.split\n",
    "args.test = args.split\n",
    "args.batch_size = 16\n",
    "args.optim = 'adamw' \n",
    "args.warmup_ratio = 0.1\n",
    "args.lr = 1e-3\n",
    "args.num_workers = 4\n",
    "args.clip_grad_norm = 5.0\n",
    "args.losses = 'sequential,direct,explanation'\n",
    "args.backbone = 't5-small'\n",
    "#####################\n",
    "args.image_feature_type = 'vitb32'\n",
    "args.image_feature_size_ratio = 2\n",
    "args.use_adapter = True\n",
    "args.reduction_factor = 8\n",
    "args.use_single_adapter = True\n",
    "args.use_vis_layer_norm = True\n",
    "args.add_adapter_cross_attn = True\n",
    "args.use_lm_head_adapter = True\n",
    "#####################\n",
    "args.epoch = 20\n",
    "args.local_rank = 0\n",
    "\n",
    "args.comment = ''\n",
    "args.train_topk = -1\n",
    "args.valid_topk = -1\n",
    "args.dropout = 0.1\n",
    "\n",
    "args.tokenizer = 'p5'\n",
    "args.max_text_length = 1024\n",
    "args.gen_max_length = 64\n",
    "args.do_lower_case = False\n",
    "\n",
    "args.weight_decay = 0.01\n",
    "args.adam_eps = 1e-6\n",
    "args.gradient_accumulation_steps = 1\n",
    "\n",
    "'''\n",
    "Set seeds\n",
    "'''\n",
    "args.seed = 2022\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "'''\n",
    "Whole word embedding & Category embedding\n",
    "'''\n",
    "args.whole_word_embed = True\n",
    "args.category_embed = True\n",
    "\n",
    "cudnn.benchmark = True\n",
    "ngpus_per_node = torch.cuda.device_count()\n",
    "args.world_size = ngpus_per_node\n",
    "\n",
    "LOSSES_NAME = [f'{name}_loss' for name in args.losses.split(',')]\n",
    "if args.local_rank in [0, -1]:\n",
    "    print(LOSSES_NAME)\n",
    "LOSSES_NAME.append('total_loss') # total loss\n",
    "\n",
    "args.LOSSES_NAME = LOSSES_NAME\n",
    "\n",
    "#####################\n",
    "gpu = 0 # Change GPU ID\n",
    "#####################\n",
    "args.gpu = gpu\n",
    "args.rank = gpu\n",
    "print(f'Process Launching at GPU {gpu}')\n",
    "\n",
    "torch.cuda.set_device('cuda:{}'.format(gpu))\n",
    "\n",
    "comments = []\n",
    "dsets = []\n",
    "if 'toys' in args.train:\n",
    "    dsets.append('toys')\n",
    "if 'beauty' in args.train:\n",
    "    dsets.append('beauty')\n",
    "if 'sports' in args.train:\n",
    "    dsets.append('sports')\n",
    "if 'clothing' in args.train:\n",
    "    dsets.append('clothing')\n",
    "comments.append(''.join(dsets))\n",
    "if args.backbone:\n",
    "    comments.append(args.backbone)\n",
    "comments.append(''.join(args.losses.split(',')))\n",
    "if args.comment != '':\n",
    "    comments.append(args.comment)\n",
    "comment = '_'.join(comments)\n",
    "\n",
    "from datetime import datetime\n",
    "current_time = datetime.now().strftime('%b%d_%H-%M')\n",
    "\n",
    "if args.local_rank in [0, -1]:\n",
    "    run_name = f'{current_time}_GPU{args.world_size}'\n",
    "    if len(comments) > 0:\n",
    "        run_name += f'_{comment}'\n",
    "    args.run_name = run_name\n",
    "    print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_feature_dim_dict = {\n",
    "    'vitb32': 512,\n",
    "    'vitb16': 512,\n",
    "    'vitl14': 768,\n",
    "    'rn50': 1024,\n",
    "    'rn101': 512\n",
    "}\n",
    "\n",
    "def create_config(args):\n",
    "    from transformers import T5Config\n",
    "    from adapters import (\n",
    "        AdapterController,\n",
    "        OutputParallelAdapterLayer,\n",
    "        AdapterConfig\n",
    "    )\n",
    "\n",
    "    if 't5' in args.backbone:\n",
    "        config_class = T5Config\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    config = config_class.from_pretrained(args.backbone)\n",
    "\n",
    "    for k, v in vars(args).items():\n",
    "        setattr(config, k, v)\n",
    "\n",
    "    config.non_linearity = \"relu\"\n",
    "\n",
    "    config.feat_dim = image_feature_dim_dict[args.image_feature_type]\n",
    "    config.n_vis_tokens = args.image_feature_size_ratio\n",
    "    config.use_vis_layer_norm = args.use_vis_layer_norm\n",
    "    config.reduction_factor = args.reduction_factor\n",
    "\n",
    "    config.use_adapter = args.use_adapter\n",
    "    config.add_adapter_cross_attn = args.add_adapter_cross_attn\n",
    "    config.use_lm_head_adapter = args.use_lm_head_adapter\n",
    "    config.use_single_adapter = args.use_single_adapter\n",
    "\n",
    "    config.dropout_rate = args.dropout\n",
    "    config.dropout = args.dropout\n",
    "    config.attention_dropout = args.dropout\n",
    "    config.activation_dropout = args.dropout\n",
    "\n",
    "    config.losses = args.losses\n",
    "\n",
    "    tasks = re.split(\"[, ]+\", args.losses) # tranform to list\n",
    "\n",
    "    if args.use_adapter:\n",
    "        CONFIG_CLASS = AdapterConfig\n",
    "\n",
    "        config.adapter_config = CONFIG_CLASS()\n",
    "        config.adapter_config.tasks = tasks\n",
    "        config.adapter_config.d_model = config.d_model # for adapter\n",
    "        config.adapter_config.use_single_adapter = args.use_single_adapter\n",
    "        config.adapter_config.reduction_factor = args.reduction_factor\n",
    "        config.adapter_config.track_z = False\n",
    "    else:\n",
    "        config.adapter_config = None\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def create_tokenizer(args):\n",
    "    from transformers import T5Tokenizer\n",
    "    from src.tokenization import P5Tokenizer\n",
    "\n",
    "    if 'p5' in args.tokenizer:\n",
    "        tokenizer_class = P5Tokenizer\n",
    "\n",
    "    tokenizer_name = args.backbone\n",
    "    \n",
    "    tokenizer = tokenizer_class.from_pretrained(\n",
    "        tokenizer_name,\n",
    "        max_length=args.max_text_length,\n",
    "        do_lower_case=args.do_lower_case,\n",
    "    )\n",
    "\n",
    "    print(tokenizer_class, tokenizer_name)\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def create_model(model_class, config=None):\n",
    "    print(f'Building Model at GPU {args.gpu}')\n",
    "\n",
    "    model_name = args.backbone\n",
    "\n",
    "    model = model_class.from_pretrained(\n",
    "        model_name,\n",
    "        config=config\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'src.tokenization.P5Tokenizer'> t5-small\n",
      "Building Model at GPU 0\n",
      "JointEncoder initialized successfully.\n",
      "T5Stack initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VIP5Tuning were not initialized from the model checkpoint at t5-small and are newly initialized: ['decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.visual_embedding.feat_embedding.0.model.2.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.visual_embedding.feat_embedding.1.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.whole_word_embeddings.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.category_embeddings.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'output_adapter.adapter.down_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.visual_embedding.feat_embedding.0.model.2.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.visual_embedding.feat_embedding.0.model.0.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'output_adapter.adapter.up_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.visual_embedding.feat_embedding.0.model.0.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'output_adapter.adapter.up_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'output_adapter.adapter.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.up_sampler.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_head initialized successfully.\n",
      "OutputParallelAdapterLayer initialized successfully.\n",
      "AdapterConfig: AdapterConfig(add_layer_norm_before_adapter=False, add_layer_norm_after_adapter=False, non_linearity='gelu_new', reduction_factor=8)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 47.50 GiB of which 23.06 MiB is free. Process 1795146 has 858.00 MiB memory in use. Process 2248567 has 5.78 GiB memory in use. Process 3031620 has 25.18 GiB memory in use. Including non-PyTorch memory, this process has 15.66 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 6.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp5\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39mtokenizer:\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize_token_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m tokenizer\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/transformers/modeling_utils.py:1303\u001b[0m, in \u001b[0;36mPreTrainedModel.resize_token_embeddings\u001b[0;34m(self, new_num_tokens)\u001b[0m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresize_token_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, new_num_tokens: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding:\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m \u001b[38;5;124;03m    Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;124;03m        `torch.nn.Embedding`: Pointer to the input tokens Embeddings Module of the model.\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1303\u001b[0m     model_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resize_token_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_num_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_num_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model_embeds\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/transformers/modeling_utils.py:1318\u001b[0m, in \u001b[0;36mPreTrainedModel._resize_token_embeddings\u001b[0;34m(self, new_num_tokens)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_resize_token_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, new_num_tokens):\n\u001b[1;32m   1317\u001b[0m     old_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_input_embeddings()\n\u001b[0;32m-> 1318\u001b[0m     new_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_resized_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_num_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_input_embeddings(new_embeddings)\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;66;03m# if word embeddings are not tied, make sure that lm head is resized as well\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/transformers/modeling_utils.py:1373\u001b[0m, in \u001b[0;36mPreTrainedModel._get_resized_embeddings\u001b[0;34m(self, old_embeddings, new_num_tokens)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;66;03m# Build new embeddings\u001b[39;00m\n\u001b[1;32m   1372\u001b[0m new_embeddings \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(new_num_tokens, old_embedding_dim)\n\u001b[0;32m-> 1373\u001b[0m \u001b[43mnew_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mold_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;66;03m# initialize all new embeddings (in particular added tokens)\u001b[39;00m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_weights(new_embeddings)\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 47.50 GiB of which 23.06 MiB is free. Process 1795146 has 858.00 MiB memory in use. Process 2248567 has 5.78 GiB memory in use. Process 3031620 has 25.18 GiB memory in use. Including non-PyTorch memory, this process has 15.66 GiB memory in use. Of the allocated memory 15.16 GiB is allocated by PyTorch, and 6.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "config = create_config(args)\n",
    "\n",
    "if args.tokenizer is None:\n",
    "    args.tokenizer = args.backbone\n",
    "    \n",
    "tokenizer = create_tokenizer(args)\n",
    "\n",
    "model_class = VIP5Tuning\n",
    "model = create_model(model_class, config)\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "if 'p5' in args.tokenizer:\n",
    "    model.resize_token_embeddings(tokenizer.vocab_size)\n",
    "    \n",
    "model.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['output_adapter.adapter.down_sampler.weight', 'output_adapter.adapter.down_sampler.bias', 'output_adapter.adapter.up_sampler.weight', 'output_adapter.adapter.up_sampler.bias'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "# args.load = \"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/snap/toys-vitb32-2-8-20/BEST_EVAL_LOSS.pth\"  \n",
    "args.load = \"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/snap/toys/0304/toys-vitb32-2-8-20/BEST_EVAL_LOSS.pth\"  \n",
    "\n",
    "# Load Checkpoint\n",
    "from src.utils import load_state_dict, LossMeter, set_global_logging_level\n",
    "from pprint import pprint\n",
    "\n",
    "def load_checkpoint(ckpt_path):\n",
    "    state_dict = load_state_dict(ckpt_path, 'cpu')\n",
    "    results = model.load_state_dict(state_dict, strict=False)\n",
    "    pprint(results)\n",
    "\n",
    "ckpt_path = args.load\n",
    "load_checkpoint(ckpt_path)\n",
    "\n",
    "from src.all_templates import all_tasks as task_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splits = load_pickle('../data/{}/rating_splits_augmented.pkl'.format(args.split))\n",
    "test_review_data = data_splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16759"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_review_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reviewerID': 'A5K3CK2PWYQ7O',\n",
       " 'asin': 'B00F4CFEYG',\n",
       " 'reviewerName': 'Ellie \"mittbooks\"',\n",
       " 'helpful': [0, 0],\n",
       " 'reviewText': \"I've found the Melissa & Doug brand to be overall good, although there are occasional negatives.  This is definitely one of the toys we'll mark a &#34;winner.&#34;  The vacuum comes in two pieces that require minimal assembly (the long handle and the base need to be put together - no tools required).  The height is perfect for our two year old who is 3 feet tall.  The top part moves at about a 45 degree angle to facilitate little people pushing the vacuum.  I'm not sure how long the six wooden pieces of &#34;trash&#34; will last.  Although not tiny, they would be easy to lose.  The vacuum does a good job of picking them up easily and there is a small area in the back of the base to take them out again.  There is also a rotating knob on the front of the handle that makes a good clicking noise when it moves.  Our son is truly enjoying this this toy and the overall quality is excellent.  Definitely a good addition to the toy room.\",\n",
       " 'overall': 4.0,\n",
       " 'summary': '... found the Melissa & Doug brand to be overall good, although there are occasional negatives',\n",
       " 'unixReviewTime': 1403913600,\n",
       " 'reviewTime': '06 28, 2014',\n",
       " 'explanation': \"I've found the Melissa & Doug brand to be overall good\",\n",
       " 'feature': 'overall'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_review_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19412\n",
      "11924\n"
     ]
    }
   ],
   "source": [
    "data_maps = load_json(os.path.join('../data', args.split, 'datamaps.json'))\n",
    "print(len(data_maps['user2id'])) # number of users\n",
    "print(len(data_maps['item2id'])) # number of items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test P5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "from src.data import get_loader\n",
    "from evaluate.utils import rouge_score, bleu_score, unique_sentence_percent, root_mean_square_error, mean_absolute_error, feature_detect, feature_matching_ratio, feature_coverage_ratio, feature_diversity\n",
    "from evaluate.metrics4rec import evaluate_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation - Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/notebooks_0114_working/data/toys/exp_splits.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.abspath(\"data/toys/exp_splits.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 importlib.reload 强制刷新 确保 Python 加载到你新修改过的 get_loader（带 data_root 参数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.data\n",
    "importlib.reload(src.data)\n",
    "from src.data import get_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sources:  ['toys']\n",
      "compute_datum_info\n",
      "646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "646it [01:08,  9.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1 12.4267\n",
      "BLEU-4  7.8793\n",
      "rouge_1/f_score 27.2064\n",
      "rouge_1/r_score 22.2341\n",
      "rouge_1/p_score 44.9808\n",
      "rouge_2/f_score  8.7732\n",
      "rouge_2/r_score  7.3634\n",
      "rouge_2/p_score 15.3068\n",
      "rouge_l/f_score 20.7347\n",
      "rouge_l/r_score 20.4198\n",
      "rouge_l/p_score 41.7443\n"
     ]
    }
   ],
   "source": [
    "test_task_list = {'explanation': ['C-12']\n",
    "}\n",
    "test_sample_numbers = {'sequential': (1, 1), 'direct': (1, 1), 'explanation': 1}\n",
    "\n",
    "zeroshot_test_loader = get_loader(\n",
    "        args,\n",
    "        test_task_list,\n",
    "        test_sample_numbers,\n",
    "        split=args.test, \n",
    "        mode='test', \n",
    "        batch_size=args.batch_size,\n",
    "        workers=args.num_workers,\n",
    "        distributed=args.distributed,\n",
    "        data_root=\"../data\",        # 显式指明数据在上级目录\n",
    "        feature_root=\"../features\"  # 依此类推\n",
    ")\n",
    "print(len(zeroshot_test_loader))\n",
    "\n",
    "tokens_predict = []\n",
    "tokens_test = []\n",
    "for i, batch in tqdm(enumerate(zeroshot_test_loader)):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        tokens_predict.extend(results) \n",
    "        tokens_test.extend(batch['target_text'])\n",
    "        \n",
    "BLEU1 = bleu_score(tokens_test, tokens_predict, n_gram=1, smooth=False)\n",
    "print('BLEU-1 {:7.4f}'.format(BLEU1))\n",
    "BLEU4 = bleu_score(tokens_test, tokens_predict, n_gram=4, smooth=False)\n",
    "print('BLEU-4 {:7.4f}'.format(BLEU4))\n",
    "\n",
    "ROUGE = rouge_score(tokens_test, tokens_predict)  # a dictionary\n",
    "for (k, v) in ROUGE.items():\n",
    "    print('{} {:7.4f}'.format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sources:  ['toys']\n",
      "compute_datum_info\n",
      "646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "646it [01:08,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1  2.9930\n",
      "BLEU-4  1.0293\n",
      "rouge_1/f_score  6.7310\n",
      "rouge_1/r_score  5.1061\n",
      "rouge_1/p_score 13.6365\n",
      "rouge_2/f_score  1.2484\n",
      "rouge_2/r_score  1.0052\n",
      "rouge_2/p_score  2.3944\n",
      "rouge_l/f_score  5.0368\n",
      "rouge_l/r_score  4.8688\n",
      "rouge_l/p_score 13.2084\n"
     ]
    }
   ],
   "source": [
    "test_task_list = {'explanation': ['C-3']\n",
    "}\n",
    "test_sample_numbers = {'sequential': (1, 1), 'direct': (1, 1), 'explanation': 1}\n",
    "\n",
    "zeroshot_test_loader = get_loader(\n",
    "        args,\n",
    "        test_task_list,\n",
    "        test_sample_numbers,\n",
    "        split=args.test, \n",
    "        mode='test', \n",
    "        batch_size=args.batch_size,\n",
    "        workers=args.num_workers,\n",
    "        distributed=args.distributed,\n",
    "        data_root=\"../data\",        # 显式指明数据在上级目录\n",
    "        feature_root=\"../features\"  # 依此类推\n",
    ")\n",
    "print(len(zeroshot_test_loader))\n",
    "\n",
    "tokens_predict = []\n",
    "tokens_test = []\n",
    "for i, batch in tqdm(enumerate(zeroshot_test_loader)):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        tokens_predict.extend(results) \n",
    "        tokens_test.extend(batch['target_text'])\n",
    "        \n",
    "BLEU1 = bleu_score(tokens_test, tokens_predict, n_gram=1, smooth=False)\n",
    "print('BLEU-1 {:7.4f}'.format(BLEU1))\n",
    "BLEU4 = bleu_score(tokens_test, tokens_predict, n_gram=4, smooth=False)\n",
    "print('BLEU-4 {:7.4f}'.format(BLEU4))\n",
    "\n",
    "ROUGE = rouge_score(tokens_test, tokens_predict)  # a dictionary\n",
    "for (k, v) in ROUGE.items():\n",
    "    print('{} {:7.4f}'.format(k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Evaluation - Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sources:  ['toys']\n",
      "compute_datum_info\n",
      "1214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1214it [11:23,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NDCG@5\tRec@5\tHits@5\tPrec@5\tMAP@5\tMRR@5\n",
      "0.0609\t0.0705\t0.0705\t0.0141\t0.0577\t0.0577\n",
      "\n",
      "NDCG@10\tRec@10\tHits@10\tPrec@10\tMAP@10\tMRR@10\n",
      "0.0637\t0.0792\t0.0792\t0.0079\t0.0588\t0.0588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('\\nNDCG@10\\tRec@10\\tHits@10\\tPrec@10\\tMAP@10\\tMRR@10\\n0.0637\\t0.0792\\t0.0792\\t0.0079\\t0.0588\\t0.0588',\n",
       " {'ndcg': 0.06367775589005802,\n",
       "  'map': 0.05883500714661851,\n",
       "  'recall': 0.0791778281475376,\n",
       "  'precision': 0.007917782814753532,\n",
       "  'mrr': 0.05883500714661851,\n",
       "  'hit': 0.0791778281475376})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_task_list = {'sequential': ['A-9']\n",
    "}\n",
    "test_sample_numbers = {'sequential': (1, 1), 'direct': (1, 1), 'explanation': 1}\n",
    "\n",
    "zeroshot_test_loader = get_loader(\n",
    "        args,\n",
    "        test_task_list,\n",
    "        test_sample_numbers,\n",
    "        split=args.test, \n",
    "        mode='test', \n",
    "        batch_size=args.batch_size,\n",
    "        workers=args.num_workers,\n",
    "        distributed=args.distributed,\n",
    "        data_root=\"../data\",        # 显式指明数据在上级目录\n",
    "        feature_root=\"../features\"  # 依此类推\n",
    ")\n",
    "print(len(zeroshot_test_loader))\n",
    "\n",
    "all_info = []\n",
    "for i, batch in tqdm(enumerate(zeroshot_test_loader)):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        beam_outputs = model.generate(\n",
    "                input_ids=batch['input_ids'].to('cuda'), \n",
    "                whole_word_ids=batch['whole_word_ids'].to('cuda'), \n",
    "                category_ids=batch['category_ids'].to('cuda'), \n",
    "                vis_feats=batch['vis_feats'].to('cuda'), \n",
    "                task=batch[\"task\"][0],\n",
    "                max_length=50, \n",
    "                num_beams=20,\n",
    "                no_repeat_ngram_size=0, \n",
    "                num_return_sequences=20,\n",
    "                early_stopping=True\n",
    "        )\n",
    "        generated_sents = model.tokenizer.batch_decode(beam_outputs, skip_special_tokens=True)\n",
    "        for j, item in enumerate(zip(results, batch['target_text'], batch['source_text'])):\n",
    "            new_info = {}\n",
    "            new_info['target_item'] = item[1]\n",
    "            new_info['gen_item_list'] = generated_sents[j*20: (j+1)*20]\n",
    "            all_info.append(new_info)\n",
    "            \n",
    "gt = {}\n",
    "ui_scores = {}\n",
    "for i, info in enumerate(all_info):\n",
    "    gt[i] = [int(info['target_item'])]\n",
    "    pred_dict = {}\n",
    "    for j in range(len(info['gen_item_list'])):\n",
    "        try:\n",
    "            pred_dict[int(info['gen_item_list'][j])] = -(j+1)\n",
    "        except:\n",
    "            pass\n",
    "    ui_scores[i] = pred_dict\n",
    "    \n",
    "evaluate_all(ui_scores, gt, 5)\n",
    "evaluate_all(ui_scores, gt, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sources:  ['toys']\n",
      "compute_datum_info\n",
      "1214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 220.00 MiB. GPU 0 has a total capacty of 47.50 GiB of which 121.06 MiB is free. Process 1795146 has 858.00 MiB memory in use. Process 2248567 has 5.78 GiB memory in use. Process 3031620 has 25.18 GiB memory in use. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 14.92 GiB is allocated by PyTorch, and 157.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     22\u001b[0m     results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate_step(batch)\n\u001b[0;32m---> 23\u001b[0m     beam_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwhole_word_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhole_word_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcategory_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvis_feats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvis_feats\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m            \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     generated_sents \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(beam_outputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(results, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_text\u001b[39m\u001b[38;5;124m'\u001b[39m], batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_text\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/transformers/generation/utils.py:1474\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1468\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1469\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1470\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1471\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m     \u001b[38;5;66;03m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[1;32m   1488\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1489\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/transformers/generation/utils.py:2722\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2718\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   2720\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 2722\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2723\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2725\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2726\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2727\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2730\u001b[0m     cur_len \u001b[38;5;241m=\u001b[39m cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/MyWork/VIP5_Shadowcast_DPA/notebooks_0114_working/modeling_vip5.py:1087\u001b[0m, in \u001b[0;36mVIP5.forward\u001b[0;34m(self, input_ids, whole_word_ids, category_ids, vis_feats, attention_mask, encoder_outputs, decoder_input_ids, decoder_attention_mask, past_key_values, use_cache, labels, inputs_embeds, decoder_inputs_embeds, head_mask, output_attentions, output_hidden_states, return_dict, reduce_loss, return_hidden_state, task, **kwargs)\u001b[0m\n\u001b[1;32m   1084\u001b[0m encoder_attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1087\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtie_word_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/MyWork/VIP5_Shadowcast_DPA/notebooks_0114_working/modeling_vip5.py:691\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, task)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    689\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[0;32m--> 691\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m layer_outputs[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/MyWork/VIP5_Shadowcast_DPA/notebooks_0114_working/modeling_vip5.py:498\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, head_mask, past_key_value, use_cache, output_attentions, return_dict, task)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     query_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 498\u001b[0m cross_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(hidden_states)\u001b[38;5;241m.\u001b[39many():\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/MyWork/VIP5_Shadowcast_DPA/notebooks_0114_working/modeling_vip5.py:406\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, head_mask, past_key_value, use_cache, query_length, output_attentions, task)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    394\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    403\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    404\u001b[0m ):\n\u001b[1;32m    405\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 406\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEncDecAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m     y \u001b[38;5;241m=\u001b[39m attention_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_attn_adapter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/MyWork/VIP5_Shadowcast_DPA/notebooks_0114_working/modeling_vip5.py:293\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    288\u001b[0m value_states \u001b[38;5;241m=\u001b[39m project(\n\u001b[1;32m    289\u001b[0m     hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv, key_value_states, past_key_value[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    290\u001b[0m )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# compute scores\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position_bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_relative_attention_bias:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 220.00 MiB. GPU 0 has a total capacty of 47.50 GiB of which 121.06 MiB is free. Process 1795146 has 858.00 MiB memory in use. Process 2248567 has 5.78 GiB memory in use. Process 3031620 has 25.18 GiB memory in use. Including non-PyTorch memory, this process has 15.56 GiB memory in use. Of the allocated memory 14.92 GiB is allocated by PyTorch, and 157.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "test_task_list = {'sequential': ['A-3']\n",
    "}\n",
    "test_sample_numbers = {'sequential': (1, 1), 'direct': (1, 1), 'explanation': 1}\n",
    "\n",
    "zeroshot_test_loader = get_loader(\n",
    "        args,\n",
    "        test_task_list,\n",
    "        test_sample_numbers,\n",
    "        split=args.test, \n",
    "        mode='test', \n",
    "        batch_size=args.batch_size,\n",
    "        workers=args.num_workers,\n",
    "        distributed=args.distributed,\n",
    "        data_root=\"../data\",        # 显式指明数据在上级目录\n",
    "        feature_root=\"../features\"  # 依此类推\n",
    ")\n",
    "print(len(zeroshot_test_loader))\n",
    "\n",
    "all_info = []\n",
    "for i, batch in tqdm(enumerate(zeroshot_test_loader)):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        beam_outputs = model.generate(\n",
    "                input_ids=batch['input_ids'].to('cuda'), \n",
    "                whole_word_ids=batch['whole_word_ids'].to('cuda'), \n",
    "                category_ids=batch['category_ids'].to('cuda'), \n",
    "                vis_feats=batch['vis_feats'].to('cuda'), \n",
    "                task=batch[\"task\"][0],\n",
    "                max_length=50, \n",
    "                num_beams=20,\n",
    "                no_repeat_ngram_size=0, \n",
    "                num_return_sequences=20,\n",
    "                early_stopping=True\n",
    "        )\n",
    "        generated_sents = model.tokenizer.batch_decode(beam_outputs, skip_special_tokens=True)\n",
    "        for j, item in enumerate(zip(results, batch['target_text'], batch['source_text'])):\n",
    "            new_info = {}\n",
    "            new_info['target_item'] = item[1]\n",
    "            new_info['gen_item_list'] = generated_sents[j*20: (j+1)*20]\n",
    "            all_info.append(new_info)\n",
    "            \n",
    "gt = {}\n",
    "ui_scores = {}\n",
    "for i, info in enumerate(all_info):\n",
    "    gt[i] = [int(info['target_item'])]\n",
    "    pred_dict = {}\n",
    "    for j in range(len(info['gen_item_list'])):\n",
    "        try:\n",
    "            pred_dict[int(info['gen_item_list'][j])] = -(j+1)\n",
    "        except:\n",
    "            pass\n",
    "    ui_scores[i] = pred_dict\n",
    "    \n",
    "evaluate_all(ui_scores, gt, 5)\n",
    "evaluate_all(ui_scores, gt, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Evaluation - Direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sources:  ['toys']\n",
      "compute_datum_info\n",
      "1214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1214it [31:26,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NDCG@1\tRec@1\tHits@1\tPrec@1\tMAP@1\tMRR@1\n",
      "0.0096\t0.0096\t0.0096\t0.0096\t0.0096\t0.0096\n",
      "\n",
      "NDCG@5\tRec@5\tHits@5\tPrec@5\tMAP@5\tMRR@5\n",
      "0.0236\t0.0380\t0.0380\t0.0076\t0.0189\t0.0189\n",
      "\n",
      "NDCG@10\tRec@10\tHits@10\tPrec@10\tMAP@10\tMRR@10\n",
      "0.0308\t0.0605\t0.0605\t0.0060\t0.0218\t0.0218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('\\nNDCG@10\\tRec@10\\tHits@10\\tPrec@10\\tMAP@10\\tMRR@10\\n0.0308\\t0.0605\\t0.0605\\t0.0060\\t0.0218\\t0.0218',\n",
       " {'ndcg': 0.030797409769482007,\n",
       "  'map': 0.021838520764442874,\n",
       "  'recall': 0.06047805481145683,\n",
       "  'precision': 0.00604780548114556,\n",
       "  'mrr': 0.021838520764442874,\n",
       "  'hit': 0.06047805481145683})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_task_list = {'direct': ['B-8']\n",
    "}\n",
    "test_sample_numbers = {'sequential': (1, 1), 'direct': (1, 1), 'explanation': 1}\n",
    "\n",
    "zeroshot_test_loader = get_loader(\n",
    "        args,\n",
    "        test_task_list,\n",
    "        test_sample_numbers,\n",
    "        split=args.test, \n",
    "        mode='test', \n",
    "        batch_size=args.batch_size,\n",
    "        workers=args.num_workers,\n",
    "        distributed=args.distributed,\n",
    "        data_root=\"../data\",        # 显式指明数据在上级目录\n",
    "        feature_root=\"../features\"  # 依此类推\n",
    ")\n",
    "print(len(zeroshot_test_loader))\n",
    "\n",
    "all_info = []\n",
    "for i, batch in tqdm(enumerate(zeroshot_test_loader)):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        beam_outputs = model.generate(\n",
    "                input_ids=batch['input_ids'].to('cuda'), \n",
    "                whole_word_ids=batch['whole_word_ids'].to('cuda'), \n",
    "                category_ids=batch['category_ids'].to('cuda'), \n",
    "                vis_feats=batch['vis_feats'].to('cuda'), \n",
    "                task=batch[\"task\"][0],\n",
    "                max_length=50, \n",
    "                num_beams=20,\n",
    "                no_repeat_ngram_size=0, \n",
    "                num_return_sequences=20,\n",
    "                early_stopping=True\n",
    "        )\n",
    "        generated_sents = model.tokenizer.batch_decode(beam_outputs, skip_special_tokens=True)\n",
    "        for j, item in enumerate(zip(results, batch['target_text'], batch['source_text'])):\n",
    "            new_info = {}\n",
    "            new_info['target_item'] = item[1]\n",
    "            new_info['gen_item_list'] = generated_sents[j*20: (j+1)*20]\n",
    "            all_info.append(new_info)\n",
    "            \n",
    "gt = {}\n",
    "ui_scores = {}\n",
    "for i, info in enumerate(all_info):\n",
    "    gt[i] = [int(info['target_item'])]\n",
    "    pred_dict = {}\n",
    "    for j in range(len(info['gen_item_list'])):\n",
    "        try:\n",
    "            pred_dict[int(info['gen_item_list'][j])] = -(j+1)\n",
    "        except:\n",
    "            pass\n",
    "    ui_scores[i] = pred_dict\n",
    "    \n",
    "evaluate_all(ui_scores, gt, 1)\n",
    "evaluate_all(ui_scores, gt, 5)\n",
    "evaluate_all(ui_scores, gt, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sources:  ['toys']\n",
      "compute_datum_info\n",
      "1214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1214it [16:18,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NDCG@1\tRec@1\tHits@1\tPrec@1\tMAP@1\tMRR@1\n",
      "0.0369\t0.0369\t0.0369\t0.0369\t0.0369\t0.0369\n",
      "\n",
      "NDCG@5\tRec@5\tHits@5\tPrec@5\tMAP@5\tMRR@5\n",
      "0.0713\t0.1048\t0.1048\t0.0210\t0.0603\t0.0603\n",
      "\n",
      "NDCG@10\tRec@10\tHits@10\tPrec@10\tMAP@10\tMRR@10\n",
      "0.0928\t0.1723\t0.1723\t0.0172\t0.0691\t0.0691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('\\nNDCG@10\\tRec@10\\tHits@10\\tPrec@10\\tMAP@10\\tMRR@10\\n0.0928\\t0.1723\\t0.1723\\t0.0172\\t0.0691\\t0.0691',\n",
       " {'ndcg': 0.09283963115235772,\n",
       "  'map': 0.06905033786988249,\n",
       "  'recall': 0.17226457861116834,\n",
       "  'precision': 0.017226457861117223,\n",
       "  'mrr': 0.06905033786988249,\n",
       "  'hit': 0.17226457861116834})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_task_list = {'direct': ['B-5']\n",
    "}\n",
    "test_sample_numbers = {'sequential': (1, 1), 'direct': (1, 1), 'explanation': 1}\n",
    "\n",
    "zeroshot_test_loader = get_loader(\n",
    "        args,\n",
    "        test_task_list,\n",
    "        test_sample_numbers,\n",
    "        split=args.test, \n",
    "        mode='test', \n",
    "        batch_size=args.batch_size,\n",
    "        workers=args.num_workers,\n",
    "        distributed=args.distributed,\n",
    "        data_root=\"../data\",        # 显式指明数据在上级目录\n",
    "        feature_root=\"../features\"  # 依此类推\n",
    ")\n",
    "print(len(zeroshot_test_loader))\n",
    "\n",
    "all_info = []\n",
    "for i, batch in tqdm(enumerate(zeroshot_test_loader)):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        beam_outputs = model.generate(\n",
    "                input_ids=batch['input_ids'].to('cuda'), \n",
    "                whole_word_ids=batch['whole_word_ids'].to('cuda'), \n",
    "                category_ids=batch['category_ids'].to('cuda'), \n",
    "                vis_feats=batch['vis_feats'].to('cuda'), \n",
    "                task=batch[\"task\"][0],\n",
    "                max_length=50, \n",
    "                num_beams=20,\n",
    "                no_repeat_ngram_size=0, \n",
    "                num_return_sequences=20,\n",
    "                early_stopping=True\n",
    "        )\n",
    "        generated_sents = model.tokenizer.batch_decode(beam_outputs, skip_special_tokens=True)\n",
    "        for j, item in enumerate(zip(results, batch['target_text'], batch['source_text'])):\n",
    "            new_info = {}\n",
    "            new_info['target_item'] = item[1]\n",
    "            new_info['gen_item_list'] = generated_sents[j*20: (j+1)*20]\n",
    "            all_info.append(new_info)\n",
    "            \n",
    "gt = {}\n",
    "ui_scores = {}\n",
    "for i, info in enumerate(all_info):\n",
    "    gt[i] = [int(info['target_item'])]\n",
    "    pred_dict = {}\n",
    "    for j in range(len(info['gen_item_list'])):\n",
    "        try:\n",
    "            pred_dict[int(info['gen_item_list'][j])] = -(j+1)\n",
    "        except:\n",
    "            pass\n",
    "    ui_scores[i] = pred_dict\n",
    "    \n",
    "evaluate_all(ui_scores, gt, 1)\n",
    "evaluate_all(ui_scores, gt, 5)\n",
    "evaluate_all(ui_scores, gt, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vip5_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
