{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 1: 环境设置与路径配置\n",
    "\n",
    "功能说明：\n",
    "将项目根目录添加到 Python 模块搜索路径中，确保后续能够正确加载项目内部模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: 环境设置与路径配置\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 将项目根目录添加到 sys.path 中\n",
    "project_path = \"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA\"\n",
    "if project_path not in sys.path:\n",
    "    sys.path.insert(0, project_path)\n",
    "print(\"Project path:\", project_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 2: 导入依赖库与模块\n",
    "\n",
    "功能说明：\n",
    "导入所有需要的第三方库和项目内部模块。注意部分模块（如 P5Tokenizer）在后续 cell 中会用到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有依赖库已导入\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: 导入依赖库与模块\n",
    "import collections\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from packaging import version\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gzip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# 导入项目内部模块\n",
    "from src.param import parse_args\n",
    "from src.utils import LossMeter, load_state_dict, set_global_logging_level\n",
    "from src.dist_utils import reduce_dict\n",
    "from transformers import T5Tokenizer\n",
    "from src.tokenization import P5Tokenizer\n",
    "from src.model import VIP5Tuning\n",
    "from src.trainer_base import TrainerBase\n",
    "\n",
    "# 判断是否使用 native AMP 或 Apex\n",
    "_use_native_amp = False\n",
    "_use_apex = False\n",
    "if version.parse(torch.__version__) < version.parse(\"1.6\"):\n",
    "    from transormers.file_utils import is_apex_available\n",
    "    if is_apex_available():\n",
    "        from apex import amp\n",
    "    _use_apex = True\n",
    "else:\n",
    "    _use_native_amp = True\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "print(\"所有依赖库已导入\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 3: 定义辅助函数\n",
    "\n",
    "功能说明：\n",
    "定义常用的辅助函数，如 pickle、json 的加载函数，以及文件读取函数等，方便后续调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "辅助函数定义完成\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: 定义辅助函数\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_pickle(data, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def ReadLineFromFile(path):\n",
    "    lines = []\n",
    "    with open(path, 'r') as fd:\n",
    "        for line in fd:\n",
    "            lines.append(line.rstrip('\\n'))\n",
    "    return lines\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "print(\"辅助函数定义完成\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 4: 定义 DotDict 类及参数设置\n",
    "\n",
    "功能说明：\n",
    "定义一个 DotDict 类，使得可以通过属性方式访问字典中的值；并设置所有实验参数、随机种子等，保证实验结果可复现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前参数配置：\n",
      "{'distributed': False, 'multiGPU': True, 'fp16': True, 'split': 'toys', 'train': 'toys', 'valid': 'toys', 'test': 'toys', 'batch_size': 16, 'optim': 'adamw', 'warmup_ratio': 0.1, 'lr': 0.001, 'num_workers': 4, 'clip_grad_norm': 5.0, 'losses': 'sequential,direct,explanation', 'backbone': 't5-small', 'image_feature_type': 'vitb32', 'image_feature_size_ratio': 2, 'use_adapter': True, 'reduction_factor': 8, 'use_single_adapter': True, 'use_vis_layer_norm': True, 'add_adapter_cross_attn': True, 'use_lm_head_adapter': True, 'epoch': 20, 'local_rank': 0, 'comment': '', 'train_topk': -1, 'valid_topk': -1, 'dropout': 0.1, 'tokenizer': 'p5', 'max_text_length': 1024, 'gen_max_length': 64, 'do_lower_case': False, 'weight_decay': 0.01, 'adam_eps': 1e-06, 'gradient_accumulation_steps': 1, 'seed': 2022, 'whole_word_embed': True, 'category_embed': True, 'world_size': 4, 'LOSSES_NAME': ['sequential_loss', 'direct_loss', 'explanation_loss', 'total_loss']}\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: 设置参数与随机种子\n",
    "# 功能：构造参数对象、设置随机种子及各项实验参数，保证实验结果可复现。\n",
    "\n",
    "class DotDict(dict):\n",
    "    \"\"\"将字典转化为对象，支持通过属性访问\"\"\"\n",
    "    def __init__(self, **kwds):\n",
    "        self.update(kwds)\n",
    "        self.__dict__ = self\n",
    "    def __repr__(self):\n",
    "        # 避免递归调用 __repr__，直接调用 dict 的 __repr__\n",
    "        return dict.__repr__(self)\n",
    "\n",
    "# 构造参数对象\n",
    "args = DotDict()\n",
    "\n",
    "# ----------------- 基本训练参数 -----------------\n",
    "args.distributed = False\n",
    "args.multiGPU = True\n",
    "args.fp16 = True\n",
    "\n",
    "args.split = \"toys\"\n",
    "args.train = args.split\n",
    "args.valid = args.split\n",
    "args.test = args.split\n",
    "args.batch_size = 16\n",
    "args.optim = 'adamw'\n",
    "args.warmup_ratio = 0.1\n",
    "args.lr = 1e-3\n",
    "args.num_workers = 4\n",
    "args.clip_grad_norm = 5.0\n",
    "args.losses = 'sequential,direct,explanation'\n",
    "args.backbone = 't5-small'\n",
    "\n",
    "# ----------------- 模型及视觉特征参数 -----------------\n",
    "args.image_feature_type = 'vitb32'\n",
    "args.image_feature_size_ratio = 2\n",
    "args.use_adapter = True\n",
    "args.reduction_factor = 8\n",
    "args.use_single_adapter = True\n",
    "args.use_vis_layer_norm = True\n",
    "args.add_adapter_cross_attn = True\n",
    "args.use_lm_head_adapter = True\n",
    "\n",
    "# ----------------- 训练轮数、随机种子等 -----------------\n",
    "args.epoch = 20\n",
    "args.local_rank = 0\n",
    "args.comment = ''\n",
    "args.train_topk = -1\n",
    "args.valid_topk = -1\n",
    "args.dropout = 0.1\n",
    "args.tokenizer = 'p5'\n",
    "args.max_text_length = 1024\n",
    "args.gen_max_length = 64\n",
    "args.do_lower_case = False\n",
    "args.weight_decay = 0.01\n",
    "args.adam_eps = 1e-6\n",
    "args.gradient_accumulation_steps = 1\n",
    "\n",
    "# 设置随机种子\n",
    "args.seed = 2022\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "# ----------------- 启用 Whole Word 和 Category Embedding -----------------\n",
    "args.whole_word_embed = True\n",
    "args.category_embed = True\n",
    "\n",
    "# ----------------- cudnn 及 GPU 参数 -----------------\n",
    "cudnn.benchmark = True\n",
    "ngpus_per_node = torch.cuda.device_count()\n",
    "args.world_size = ngpus_per_node\n",
    "\n",
    "# 设置损失项名称列表\n",
    "LOSSES_NAME = [f'{name}_loss' for name in args.losses.split(',')]\n",
    "LOSSES_NAME.append('total_loss')\n",
    "args.LOSSES_NAME = LOSSES_NAME\n",
    "\n",
    "print(\"当前参数配置：\")\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 5: GPU设置与生成运行名称\n",
    "\n",
    "功能说明：\n",
    "指定使用的 GPU（手动设置），并构造一个运行名称（run_name），便于后续日志及保存结果区分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Launching at GPU 3\n",
      "运行名称: 0419_GPU4_toys_t5-small_sequentialdirectexplanation\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: GPU设置与生成运行名称\n",
    "# 功能：指定 GPU（手动设置），并构造一个运行名称\n",
    "\n",
    "# 手动指定 GPU ID\n",
    "gpu = 3\n",
    "args.gpu = gpu\n",
    "args.rank = gpu\n",
    "print(f'Process Launching at GPU {gpu}')\n",
    "\n",
    "# 设置当前 GPU 设备\n",
    "torch.cuda.set_device(f'cuda:{gpu}')\n",
    "\n",
    "# 构造运行名称\n",
    "comments = []\n",
    "dsets = []\n",
    "if 'toys' in args.train:\n",
    "    dsets.append('toys')\n",
    "if 'beauty' in args.train:\n",
    "    dsets.append('beauty')\n",
    "if 'sports' in args.train:\n",
    "    dsets.append('sports')\n",
    "if 'clothing' in args.train:\n",
    "    dsets.append('clothing')\n",
    "comments.append(''.join(dsets))\n",
    "if args.backbone:\n",
    "    comments.append(args.backbone)\n",
    "comments.append(''.join(args.losses.split(',')))\n",
    "if args.comment != '':\n",
    "    comments.append(args.comment)\n",
    "comment = '_'.join(comments)\n",
    "\n",
    "from datetime import datetime\n",
    "current_time = datetime.now().strftime('%m%d')  # 例如 '0304'\n",
    "\n",
    "if args.local_rank in [0, -1]:\n",
    "    run_name = f'{current_time}_GPU{args.world_size}'\n",
    "    if len(comments) > 0:\n",
    "        run_name += f'_{comment}'\n",
    "    args.run_name = run_name\n",
    "    print(\"运行名称:\", args.run_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 6: 构建模型配置、Tokenizer 与模型\n",
    "\n",
    "功能说明：\n",
    "根据参数构建模型配置（config）、创建 Tokenizer，并加载预训练模型。\n",
    "注意：由于 checkpoint 使用的是 T5Tokenizer，而我们调用 P5Tokenizer，所以会有警告信息，但功能不受影响。\n",
    "另外，为了适配 adapter，需要将 config.d_model 赋值给 adapter_config。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: <class 'src.tokenization.P5Tokenizer'> t5-small\n",
      "Building Model at GPU 3\n",
      "JointEncoder initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VIP5Tuning were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.block.0.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'output_adapter.adapter.down_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.category_embeddings.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'output_adapter.adapter.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.visual_embedding.feat_embedding.0.model.0.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'output_adapter.adapter.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.visual_embedding.feat_embedding.0.model.2.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.visual_embedding.feat_embedding.0.model.2.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.whole_word_embeddings.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.visual_embedding.feat_embedding.1.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'output_adapter.adapter.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.visual_embedding.feat_embedding.0.model.0.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5Stack initialized successfully.\n",
      "lm_head initialized successfully.\n",
      "OutputParallelAdapterLayer initialized successfully.\n",
      "AdapterConfig: AdapterConfig(add_layer_norm_before_adapter=False, add_layer_norm_after_adapter=False, non_linearity='gelu_new', reduction_factor=8)\n",
      "模型和 Tokenizer 构建完成\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: 构建模型配置、Tokenizer 与模型\n",
    "# 功能：根据参数构建模型配置，创建 Tokenizer，并加载预训练模型\n",
    "import re  # 确保导入 re 模块\n",
    "\n",
    "def create_config(args):\n",
    "    from transformers import T5Config\n",
    "    from adapters import AdapterConfig  # 使用适配器配置\n",
    "\n",
    "    # 从预训练 checkpoint 加载 T5 配置\n",
    "    config = T5Config.from_pretrained(args.backbone)\n",
    "    # 将所有参数写入配置中\n",
    "    for k, v in vars(args).items():\n",
    "        setattr(config, k, v)\n",
    "    config.non_linearity = \"relu\"\n",
    "\n",
    "    # 设置视觉特征参数\n",
    "    image_feature_dim_dict = {\n",
    "        'vitb32': 512,\n",
    "        'vitb16': 512,\n",
    "        'vitl14': 768,\n",
    "        'rn50': 1024,\n",
    "        'rn101': 512\n",
    "    }\n",
    "    config.feat_dim = image_feature_dim_dict[args.image_feature_type]\n",
    "    config.n_vis_tokens = args.image_feature_size_ratio\n",
    "    config.use_vis_layer_norm = args.use_vis_layer_norm\n",
    "    config.reduction_factor = args.reduction_factor\n",
    "\n",
    "    config.use_adapter = args.use_adapter\n",
    "    config.add_adapter_cross_attn = args.add_adapter_cross_attn\n",
    "    config.use_lm_head_adapter = args.use_lm_head_adapter\n",
    "    config.use_single_adapter = args.use_single_adapter\n",
    "\n",
    "    config.dropout_rate = args.dropout\n",
    "    config.dropout = args.dropout\n",
    "    config.attention_dropout = args.dropout\n",
    "    config.activation_dropout = args.dropout\n",
    "\n",
    "    config.losses = args.losses\n",
    "\n",
    "    # 如果使用适配器，则创建适配器配置，并将主配置的 d_model 传给 adapter_config\n",
    "    tasks = re.split(\"[, ]+\", args.losses)\n",
    "    if args.use_adapter:\n",
    "        adapter_config = AdapterConfig()\n",
    "        adapter_config.tasks = tasks\n",
    "        adapter_config.d_model = config.d_model  # 传递隐藏维度\n",
    "        adapter_config.use_single_adapter = args.use_single_adapter\n",
    "        adapter_config.reduction_factor = args.reduction_factor\n",
    "        adapter_config.track_z = False\n",
    "        config.adapter_config = adapter_config\n",
    "    else:\n",
    "        config.adapter_config = None\n",
    "\n",
    "    return config\n",
    "\n",
    "def create_tokenizer(args):\n",
    "    from transformers import T5Tokenizer\n",
    "    # 根据参数决定使用 P5Tokenizer 或 T5Tokenizer\n",
    "    if 'p5' in args.tokenizer:\n",
    "        from src.tokenization import P5Tokenizer\n",
    "        tokenizer_class = P5Tokenizer\n",
    "    else:\n",
    "        tokenizer_class = T5Tokenizer\n",
    "\n",
    "    tokenizer = tokenizer_class.from_pretrained(\n",
    "        args.backbone,\n",
    "        max_length=args.max_text_length,\n",
    "        do_lower_case=args.do_lower_case,\n",
    "    )\n",
    "    print(\"Tokenizer:\", tokenizer_class, args.backbone)\n",
    "    return tokenizer\n",
    "\n",
    "def create_model(model_class, config):\n",
    "    print(f'Building Model at GPU {args.gpu}')\n",
    "    model = model_class.from_pretrained(\n",
    "        args.backbone,\n",
    "        config=config\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 构建配置、Tokenizer 和模型\n",
    "config = create_config(args)\n",
    "if args.tokenizer is None:\n",
    "    args.tokenizer = args.backbone\n",
    "tokenizer = create_tokenizer(args)\n",
    "model_class = VIP5Tuning\n",
    "model = create_model(model_class, config)\n",
    "\n",
    "# 将模型移至指定 GPU\n",
    "model = model.cuda()\n",
    "\n",
    "# 如果使用 P5Tokenizer，则调整模型的词嵌入\n",
    "if 'p5' in args.tokenizer:\n",
    "    model.resize_token_embeddings(tokenizer.vocab_size)\n",
    "model.tokenizer = tokenizer\n",
    "\n",
    "print(\"模型和 Tokenizer 构建完成\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 7: 加载预训练模型权重\n",
    "\n",
    "功能说明：\n",
    "从指定 checkpoint 路径加载预训练模型权重，并打印加载结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['output_adapter.adapter.down_sampler.weight', 'output_adapter.adapter.down_sampler.bias', 'output_adapter.adapter.up_sampler.weight', 'output_adapter.adapter.up_sampler.bias'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: 加载预训练模型权重\n",
    "# 功能：从 checkpoint 加载预训练模型权重\n",
    "from pprint import pprint\n",
    "\n",
    "def load_checkpoint(ckpt_path):\n",
    "    state_dict = load_state_dict(ckpt_path, 'cpu')\n",
    "    results = model.load_state_dict(state_dict, strict=False)\n",
    "    pprint(results)\n",
    "\n",
    "# 指定 checkpoint 路径（需根据实际路径修改）\n",
    "args.load = \"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/snap/toys/0417/toys-vitb32-2-8-20-DirectBoostingAttack/BEST_EVAL_LOSS.pth\"\n",
    "ckpt_path = args.load\n",
    "load_checkpoint(ckpt_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 8: 加载数据集及数据映射\n",
    "\n",
    "功能说明：\n",
    "加载数据分割文件（如 rating_splits_augmented.pkl）以及数据映射文件（datamaps.json），用于后续评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data长度: 16759\n",
      "Test data示例: {'reviewerID': 'A5K3CK2PWYQ7O', 'asin': 'B00F4CFEYG', 'reviewerName': 'Ellie \"mittbooks\"', 'helpful': [0, 0], 'reviewText': \"I've found the Melissa & Doug brand to be overall good, although there are occasional negatives.  This is definitely one of the toys we'll mark a &#34;winner.&#34;  The vacuum comes in two pieces that require minimal assembly (the long handle and the base need to be put together - no tools required).  The height is perfect for our two year old who is 3 feet tall.  The top part moves at about a 45 degree angle to facilitate little people pushing the vacuum.  I'm not sure how long the six wooden pieces of &#34;trash&#34; will last.  Although not tiny, they would be easy to lose.  The vacuum does a good job of picking them up easily and there is a small area in the back of the base to take them out again.  There is also a rotating knob on the front of the handle that makes a good clicking noise when it moves.  Our son is truly enjoying this this toy and the overall quality is excellent.  Definitely a good addition to the toy room.\", 'overall': 4.0, 'summary': '... found the Melissa & Doug brand to be overall good, although there are occasional negatives', 'unixReviewTime': 1403913600, 'reviewTime': '06 28, 2014', 'explanation': \"I've found the Melissa & Doug brand to be overall good\", 'feature': 'overall'}\n",
      "用户数量: 19412\n",
      "物品数量: 11924\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: 加载数据集及数据映射\n",
    "# 功能：加载 rating_splits_augmented.pkl 和 datamaps.json 数据文件\n",
    "\n",
    "data_splits = load_pickle(f'../data/{args.split}/rating_splits_augmented.pkl')\n",
    "test_review_data = data_splits['test']\n",
    "print(\"Test data长度:\", len(test_review_data))\n",
    "print(\"Test data示例:\", test_review_data[0])\n",
    "\n",
    "data_maps = load_json(os.path.join('../data', args.split, 'datamaps.json'))\n",
    "print(\"用户数量:\", len(data_maps['user2id']))\n",
    "print(\"物品数量:\", len(data_maps['item2id']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 9: 加载数据生成器与评价指标\n",
    "\n",
    "功能说明：\n",
    "导入数据加载函数和评价指标函数，为后续评估生成数据加载器和计算 BLEU/ROUGE 等指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据加载器与评价指标函数已导入\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: 导入数据加载器与评价指标函数\n",
    "# 功能：导入 get_loader、BLEU、ROUGE 等评价指标函数\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "from src.data import get_loader\n",
    "from evaluate.utils import rouge_score, bleu_score, unique_sentence_percent, root_mean_square_error, mean_absolute_error, feature_detect, feature_matching_ratio, feature_coverage_ratio, feature_diversity\n",
    "from evaluate.metrics4rec import evaluate_all\n",
    "\n",
    "print(\"数据加载器与评价指标函数已导入\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 10: Evaluation - Explanation 任务\n",
    "\n",
    "功能说明：\n",
    "加载 explanation 任务的数据生成器，调用模型生成输出，并计算 BLEU、ROUGE 指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sources:  ['toys']\n",
      "[INFO] 加载扩展映射: ../data/toys/user_id2name_poisoned.pkl\n",
      "compute_datum_info\n",
      "Explanation 任务 (Prompt: C-12) 数据量: 646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 646/646 [01:27<00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1 12.4267\n",
      "BLEU-4  7.8793\n",
      "rouge_1/f_score 27.2064\n",
      "rouge_1/r_score 22.2341\n",
      "rouge_1/p_score 44.9808\n",
      "rouge_2/f_score  8.7732\n",
      "rouge_2/r_score  7.3634\n",
      "rouge_2/p_score 15.3068\n",
      "rouge_l/f_score 20.7347\n",
      "rouge_l/r_score 20.4198\n",
      "rouge_l/p_score 41.7443\n",
      "Explanation 任务 (Prompt: C-12) 评价结果已保存至: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log/toys/0417/evaluation_logs/VIP5_toys_vitb32_8_20_evaluation_explanation_C-12.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 10: Evaluation - Explanation 任务（带 Prompt 信息）\n",
    "# 功能说明：\n",
    "#   1. 加载指定 prompt（例如 'C-12'）下的 Explanation 任务测试数据；\n",
    "#   2. 调用模型生成预测结果，并计算 BLEU（1-gram 和 4-gram）与 ROUGE 指标；\n",
    "#   3. 在保存评估结果文件时，文件名和文件内容中都会包含当前使用的 prompt 信息，\n",
    "#      便于后续对比不同 prompt 的评估效果。\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# 如果 args.load 不为空，则从其中提取日期，否则使用当前日期\n",
    "if args.load is not None:\n",
    "    # 假定 args.load 形如 \".../snap/<split>/<日期>/<exp_name>/BEST_EVAL_LOSS.pth\"\n",
    "    eval_date = Path(args.load).parents[1].name\n",
    "else:\n",
    "    eval_date = datetime.now().strftime(\"%m%d\")\n",
    "\n",
    "# 指定 Explanation 任务的 prompt 及样本数量\n",
    "exp_prompt = 'C-12'  # 可修改为 'C-12', 'C-3' 等所需的 prompt 编号\n",
    "test_task_list = {'explanation': [exp_prompt]}\n",
    "test_sample_numbers = {'sequential': (1, 1), 'direct': (1, 1), 'explanation': 1}\n",
    "\n",
    "# 获取 Explanation 任务的测试数据加载器\n",
    "zeroshot_test_loader = get_loader(\n",
    "    args,\n",
    "    test_task_list,\n",
    "    test_sample_numbers,\n",
    "    split=args.test, \n",
    "    mode='test', \n",
    "    batch_size=args.batch_size,\n",
    "    workers=args.num_workers,\n",
    "    distributed=args.distributed,\n",
    "    data_root=\"../data\",        # 显式指定数据目录\n",
    "    feature_root=\"../features\"  # 显式指定视觉特征目录\n",
    ")\n",
    "print(f\"Explanation 任务 (Prompt: {exp_prompt}) 数据量:\", len(zeroshot_test_loader))\n",
    "\n",
    "tokens_predict = []\n",
    "tokens_test = []\n",
    "\n",
    "# 遍历测试数据加载器，调用模型生成预测结果\n",
    "for i, batch in tqdm(enumerate(zeroshot_test_loader), total=len(zeroshot_test_loader), ncols=100):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        tokens_predict.extend(results)\n",
    "        tokens_test.extend(batch['target_text'])\n",
    "\n",
    "# 计算 BLEU 与 ROUGE 指标\n",
    "BLEU1 = bleu_score(tokens_test, tokens_predict, n_gram=1, smooth=False)\n",
    "print(f'BLEU-1 {BLEU1:7.4f}')\n",
    "BLEU4 = bleu_score(tokens_test, tokens_predict, n_gram=4, smooth=False)\n",
    "print(f'BLEU-4 {BLEU4:7.4f}')\n",
    "\n",
    "ROUGE = rouge_score(tokens_test, tokens_predict)\n",
    "for k, v in ROUGE.items():\n",
    "    print(f'{k} {v:7.4f}')\n",
    "\n",
    "# 构建保存评估结果的目录和文件名，文件名中包含当前使用的 prompt 信息\n",
    "eval_dir = f\"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log/{args.split}/{eval_date}/evaluation_logs\"\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "\n",
    "explanation_filename = (\n",
    "    f\"VIP5_{args.split}_{args.image_feature_type}_\"\n",
    "    f\"{args.reduction_factor}_{args.epoch}_evaluation_explanation_{exp_prompt}.txt\"\n",
    ")\n",
    "explanation_log_path = os.path.join(eval_dir, explanation_filename)\n",
    "\n",
    "# 保存评估结果，文件内容中也包含 prompt 信息\n",
    "with open(explanation_log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Explanation Evaluation Results\\n\")\n",
    "    f.write(f\"Prompt: {exp_prompt}\\n\")\n",
    "    f.write(f\"BLEU-1: {BLEU1:7.4f}\\n\")\n",
    "    f.write(f\"BLEU-4: {BLEU4:7.4f}\\n\")\n",
    "    for k, v in ROUGE.items():\n",
    "        f.write(f\"{k}: {v:7.4f}\\n\")\n",
    "\n",
    "print(f\"Explanation 任务 (Prompt: {exp_prompt}) 评价结果已保存至: {explanation_log_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 11: Evaluation - Direct 任务\n",
    "\n",
    "功能说明：\n",
    "加载 direct 任务的测试数据，生成输出并计算评价指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sources:  ['toys']\n",
      "[INFO] 加载扩展映射: ../data/toys/user_id2name_poisoned.pkl\n",
      "compute_datum_info\n",
      "Direct 任务 (Prompt: B-8) 数据量: 1214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1214 [00:00<?, ?it/s]/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/site-packages/transformers/modeling_utils.py:810: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/site-packages/transformers/modeling_utils.py:764: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1214/1214 [16:30<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NDCG@1\tRec@1\tHits@1\tPrec@1\tMAP@1\tMRR@1\tER@1\n",
      "0.0433\t0.0433\t0.0433\t0.0433\t0.0433\t0.0433\t0.0433\n",
      "\n",
      "NDCG@5\tRec@5\tHits@5\tPrec@5\tMAP@5\tMRR@5\tER@5\n",
      "0.0833\t0.1225\t0.1225\t0.0245\t0.0705\t0.0705\t0.1225\n",
      "\n",
      "NDCG@10\tRec@10\tHits@10\tPrec@10\tMAP@10\tMRR@10\tER@10\n",
      "0.1048\t0.1899\t0.1899\t0.0190\t0.0792\t0.0792\t0.1899\n",
      "\n",
      "Evaluation Metrics at top-1:\n",
      "\n",
      "NDCG@1\tRec@1\tHits@1\tPrec@1\tMAP@1\tMRR@1\tER@1\n",
      "0.0433\t0.0433\t0.0433\t0.0433\t0.0433\t0.0433\t0.0433\n",
      "ER@1: 0.0433\n",
      "\n",
      "Evaluation Metrics at top-5:\n",
      "\n",
      "NDCG@5\tRec@5\tHits@5\tPrec@5\tMAP@5\tMRR@5\tER@5\n",
      "0.0833\t0.1225\t0.1225\t0.0245\t0.0705\t0.0705\t0.1225\n",
      "ER@5: 0.1225\n",
      "\n",
      "Evaluation Metrics at top-10:\n",
      "\n",
      "NDCG@10\tRec@10\tHits@10\tPrec@10\tMAP@10\tMRR@10\tER@10\n",
      "0.1048\t0.1899\t0.1899\t0.0190\t0.0792\t0.0792\t0.1899\n",
      "ER@10: 0.1899\n",
      "Direct 任务 (Prompt: B-8) 评价结果已保存至: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log/toys/0417/evaluation_logs/VIP5_toys_vitb32_8_20_evaluation_direct_B-8.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 11: Evaluation - Direct 任务（带 Prompt 信息）\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# 如果 args.load 不为空，则从其中提取日期，否则使用当前日期\n",
    "if args.load is not None:\n",
    "    eval_date = Path(args.load).parents[1].name\n",
    "else:\n",
    "    eval_date = datetime.now().strftime(\"%m%d\")\n",
    "\n",
    "# 指定 Direct 任务的测试任务与 Prompt\n",
    "test_task_list = {'direct': ['B-8']}  # 例：可选 'B-5' 或 'B-8'\n",
    "prompt = test_task_list['direct'][0]\n",
    "\n",
    "test_sample_numbers = {\n",
    "    'sequential': (1, 1),\n",
    "    'direct': (1, 1),\n",
    "    'explanation': 1\n",
    "}\n",
    "\n",
    "# 获取 Direct 任务的测试数据加载器\n",
    "zeroshot_test_loader = get_loader(\n",
    "    args,\n",
    "    test_task_list,\n",
    "    test_sample_numbers,\n",
    "    split=args.test,\n",
    "    mode='test',\n",
    "    batch_size=args.batch_size,\n",
    "    workers=args.num_workers,\n",
    "    distributed=args.distributed,\n",
    "    data_root=\"../data\",\n",
    "    feature_root=\"../features\"\n",
    ")\n",
    "\n",
    "print(f\"Direct 任务 (Prompt: {prompt}) 数据量:\", len(zeroshot_test_loader))\n",
    "\n",
    "# 收集所有样本的 ground truth 和模型输出\n",
    "all_info = []\n",
    "for i, batch in tqdm(enumerate(zeroshot_test_loader), total=len(zeroshot_test_loader)):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        beam_outputs = model.generate(\n",
    "            input_ids=batch['input_ids'].to('cuda'),\n",
    "            whole_word_ids=batch['whole_word_ids'].to('cuda'),\n",
    "            category_ids=batch['category_ids'].to('cuda'),\n",
    "            vis_feats=batch['vis_feats'].to('cuda'),\n",
    "            task=batch[\"task\"][0],\n",
    "            max_length=50,\n",
    "            num_beams=20,\n",
    "            no_repeat_ngram_size=0,\n",
    "            num_return_sequences=20,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        generated_sents = model.tokenizer.batch_decode(beam_outputs, skip_special_tokens=True)\n",
    "\n",
    "        # 遍历当前批次中每个样本（每个样本生成20个候选）\n",
    "        for j, (_, tgt_text, _) in enumerate(zip(results, batch['target_text'], batch['source_text'])):\n",
    "            all_info.append({\n",
    "                'target_item': tgt_text,\n",
    "                'gen_item_list': generated_sents[j * 20: (j + 1) * 20]\n",
    "            })\n",
    "\n",
    "# 构造 ground truth 字典和用户-物品分数字典\n",
    "gt = {}\n",
    "ui_scores = {}\n",
    "for i, info in enumerate(all_info):\n",
    "    gt[i] = [int(info['target_item'])]\n",
    "    pred_dict = {}\n",
    "    for j, pred in enumerate(info['gen_item_list']):\n",
    "        try:\n",
    "            pred_dict[int(pred)] = -(j + 1)  # 候选越靠前分数越大\n",
    "        except:\n",
    "            pass\n",
    "    ui_scores[i] = pred_dict\n",
    "\n",
    "# 如果想测某些特定集合的曝光率，定义 targeted_items\n",
    "# 这里示例中用 ground truth 本身\n",
    "targeted_items = gt.copy()\n",
    "\n",
    "# 计算推荐指标 + ER@K\n",
    "msg_top1,  res_top1  = evaluate_all(ui_scores, gt, topk=1,  targeted_items=targeted_items)\n",
    "msg_top5,  res_top5  = evaluate_all(ui_scores, gt, topk=5,  targeted_items=targeted_items)\n",
    "msg_top10, res_top10 = evaluate_all(ui_scores, gt, topk=10, targeted_items=targeted_items)\n",
    "\n",
    "print(\"\\nEvaluation Metrics at top-1:\")\n",
    "print(msg_top1)\n",
    "print(\"ER@1: {:.4f}\".format(res_top1['er']))\n",
    "\n",
    "print(\"\\nEvaluation Metrics at top-5:\")\n",
    "print(msg_top5)\n",
    "print(\"ER@5: {:.4f}\".format(res_top5['er']))\n",
    "\n",
    "print(\"\\nEvaluation Metrics at top-10:\")\n",
    "print(msg_top10)\n",
    "print(\"ER@10: {:.4f}\".format(res_top10['er']))\n",
    "\n",
    "# 保存 Direct 任务评价结果到文件，文件名包含 Prompt\n",
    "eval_dir = f\"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log/{args.split}/{eval_date}/evaluation_logs\"\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "\n",
    "direct_filename = (\n",
    "    f\"VIP5_{args.split}_\"\n",
    "    f\"{args.image_feature_type}_\"\n",
    "    f\"{args.reduction_factor}_\"\n",
    "    f\"{args.epoch}_evaluation_direct_{prompt}.txt\"\n",
    ")\n",
    "direct_log_path = os.path.join(eval_dir, direct_filename)\n",
    "\n",
    "with open(direct_log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Direct Evaluation Results\\n\")\n",
    "    f.write(f\"Prompt: {prompt}\\n\\n\")\n",
    "    f.write(\"Metrics @1:\\n\")\n",
    "    f.write(msg_top1 + \"\\n\")\n",
    "    f.write(f\"ER@1:  {res_top1['er']:.4f}\\n\\n\")\n",
    "    f.write(\"Metrics @5:\\n\")\n",
    "    f.write(msg_top5 + \"\\n\")\n",
    "    f.write(f\"ER@5:  {res_top5['er']:.4f}\\n\\n\")\n",
    "    f.write(\"Metrics @10:\\n\")\n",
    "    f.write(msg_top10 + \"\\n\")\n",
    "    f.write(f\"ER@10: {res_top10['er']:.4f}\\n\")\n",
    "\n",
    "print(f\"Direct 任务 (Prompt: {prompt}) 评价结果已保存至: {direct_log_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 12: Evaluation - Sequential 任务\n",
    "\n",
    "功能说明：\n",
    "加载 sequential 任务的测试数据，生成输出并计算评价指标，同时对 beam search 结果进行解码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sources:  ['toys']\n",
      "[INFO] 加载扩展映射: ../data/toys/user_id2name_poisoned.pkl\n",
      "compute_datum_info\n",
      "Sequential 任务 (Prompt: A-3) 数据量: 1214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 702/1214 [08:04<05:53,  1.45it/s]Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "\n",
      "    self.run()\n",
      "  File \"/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py\", line 54, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py\", line 31, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 355, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/multiprocessing/connection.py\", line 502, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/multiprocessing/connection.py\", line 630, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     42\u001b[0m     results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate_step(batch)\n\u001b[0;32m---> 43\u001b[0m     beam_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhole_word_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhole_word_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategory_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvis_feats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvis_feats\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     generated_sents \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(beam_outputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# 遍历当前批次中每个样本（假设每个样本生成20个候选）\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/transformers/generation/utils.py:1252\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1244\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1245\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1246\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration results, please set `padding_side=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` when initializing the tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1247\u001b[0m         )\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;66;03m# and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1252\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/transformers/generation/utils.py:617\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[1;32m    615\u001b[0m encoder_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    616\u001b[0m encoder_kwargs[model_input_name] \u001b[38;5;241m=\u001b[39m inputs_tensor\n\u001b[0;32m--> 617\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]: ModelOutput \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoder_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/MyWork/VIP5_Shadowcast_DPA/notebooks/modeling_vip5.py:820\u001b[0m, in \u001b[0;36mJointEncoder.forward\u001b[0;34m(self, input_ids, whole_word_ids, category_ids, vis_feats, attention_mask, inputs_embeds, head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, task)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# Merge vis_embeds into inputs_embeds (use category_ids, vis_embeds, inputs_embeds)\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(B):\n\u001b[0;32m--> 820\u001b[0m     inputs_embeds[i][torch\u001b[38;5;241m.\u001b[39mwhere(\u001b[43mcategory_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m)[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m vis_embeds[i][:\u001b[38;5;28mlen\u001b[39m(torch\u001b[38;5;241m.\u001b[39mwhere(category_ids[i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m])]\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# Add whole_word_embeds & category_embeds\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m whole_word_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 12: Evaluation - Sequential 任务（带 Prompt 信息）\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# 如果 args.load 不为空，则从其中提取日期，否则使用当前日期\n",
    "if args.load is not None:\n",
    "    eval_date = Path(args.load).parents[1].name\n",
    "else:\n",
    "    eval_date = datetime.now().strftime(\"%m%d\")\n",
    "\n",
    "# 指定 Sequential 任务的测试任务与 Prompt\n",
    "test_task_list = {'sequential': ['A-3']}  # 例：可选 'A-9', 'A-3' 等\n",
    "prompt = test_task_list['sequential'][0]\n",
    "\n",
    "test_sample_numbers = {\n",
    "    'sequential': (1, 1),\n",
    "    'direct': (1, 1),\n",
    "    'explanation': 1\n",
    "}\n",
    "\n",
    "# 获取 Sequential 任务的测试数据加载器\n",
    "zeroshot_test_loader = get_loader(\n",
    "    args,\n",
    "    test_task_list,\n",
    "    test_sample_numbers,\n",
    "    split=args.test,\n",
    "    mode='test',\n",
    "    batch_size=args.batch_size,\n",
    "    workers=args.num_workers,\n",
    "    distributed=args.distributed,\n",
    "    data_root=\"../data\",\n",
    "    feature_root=\"../features\"\n",
    ")\n",
    "\n",
    "print(f\"Sequential 任务 (Prompt: {prompt}) 数据量:\", len(zeroshot_test_loader))\n",
    "\n",
    "# 收集所有样本的 ground truth 和模型输出\n",
    "all_info = []\n",
    "for i, batch in tqdm(enumerate(zeroshot_test_loader), total=len(zeroshot_test_loader)):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        beam_outputs = model.generate(\n",
    "            input_ids=batch['input_ids'].to('cuda'),\n",
    "            whole_word_ids=batch['whole_word_ids'].to('cuda'),\n",
    "            category_ids=batch['category_ids'].to('cuda'),\n",
    "            vis_feats=batch['vis_feats'].to('cuda'),\n",
    "            task=batch[\"task\"][0],\n",
    "            max_length=50,\n",
    "            num_beams=20,\n",
    "            no_repeat_ngram_size=0,\n",
    "            num_return_sequences=20,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        generated_sents = model.tokenizer.batch_decode(beam_outputs, skip_special_tokens=True)\n",
    "\n",
    "        # 遍历当前批次中每个样本（每个样本生成20个候选）\n",
    "        for j in range(len(batch['target_text'])):\n",
    "            all_info.append({\n",
    "                'target_item': batch['target_text'][j],\n",
    "                'gen_item_list': generated_sents[j * 20: (j + 1) * 20]\n",
    "            })\n",
    "\n",
    "# 构造 ground truth 字典和用户-物品分数字典\n",
    "gt = {}\n",
    "ui_scores = {}\n",
    "for i, info in enumerate(all_info):\n",
    "    gt[i] = [int(info['target_item'])]\n",
    "    pred_dict = {}\n",
    "    for j, pred in enumerate(info['gen_item_list']):\n",
    "        try:\n",
    "            pred_dict[int(pred)] = -(j + 1)\n",
    "        except:\n",
    "            pass\n",
    "    ui_scores[i] = pred_dict\n",
    "\n",
    "# 定义 exposure target 集合\n",
    "targeted_items = gt.copy()\n",
    "\n",
    "# 计算推荐指标 + ER@K\n",
    "msg_top1,  res_top1  = evaluate_all(ui_scores, gt, topk=1,  targeted_items=targeted_items)\n",
    "msg_top5,  res_top5  = evaluate_all(ui_scores, gt, topk=5,  targeted_items=targeted_items)\n",
    "msg_top10, res_top10 = evaluate_all(ui_scores, gt, topk=10, targeted_items=targeted_items)\n",
    "\n",
    "print(\"\\nEvaluation Metrics at top-1:\")\n",
    "print(msg_top1)\n",
    "print(\"ER@1: {:.4f}\".format(res_top1['er']))\n",
    "\n",
    "print(\"\\nEvaluation Metrics at top-5:\")\n",
    "print(msg_top5)\n",
    "print(\"ER@5: {:.4f}\".format(res_top5['er']))\n",
    "\n",
    "print(\"\\nEvaluation Metrics at top-10:\")\n",
    "print(msg_top10)\n",
    "print(\"ER@10: {:.4f}\".format(res_top10['er']))\n",
    "\n",
    "# 保存 Sequential 任务评价结果到文件，文件名包含 Prompt\n",
    "eval_dir = f\"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log/{args.split}/{eval_date}/evaluation_logs\"\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "\n",
    "sequential_filename = (\n",
    "    f\"VIP5_{args.split}_\"\n",
    "    f\"{args.image_feature_type}_\"\n",
    "    f\"{args.reduction_factor}_\"\n",
    "    f\"{args.epoch}_evaluation_sequential_{prompt}.txt\"\n",
    ")\n",
    "sequential_log_path = os.path.join(eval_dir, sequential_filename)\n",
    "\n",
    "with open(sequential_log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Sequential Evaluation Results\\n\")\n",
    "    f.write(f\"Prompt: {prompt}\\n\\n\")\n",
    "    f.write(\"Metrics @1:\\n\")\n",
    "    f.write(msg_top1 + \"\\n\")\n",
    "    f.write(f\"ER@1:  {res_top1['er']:.4f}\\n\\n\")\n",
    "    f.write(\"Metrics @5:\\n\")\n",
    "    f.write(msg_top5 + \"\\n\")\n",
    "    f.write(f\"ER@5:  {res_top5['er']:.4f}\\n\\n\")\n",
    "    f.write(\"Metrics @10:\\n\")\n",
    "    f.write(msg_top10 + \"\\n\")\n",
    "    f.write(f\"ER@10: {res_top10['er']:.4f}\\n\")\n",
    "\n",
    "print(f\"Sequential 任务 (Prompt: {prompt}) 评价结果已保存至: {sequential_log_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vip5_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
