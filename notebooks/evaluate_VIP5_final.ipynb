{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 1: 环境设置与路径配置\n",
    "\n",
    "功能说明：\n",
    "将项目根目录添加到 Python 模块搜索路径中，确保后续能够正确加载项目内部模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Current working directory: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA\n",
      "→ First entries in sys.path: ['/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/notebooks', '/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/src', '/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA']\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: 环境设置与路径配置\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 项目根目录\n",
    "project_path   = \"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA\"\n",
    "# src/ 子模块目录\n",
    "src_path       = os.path.join(project_path, \"src\")\n",
    "# notebooks/ 目录（evaluate 模块就在这里）\n",
    "notebooks_path = os.path.join(project_path, \"notebooks\")\n",
    "\n",
    "# 1) 把 project_root、src/ 和 notebooks/ 都加入到 sys.path，\n",
    "#    这样后续 import src.* 和 import evaluate.* 都能正常工作\n",
    "for p in (project_path, src_path, notebooks_path):\n",
    "    if p not in sys.path:\n",
    "        sys.path.insert(0, p)\n",
    "\n",
    "# 2) 切到项目根目录，这样后续 open(\"config_eval.yaml\")、get_loader(data_root=\"data\") 等都可以直接用相对路径\n",
    "os.chdir(project_path)\n",
    "\n",
    "print(\"→ Current working directory:\", os.getcwd())\n",
    "print(\"→ First entries in sys.path:\", sys.path[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 2: 导入依赖库与模块\n",
    "\n",
    "功能说明：\n",
    "导入所有需要的第三方库和项目内部模块。注意部分模块（如 P5Tokenizer）在后续 cell 中会用到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有依赖库已导入\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: 导入依赖库与模块\n",
    "import collections\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from packaging import version\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gzip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# 导入项目内部模块\n",
    "from src.param import parse_args\n",
    "from src.utils import LossMeter, load_state_dict, set_global_logging_level\n",
    "from src.dist_utils import reduce_dict\n",
    "from transformers import T5Tokenizer\n",
    "from src.tokenization import P5Tokenizer\n",
    "from src.model import VIP5Tuning\n",
    "from src.trainer_base import TrainerBase\n",
    "\n",
    "# 判断是否使用 native AMP 或 Apex\n",
    "_use_native_amp = False\n",
    "_use_apex = False\n",
    "if version.parse(torch.__version__) < version.parse(\"1.6\"):\n",
    "    from transormers.file_utils import is_apex_available\n",
    "    if is_apex_available():\n",
    "        from apex import amp\n",
    "    _use_apex = True\n",
    "else:\n",
    "    _use_native_amp = True\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "print(\"所有依赖库已导入\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 3: 定义辅助函数\n",
    "\n",
    "功能说明：\n",
    "定义常用的辅助函数，如 pickle、json 的加载函数，以及文件读取函数等，方便后续调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "辅助函数定义完成\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: 定义辅助函数\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_pickle(data, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def ReadLineFromFile(path):\n",
    "    lines = []\n",
    "    with open(path, 'r') as fd:\n",
    "        for line in fd:\n",
    "            lines.append(line.rstrip('\\n'))\n",
    "    return lines\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "print(\"辅助函数定义完成\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 4: 定义 DotDict 类及参数设置\n",
    "\n",
    "功能说明：\n",
    "定义一个 DotDict 类，使得可以通过属性方式访问字典中的值；并设置所有实验参数、随机种子等，保证实验结果可复现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Parsed from checkpoint path:\n",
      "  attack_mode=ShadowCastAttack, mr=0.3\n",
      "  split=clothing, feat=vitb32, size_ratio=2, reduction=8, epoch=20\n",
      "✔️ Wrote temporary config_eval.yaml: {'experiment': {'suffix': 'ShadowCastAttack', 'mr': 0.3}}\n",
      "✔️ 完整 args 配置：\n",
      "{'load': '/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/snap/clothing/0816/ShadowCastAttack_0.3_clothing-vitb32-2-8-20/BEST_EVAL_LOSS.pth', 'attack_mode': 'ShadowCastAttack', 'mr': 0.3, 'split': 'clothing', 'train': 'clothing', 'valid': 'clothing', 'test': 'clothing', 'image_feature_type': 'vitb32', 'image_feature_size_ratio': 2, 'reduction_factor': 8, 'epoch': 20, 'distributed': False, 'multiGPU': True, 'fp16': True, 'batch_size': 16, 'optim': 'adamw', 'warmup_ratio': 0.1, 'lr': 0.001, 'num_workers': 4, 'clip_grad_norm': 5.0, 'losses': 'sequential,direct,explanation', 'backbone': 't5-small', 'comment': '', 'local_rank': 0, 'data_target': {'beauty': [2], 'clothing': [8], 'sports': [53], 'toys': [62]}, 'use_adapter': True, 'use_single_adapter': True, 'use_vis_layer_norm': True, 'add_adapter_cross_attn': True, 'use_lm_head_adapter': True, 'tokenizer': 'p5', 'max_text_length': 1024, 'gen_max_length': 64, 'do_lower_case': False, 'dropout': 0.1, 'weight_decay': 0.01, 'adam_eps': 1e-06, 'gradient_accumulation_steps': 1, 'seed': 2022, 'whole_word_embed': True, 'category_embed': True, 'world_size': 4, 'LOSSES_NAME': ['sequential_loss', 'direct_loss', 'explanation_loss', 'total_loss']}\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: 设置参数与随机种子\n",
    "import re\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "class DotDict(dict):\n",
    "    def __init__(self, **kwds):\n",
    "        super().__init__(**kwds)\n",
    "        self.__dict__ = self\n",
    "    def __repr__(self):\n",
    "        return dict.__repr__(self)\n",
    "\n",
    "# 构造参数对象\n",
    "args = DotDict()\n",
    "\n",
    "# ──── 1) checkpoint 路径 ────────────────────────────────────────────────\n",
    "args.load = (\n",
    "    \"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/snap/clothing/0816/ShadowCastAttack_0.3_clothing-vitb32-2-8-20/BEST_EVAL_LOSS.pth\"\n",
    ")\n",
    "\n",
    "# ──── 2) 自动解析 attack_mode / mr / split / feat / size_ratio / reduction / epoch ─────────\n",
    "ckpt_folder = Path(args.load).parent.name\n",
    "# 现在 ckpt_folder = \"NoAttack_0.0_toys-vitb32-2-8-20\"\n",
    "mode, mr_str, rest = ckpt_folder.split(\"_\", 2)\n",
    "args.attack_mode = mode\n",
    "args.mr = float(mr_str)\n",
    "\n",
    "# rest 里再拆： dataset=toys, img_feat=vitb32, size_ratio=2, reduction=8, epoch=20\n",
    "dataset, img_feat, size_ratio, reduction, epoch = rest.split(\"-\")\n",
    "args.split = dataset\n",
    "args.train = args.valid = args.test = dataset\n",
    "\n",
    "args.image_feature_type       = img_feat\n",
    "args.image_feature_size_ratio = int(size_ratio)\n",
    "args.reduction_factor         = int(reduction)\n",
    "args.epoch                    = int(epoch)\n",
    "\n",
    "print(\"✔️ Parsed from checkpoint path:\")\n",
    "print(f\"  attack_mode={args.attack_mode}, mr={args.mr}\")\n",
    "print(f\"  split={args.split}, feat={args.image_feature_type},\",\n",
    "      f\"size_ratio={args.image_feature_size_ratio},\",\n",
    "      f\"reduction={args.reduction_factor}, epoch={args.epoch}\")\n",
    "\n",
    "# ──── 3) 写临时 config_eval.yaml，供 VIP5_Dataset 读取 ──────────────────────────────\n",
    "cfg = {\"experiment\": {\"suffix\": args.attack_mode, \"mr\": args.mr}}\n",
    "with open(\"config_eval.yaml\", \"w\") as f:\n",
    "    yaml.safe_dump(cfg, f)\n",
    "print(\"✔️ Wrote temporary config_eval.yaml:\", cfg)\n",
    "\n",
    "# ──── 4) 其余静态参数 ────────────────────────────────────────────────\n",
    "args.distributed = False\n",
    "args.multiGPU    = True\n",
    "args.fp16        = True\n",
    "\n",
    "args.batch_size      = 16\n",
    "args.optim           = 'adamw'\n",
    "args.warmup_ratio    = 0.1\n",
    "args.lr              = 1e-3\n",
    "args.num_workers     = 4\n",
    "args.clip_grad_norm  = 5.0\n",
    "args.losses          = 'sequential,direct,explanation'\n",
    "args.backbone        = 't5-small'\n",
    "args.comment         = ''    # ← 新增\n",
    "args.local_rank      = 0         # ← 在这里新增\n",
    "\n",
    "# 数据集目标物品（你现有的逻辑）\n",
    "args.data_target = {}\n",
    "for ds in [\"beauty\", \"clothing\", \"sports\", \"toys\"]:\n",
    "    path = (\n",
    "        f\"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/\"\n",
    "        f\"analysis/results/{ds}/low_pop_items_{ds}_lowcount_1.txt\"\n",
    "    )\n",
    "    ids = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as fin:\n",
    "        for line in fin:\n",
    "            m = re.search(r\"\\(ID:\\s*(\\d+)\\)\", line)\n",
    "            if m:\n",
    "                ids.append(int(m.group(1)))\n",
    "    if not ids:\n",
    "        raise RuntimeError(f\"{ds} 没有解析到任何 ID，请检查 {path}\")\n",
    "    args.data_target[ds] = ids\n",
    "\n",
    "# 模型＋视觉特征\n",
    "args.use_adapter            = True\n",
    "args.use_single_adapter     = True\n",
    "args.use_vis_layer_norm     = True\n",
    "args.add_adapter_cross_attn = True\n",
    "args.use_lm_head_adapter    = True\n",
    "\n",
    "# 文本长度／dropout／tokenizer\n",
    "args.tokenizer               = 'p5'\n",
    "args.max_text_length         = 1024\n",
    "args.gen_max_length          = 64\n",
    "args.do_lower_case           = False\n",
    "args.dropout                 = 0.1\n",
    "args.weight_decay            = 0.01\n",
    "args.adam_eps                = 1e-6\n",
    "args.gradient_accumulation_steps = 1\n",
    "\n",
    "# 随机种子\n",
    "args.seed = 2022\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "# Whole Word & Category Embedding\n",
    "args.whole_word_embed = True\n",
    "args.category_embed   = True\n",
    "\n",
    "# cudnn & GPU\n",
    "cudnn.benchmark     = True\n",
    "args.world_size     = torch.cuda.device_count()\n",
    "\n",
    "# 损失名称列表\n",
    "LOSSES_NAME = [f'{n}_loss' for n in args.losses.split(',')] + ['total_loss']\n",
    "args.LOSSES_NAME = LOSSES_NAME\n",
    "\n",
    "print(\"✔️ 完整 args 配置：\")\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 5: GPU设置与生成运行名称\n",
    "\n",
    "功能说明：\n",
    "指定使用的 GPU（手动设置），并构造一个运行名称（run_name），便于后续日志及保存结果区分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Launching at GPU 1\n",
      "运行名称: 0817_GPU4_clothing_t5-small_sequentialdirectexplanation\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: GPU设置与生成运行名称\n",
    "# 功能：指定 GPU（手动设置），并构造一个运行名称\n",
    "\n",
    "# 手动指定 GPU ID\n",
    "gpu = 1\n",
    "args.gpu = gpu\n",
    "args.rank = gpu\n",
    "print(f'Process Launching at GPU {gpu}')\n",
    "\n",
    "# 设置当前 GPU 设备\n",
    "torch.cuda.set_device(f'cuda:{gpu}')\n",
    "\n",
    "# 构造运行名称\n",
    "comments = []\n",
    "dsets = []\n",
    "if 'toys' in args.train:\n",
    "    dsets.append('toys')\n",
    "if 'beauty' in args.train:\n",
    "    dsets.append('beauty')\n",
    "if 'sports' in args.train:\n",
    "    dsets.append('sports')\n",
    "if 'clothing' in args.train:\n",
    "    dsets.append('clothing')\n",
    "comments.append(''.join(dsets))\n",
    "if args.backbone:\n",
    "    comments.append(args.backbone)\n",
    "comments.append(''.join(args.losses.split(',')))\n",
    "if args.comment != '':\n",
    "    comments.append(args.comment)\n",
    "comment = '_'.join(comments)\n",
    "\n",
    "from datetime import datetime\n",
    "current_time = datetime.now().strftime('%m%d')  # 例如 '0304'\n",
    "\n",
    "if args.local_rank in [0, -1]:\n",
    "    run_name = f'{current_time}_GPU{args.world_size}'\n",
    "    if len(comments) > 0:\n",
    "        run_name += f'_{comment}'\n",
    "    args.run_name = run_name\n",
    "    print(\"运行名称:\", args.run_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 6: 构建模型配置、Tokenizer 与模型\n",
    "\n",
    "功能说明：\n",
    "根据参数构建模型配置（config）、创建 Tokenizer，并加载预训练模型。\n",
    "注意：由于 checkpoint 使用的是 T5Tokenizer，而我们调用 P5Tokenizer，所以会有警告信息，但功能不受影响。\n",
    "另外，为了适配 adapter，需要将 config.d_model 赋值给 adapter_config。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: <class 'src.tokenization.P5Tokenizer'> t5-small\n",
      "→ 正在从预训练模型 't5-small' 初始化 VIP5Tuning 结构…\n",
      "JointEncoder initialized successfully.\n",
      "T5Stack initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VIP5Tuning were not initialized from the model checkpoint at t5-small and are newly initialized: ['decoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'output_adapter.adapter.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.visual_embedding.feat_embedding.0.model.0.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.category_embeddings.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'output_adapter.adapter.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.visual_embedding.feat_embedding.0.model.2.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.visual_embedding.feat_embedding.0.model.0.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'output_adapter.adapter.up_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.whole_word_embeddings.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'output_adapter.adapter.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.visual_embedding.feat_embedding.0.model.2.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.visual_embedding.feat_embedding.1.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.down_sampler.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_head initialized successfully.\n",
      "OutputParallelAdapterLayer initialized successfully.\n",
      "AdapterConfig: AdapterConfig(add_layer_norm_before_adapter=False, add_layer_norm_after_adapter=False, non_linearity='gelu_new', reduction_factor=8)\n",
      "✔️ 模型结构与 Tokenizer 初始化完成，下一步 Cell 7 再加载你的 .pth 权重\n"
     ]
    }
   ],
   "source": [
    "# ──── Cell 6: 构建模型配置、Tokenizer 与模型 ────────────────────────\n",
    "import re\n",
    "from transformers import T5Config, T5Tokenizer\n",
    "from adapters import AdapterConfig\n",
    "from src.tokenization import P5Tokenizer\n",
    "from src.model import VIP5Tuning\n",
    "\n",
    "# ──── Monkey-patch：给 VIP5Tuning 增加一个 model 属性，指向自身 ─────────────\n",
    "# 这样 VIP5.__init__ 里 self.model.shared 就能正常访问 self.shared\n",
    "VIP5Tuning.model = property(lambda self: self)\n",
    "\n",
    "def create_config(args):\n",
    "    # 1) 从 backbone pretrained 拿到基础 config\n",
    "    config = T5Config.from_pretrained(args.backbone)\n",
    "    # 2) 把所有我们在 args 里写的字段都塞进 config\n",
    "    for k, v in vars(args).items():\n",
    "        setattr(config, k, v)\n",
    "    config.non_linearity = \"relu\"\n",
    "\n",
    "    # 3) 视觉特征维度映射\n",
    "    dim_map = {\n",
    "        'vitb32': 512, 'vitb16': 512, 'vitl14': 768,\n",
    "        'rn50': 1024, 'rn101': 512,\n",
    "    }\n",
    "    config.feat_dim           = dim_map[args.image_feature_type]\n",
    "    config.n_vis_tokens       = args.image_feature_size_ratio\n",
    "    config.use_vis_layer_norm = args.use_vis_layer_norm\n",
    "    config.reduction_factor   = args.reduction_factor\n",
    "\n",
    "    # 4) Adapter 相关开关\n",
    "    config.use_adapter            = args.use_adapter\n",
    "    config.add_adapter_cross_attn = args.add_adapter_cross_attn\n",
    "    config.use_lm_head_adapter    = args.use_lm_head_adapter\n",
    "    config.use_single_adapter     = args.use_single_adapter\n",
    "    config.dropout_rate           = args.dropout\n",
    "    config.attention_dropout      = args.dropout\n",
    "    config.activation_dropout     = args.dropout\n",
    "    config.losses                 = args.losses\n",
    "\n",
    "    if args.use_adapter:\n",
    "        tasks = re.split(\"[, ]+\", args.losses)\n",
    "        adapter_cfg = AdapterConfig()\n",
    "        adapter_cfg.tasks             = tasks\n",
    "        adapter_cfg.d_model           = config.d_model\n",
    "        adapter_cfg.use_single_adapter= args.use_single_adapter\n",
    "        adapter_cfg.reduction_factor  = args.reduction_factor\n",
    "        adapter_cfg.track_z           = False\n",
    "        config.adapter_config        = adapter_cfg\n",
    "    else:\n",
    "        config.adapter_config = None\n",
    "\n",
    "    return config\n",
    "\n",
    "def create_tokenizer(args):\n",
    "    # 根据 args.tokenizer 决定用 P5Tokenizer 还是 T5Tokenizer\n",
    "    if 'p5' in args.tokenizer:\n",
    "        tok_cls = P5Tokenizer\n",
    "    else:\n",
    "        tok_cls = T5Tokenizer\n",
    "\n",
    "    tokenizer = tok_cls.from_pretrained(\n",
    "        args.backbone,\n",
    "        max_length=args.max_text_length,\n",
    "        do_lower_case=args.do_lower_case\n",
    "    )\n",
    "    print(\"Tokenizer:\", tok_cls, args.backbone)\n",
    "    return tokenizer\n",
    "\n",
    "def create_model(args, config):\n",
    "    # 用 from_pretrained 搭好所有底层组件（包括 self.shared, self.encoder, adapter 层…）\n",
    "    print(f\"→ 正在从预训练模型 '{args.backbone}' 初始化 VIP5Tuning 结构…\")\n",
    "    model = VIP5Tuning.from_pretrained(\n",
    "        args.backbone,\n",
    "        config=config\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# —— 真·执行三步 —— \n",
    "config    = create_config(args)\n",
    "tokenizer = create_tokenizer(args)\n",
    "model     = create_model(args, config).cuda()\n",
    "\n",
    "# 如果用的是 P5Tokenizer，就扩增词表\n",
    "if 'p5' in args.tokenizer:\n",
    "    model.resize_token_embeddings(tokenizer.vocab_size)\n",
    "\n",
    "# 挂上 tokenizer，方便后续 decode\n",
    "model.tokenizer = tokenizer\n",
    "\n",
    "print(\"✔️ 模型结构与 Tokenizer 初始化完成，下一步 Cell 7 再加载你的 .pth 权重\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 7: 加载预训练模型权重\n",
    "\n",
    "功能说明：\n",
    "从指定 checkpoint 路径加载预训练模型权重，并打印加载结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading checkpoint from: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/snap/clothing/0816/ShadowCastAttack_0.3_clothing-vitb32-2-8-20/BEST_EVAL_LOSS.pth\n",
      "ℹ️  load_state_dict 结果：\n",
      "_IncompatibleKeys(missing_keys=['output_adapter.adapter.down_sampler.weight', 'output_adapter.adapter.down_sampler.bias', 'output_adapter.adapter.up_sampler.weight', 'output_adapter.adapter.up_sampler.bias'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: 加载预训练模型权重\n",
    "from pprint import pprint\n",
    "from src.utils import load_state_dict\n",
    "\n",
    "def load_checkpoint(ckpt_path):\n",
    "    if not ckpt_path.endswith('.pth'):\n",
    "        ckpt_path += '.pth'\n",
    "    print(f\"📥 Loading checkpoint from: {ckpt_path}\")\n",
    "    state_dict = load_state_dict(ckpt_path, 'cpu')\n",
    "    res = model.load_state_dict(state_dict, strict=False)\n",
    "    print(\"ℹ️  load_state_dict 结果：\")\n",
    "    pprint(res)\n",
    "\n",
    "# 直接用 Cell 4 里设置好的 args.load\n",
    "load_checkpoint(args.load)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 8: 加载数据集及数据映射\n",
    "\n",
    "功能说明：\n",
    "加载数据分割文件（如 rating_splits_augmented.pkl）以及数据映射文件（datamaps.json），用于后续评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data长度: 27867\n",
      "Test data示例: {'reviewerID': 'AQVU2X4NK5V31', 'asin': 'B004L4B7HG', 'reviewerName': 'Michael Duffy', 'helpful': [0, 3], 'reviewText': 'I loved the look of this shoe.  certainly better constructed than the Minimus but a different fit completely.  harder to break in.  another dust collector in my closet.', 'overall': 3.0, 'summary': 'Merrell Trail Glove Running', 'unixReviewTime': 1368144000, 'reviewTime': '05 10, 2013', 'explanation': 'certainly better constructed than the Minimus but a different fit completely', 'feature': 'fit'}\n",
      "用户数量: 39387\n",
      "物品数量: 23033\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: 加载数据集及数据映射\n",
    "# 功能：加载 rating_splits_augmented.pkl 和 datamaps.json 数据文件\n",
    "\n",
    "\n",
    "data_splits = load_pickle(f'data/{args.split}/rating_splits_augmented.pkl')\n",
    "test_review_data = data_splits['test']\n",
    "print(\"Test data长度:\", len(test_review_data))\n",
    "print(\"Test data示例:\", test_review_data[0])\n",
    "\n",
    "data_maps = load_json(os.path.join('data', args.split, 'datamaps.json'))\n",
    "print(\"用户数量:\", len(data_maps['user2id']))\n",
    "print(\"物品数量:\", len(data_maps['item2id']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 9: 加载数据生成器与评价指标\n",
    "\n",
    "功能说明：\n",
    "导入数据加载函数和评价指标函数，为后续评估生成数据加载器和计算 BLEU/ROUGE 等指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据加载器与评价指标函数已导入\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: 导入数据加载器与评价指标函数\n",
    "# 功能：导入 get_loader、BLEU、ROUGE 等评价指标函数\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "from src.data import get_loader\n",
    "from evaluate.utils import rouge_score, bleu_score, unique_sentence_percent, root_mean_square_error, mean_absolute_error, feature_detect, feature_matching_ratio, feature_coverage_ratio, feature_diversity\n",
    "from evaluate.metrics4rec import evaluate_all\n",
    "\n",
    "print(\"数据加载器与评价指标函数已导入\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 10: Evaluation - Explanation 任务\n",
    "\n",
    "功能说明：\n",
    "加载 explanation 任务的数据生成器，调用模型生成输出，并计算 BLEU、ROUGE 指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading checkpoint from:\", args.load)\n",
    "\n",
    "# =============================================================================\n",
    "# Cell 10: Evaluation - Explanation 任务（带 Prompt 信息）\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# 如果 args.load 不为空，则从其中提取日期，否则使用当前日期\n",
    "if args.load is not None:\n",
    "    eval_date = Path(args.load).parents[1].name\n",
    "else:\n",
    "    eval_date = datetime.now().strftime(\"%m%d\")\n",
    "\n",
    "# 指定 Explanation 任务的 prompt 及样本数量\n",
    "exp_prompt = 'C-3'  # 可修改为 'C-12', 'C-3' 等所需的 prompt 编号\n",
    "test_task_list = {'explanation': [exp_prompt]}\n",
    "test_sample_numbers = {'sequential': (1, 1), 'direct': (1, 1), 'explanation': 1}\n",
    "\n",
    "# 获取 Explanation 任务的测试数据加载器\n",
    "zeroshot_test_loader = get_loader(\n",
    "    args,\n",
    "    test_task_list,\n",
    "    test_sample_numbers,\n",
    "    split=args.test, \n",
    "    mode='test', \n",
    "    batch_size=args.batch_size,\n",
    "    workers=args.num_workers,\n",
    "    distributed=args.distributed,\n",
    "    data_root=\"data\",\n",
    "    feature_root=\"features\"\n",
    ")\n",
    "print(f\"Explanation 任务 (Prompt: {exp_prompt}) 数据量:\", len(zeroshot_test_loader))\n",
    "\n",
    "tokens_predict = []\n",
    "tokens_test = []\n",
    "\n",
    "# 遍历测试数据加载器，调用模型生成预测结果\n",
    "for _, batch in tqdm(enumerate(zeroshot_test_loader), total=len(zeroshot_test_loader), ncols=100):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        tokens_predict.extend(results)\n",
    "        tokens_test.extend(batch['target_text'])\n",
    "\n",
    "# 计算 BLEU 与 ROUGE 指标\n",
    "BLEU1 = bleu_score(tokens_test, tokens_predict, n_gram=1, smooth=False)\n",
    "BLEU4 = bleu_score(tokens_test, tokens_predict, n_gram=4, smooth=False)\n",
    "ROUGE = rouge_score(tokens_test, tokens_predict)\n",
    "\n",
    "print(f'BLEU-1 {BLEU1:7.4f}, BLEU-4 {BLEU4:7.4f}')\n",
    "for k, v in ROUGE.items():\n",
    "    print(f'{k} {v:7.4f}')\n",
    "\n",
    "# 构建保存评估结果的目录和文件名\n",
    "eval_dir = Path(\"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log\") \\\n",
    "           / args.split / eval_date / \"evaluation_logs\"\n",
    "eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 保持与训练时相同的 base_name\n",
    "suffix = args.attack_mode\n",
    "mr = args.mr\n",
    "dataset = args.split\n",
    "img_feat = args.image_feature_type\n",
    "reduction = args.reduction_factor\n",
    "epoch = args.epoch\n",
    "base_name = f\"{suffix}_{mr}_VIP5_{dataset}_{img_feat}_{reduction}_{epoch}\"\n",
    "\n",
    "explanation_filename = f\"{base_name}_eval_explanation_{exp_prompt}.txt\"\n",
    "explanation_log_path = eval_dir / explanation_filename\n",
    "\n",
    "# 保存评估结果，文件头加入 Dataset、AttackMode 和 MaliciousRatio\n",
    "with open(explanation_log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Explanation Evaluation Results\\n\")\n",
    "    f.write(f\"Dataset: {dataset}\\n\")\n",
    "    f.write(f\"AttackMode: {args.attack_mode}\\n\")\n",
    "    f.write(f\"MaliciousRatio: {args.mr}\\n\")\n",
    "    f.write(f\"Prompt: {exp_prompt}\\n\\n\")\n",
    "    f.write(f\"BLEU-1: {BLEU1:7.4f}\\n\")\n",
    "    f.write(f\"BLEU-4: {BLEU4:7.4f}\\n\")\n",
    "    for k, v in ROUGE.items():\n",
    "        f.write(f\"{k}: {v:7.4f}\\n\")\n",
    "\n",
    "print(f\"Explanation 任务 (Prompt: {exp_prompt}) 评价结果已保存至: {explanation_log_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 11: Evaluation - Direct 任务\n",
    "\n",
    "功能说明：\n",
    "加载 direct 任务的测试数据，生成输出并计算评价指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VIP5_Dataset] mode=test, attack_mode=ShadowCastAttack, mr=0.3\n",
      "[DEBUG] exp_splits_path = data/clothing/exp_splits.pkl\n",
      "[DEBUG] seq_path        = data/clothing/sequential_data.txt\n",
      "[DEBUG] idx_path        = data/clothing/user_id2idx.pkl\n",
      "[DEBUG] name_path       = data/clothing/user_id2name.pkl\n",
      "[WARN] Clean val/test 模式下，动态构建了 53278 个用户映射\n",
      "compute_datum_info\n",
      "攻击模式：ShadowCastAttack，恶意比例：0.3\n",
      "Direct 任务 (Prompt: B-8) 数据量: 2462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2462/2462 [35:58<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39387 cnt of users\n",
      "\n",
      "NDCG@1\tRec@1\tHits@1\tPrec@1\tMAP@1\tMRR@1\tER@1\n",
      "0.0291\t0.0291\t0.0291\t0.0291\t0.0291\t0.0291\t0.0008\n",
      "39387 cnt of users\n",
      "\n",
      "NDCG@5\tRec@5\tHits@5\tPrec@5\tMAP@5\tMRR@5\tER@5\n",
      "0.0619\t0.0953\t0.0953\t0.0191\t0.0510\t0.0510\t0.0013\n",
      "39387 cnt of users\n",
      "\n",
      "NDCG@10\tRec@10\tHits@10\tPrec@10\tMAP@10\tMRR@10\tER@10\n",
      "0.0858\t0.1702\t0.1702\t0.0170\t0.0608\t0.0608\t0.0015\n",
      "Direct 结果已保存至: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log/clothing/0816/evaluation_logs/ShadowCastAttack_0.3_VIP5_clothing_vitb32_8_20_eval_direct_B-8.txt\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 11: Evaluation - Direct 任务（带 Prompt 信息）\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "# 1. 确定 eval_date：若从 checkpoint 恢复则取目录名，否则用当前日期\n",
    "if args.load is not None:\n",
    "    eval_date = Path(args.load).parents[1].name\n",
    "else:\n",
    "    eval_date = datetime.now().strftime(\"%m%d\")\n",
    "\n",
    "# 2. 指定 Direct 任务的 Prompt\n",
    "test_task_list = {'direct': ['B-8']}  # 可选 'B-5' 或 'B-8'\n",
    "prompt = test_task_list['direct'][0]\n",
    "\n",
    "test_sample_numbers = {\n",
    "    'sequential': (1, 1),\n",
    "    'direct': (1, 1),\n",
    "    'explanation': 1\n",
    "}\n",
    "\n",
    "# 3. 获取 Direct 测试 Loader\n",
    "zeroshot_test_loader = get_loader(\n",
    "    args,\n",
    "    test_task_list,\n",
    "    test_sample_numbers,\n",
    "    split=args.test,\n",
    "    mode='test',\n",
    "    batch_size=args.batch_size,\n",
    "    workers=args.num_workers,\n",
    "    distributed=args.distributed,\n",
    "    data_root=\"data\",\n",
    "    feature_root=\"features\"\n",
    ")\n",
    "\n",
    "print(f\"攻击模式：{args.attack_mode}，恶意比例：{args.mr}\")\n",
    "print(f\"Direct 任务 (Prompt: {prompt}) 数据量:\", len(zeroshot_test_loader))\n",
    "\n",
    "# 4. 收集所有样本的 GT 与模型输出\n",
    "all_info = []\n",
    "for _, batch in tqdm(enumerate(zeroshot_test_loader), total=len(zeroshot_test_loader)):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        beam_outputs = model.generate(\n",
    "            input_ids=batch['input_ids'].cuda(),\n",
    "            whole_word_ids=batch['whole_word_ids'].cuda(),\n",
    "            category_ids=batch['category_ids'].cuda(),\n",
    "            vis_feats=batch['vis_feats'].cuda(),\n",
    "            task=batch[\"task\"][0],\n",
    "            max_length=50,\n",
    "            num_beams=20,\n",
    "            no_repeat_ngram_size=0,\n",
    "            num_return_sequences=20,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        generated_sents = model.tokenizer.batch_decode(beam_outputs, skip_special_tokens=True)\n",
    "\n",
    "        # for j, (_, tgt_text, _) in enumerate(zip(results, batch['target_text'], batch['source_text'])):\n",
    "        #     all_info.append({\n",
    "        #         'target_item': tgt_text,\n",
    "        #         'gen_item_list': generated_sents[j * 20: (j + 1) * 20]\n",
    "        #     })\n",
    "        for j, item in enumerate(zip(results, batch['target_text'], batch['source_text'])):\n",
    "            new_info = {}\n",
    "            new_info['target_item'] = item[1]\n",
    "            new_info['gen_item_list'] = generated_sents[j*20: (j+1)*20]\n",
    "            all_info.append(new_info)\n",
    "\n",
    "# 5. 构造 GT 与评分字典\n",
    "gt = {}\n",
    "ui_scores = {}\n",
    "for i, info in enumerate(all_info):\n",
    "    gt[i] = [int(info['target_item'])]\n",
    "    pred_dict = {}\n",
    "    for j in range(len(info['gen_item_list'])):\n",
    "        try:\n",
    "            pred_dict[int(info['gen_item_list'][j])] = -(j+1)\n",
    "        except:\n",
    "            pass\n",
    "    ui_scores[i] = pred_dict\n",
    "\n",
    "# 6. 定义用于 ER@K 的目标集合\n",
    "targeted_items = args.data_target[args.split] \n",
    "\n",
    "# 7. 计算指标 + ER@K\n",
    "msg1, res1 = evaluate_all(ui_scores, gt, targeted_items, 1)\n",
    "msg5, res5 = evaluate_all(ui_scores, gt, targeted_items, 5)\n",
    "msg10, res10 = evaluate_all(ui_scores, gt,targeted_items, 10)\n",
    "\n",
    "\n",
    "# print(\"\\nMetrics @1:\")\n",
    "# print(msg1)\n",
    "# # print(f\"ER@1: {res1['er']:.4f}\")\n",
    "# print(\"\\nMetrics @5:\")\n",
    "# print(msg5)\n",
    "# # print(f\"ER@5: {res5['er']:.4f}\")\n",
    "# print(\"\\nMetrics @10:\")\n",
    "# print(msg10)\n",
    "# # print(f\"ER@10: {res10['er']:.4f}\")\n",
    "\n",
    "# 8. 保存结果目录\n",
    "eval_dir = Path(\"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log\") \\\n",
    "           / args.split / eval_date / \"evaluation_logs\"\n",
    "eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 9. 生成与训练时一致的 base_name，并组装文件名（改为 eval_direct）\n",
    "suffix = args.attack_mode          # e.g. DirectBoostingAttack\n",
    "mr = args.mr                       # e.g. 0.1\n",
    "dataset = args.split               # e.g. toys\n",
    "img_feat = args.image_feature_type # e.g. t5-small\n",
    "reduction = args.reduction_factor  # e.g. 8\n",
    "epoch = args.epoch                 # e.g. 20\n",
    "base_name = f\"{suffix}_{mr}_VIP5_{dataset}_{img_feat}_{reduction}_{epoch}\"\n",
    "\n",
    "direct_filename = f\"{base_name}_eval_direct_{prompt}.txt\"\n",
    "direct_log_path = eval_dir / direct_filename\n",
    "\n",
    "# 10. 写入文件，开头加入 Direct Evaluation Results 与 Dataset\n",
    "with open(direct_log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Direct Evaluation Results\\n\")\n",
    "    f.write(f\"Dataset: {dataset}\\n\\n\")\n",
    "    f.write(f\"Prompt: {prompt}\\n\")\n",
    "    f.write(f\"AttackMode: {args.attack_mode}\\n\")\n",
    "    f.write(f\"MaliciousRatio: {args.mr}\\n\\n\")\n",
    "    f.write(\"=== Metrics @1 ===\\n\")\n",
    "    f.write(msg1 + \"\\n\")\n",
    "    #f.write(f\"ER@1: {res1['er']:.4f}\\n\\n\")\n",
    "    f.write(\"=== Metrics @5 ===\\n\")\n",
    "    f.write(msg5 + \"\\n\")\n",
    "    #f.write(f\"ER@5: {res5['er']:.4f}\\n\\n\")\n",
    "    f.write(\"=== Metrics @10 ===\\n\")\n",
    "    f.write(msg10 + \"\\n\")\n",
    "    #f.write(f\"ER@10: {res10['er']:.4f}\\n\")\n",
    "\n",
    "print(f\"Direct 结果已保存至: {direct_log_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 12: Evaluation - Sequential 任务\n",
    "\n",
    "功能说明：\n",
    "加载 sequential 任务的测试数据，生成输出并计算评价指标，同时对 beam search 结果进行解码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] exp_splits_path = data/beauty/exp_splits.pkl\n",
      "[DEBUG] seq_path        = data/beauty/sequential_data.txt\n",
      "[DEBUG] idx_path        = data/beauty/user_id2idx.pkl\n",
      "[DEBUG] name_path       = data/beauty/user_id2name.pkl\n",
      "[WARN] NoAttack 模式下，动态构建了 31054 个用户映射\n",
      "compute_datum_info\n",
      "攻击模式：NoAttack，恶意比例：0.0\n",
      "Sequential 任务 (Prompt: A-3) 数据量: 1398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                      | 0/1398 [00:00<?, ?it/s]/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/site-packages/transformers/modeling_utils.py:810: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/site-packages/transformers/modeling_utils.py:764: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████████████████████████| 1398/1398 [13:49<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NDCG@1\tRec@1\tHits@1\tPrec@1\tMAP@1\tMRR@1\tER@1\n",
      "0.0268\t0.0268\t0.0268\t0.0268\t0.0268\t0.0268\t0.0000\n",
      "\n",
      "NDCG@5\tRec@5\tHits@5\tPrec@5\tMAP@5\tMRR@5\tER@5\n",
      "0.0381\t0.0485\t0.0485\t0.0097\t0.0347\t0.0347\t0.0000\n",
      "\n",
      "NDCG@10\tRec@10\tHits@10\tPrec@10\tMAP@10\tMRR@10\tER@10\n",
      "0.0417\t0.0594\t0.0594\t0.0059\t0.0362\t0.0362\t0.0001\n",
      "Sequential 结果已保存至: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log/beauty/0716/evaluation_logs/NoAttack_0.0_VIP5_beauty_vitb32_8_20_eval_sequential_A-3.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 12: Evaluation - Sequential 任务（带 Prompt 信息）\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# 如果 args.load 不为空，则从 load 路径中提取日期，否则使用当前日期\n",
    "if args.load is not None:\n",
    "    eval_date = Path(args.load).parents[1].name\n",
    "else:\n",
    "    eval_date = datetime.now().strftime(\"%m%d\")\n",
    "\n",
    "# 指定 Sequential 任务的 prompt 及样本数量\n",
    "test_task_list = {'sequential': ['A-3']} # A-3 or A-9\n",
    "prompt = test_task_list['sequential'][0]\n",
    "test_sample_numbers = {'sequential': (1, 1), 'direct': (1, 1), 'explanation': 1}\n",
    "\n",
    "# 获取 Sequential 任务的测试数据加载器\n",
    "zeroshot_test_loader = get_loader(\n",
    "    args,\n",
    "    test_task_list,\n",
    "    test_sample_numbers,\n",
    "    split=args.test,\n",
    "    mode='test',\n",
    "    batch_size=args.batch_size,\n",
    "    workers=args.num_workers,\n",
    "    distributed=args.distributed,\n",
    "    data_root=\"data\",\n",
    "    feature_root=\"features\"\n",
    ")\n",
    "\n",
    "print(f\"攻击模式：{args.attack_mode}，恶意比例：{args.mr}\")\n",
    "print(f\"Sequential 任务 (Prompt: {prompt}) 数据量:\", len(zeroshot_test_loader))\n",
    "\n",
    "# 生成候选并收集结果\n",
    "all_info = []\n",
    "for _, batch in tqdm(enumerate(zeroshot_test_loader), total=len(zeroshot_test_loader), ncols=100):\n",
    "    with torch.no_grad():\n",
    "        # 单次生成\n",
    "        results = model.generate_step(batch)\n",
    "        # Beam search 多样本生成\n",
    "        beam_outputs = model.generate(\n",
    "            input_ids=batch['input_ids'].cuda(),\n",
    "            whole_word_ids=batch['whole_word_ids'].cuda(),\n",
    "            category_ids=batch['category_ids'].cuda(),\n",
    "            vis_feats=batch['vis_feats'].cuda(),\n",
    "            task=batch[\"task\"][0],\n",
    "            max_length=50,\n",
    "            num_beams=20,\n",
    "            no_repeat_ngram_size=0,\n",
    "            num_return_sequences=20,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        generated_sents = model.tokenizer.batch_decode(beam_outputs, skip_special_tokens=True)\n",
    "\n",
    "        for j, item in enumerate(zip(results, batch['target_text'], batch['source_text'])):\n",
    "            new_info = {}\n",
    "            new_info['target_item'] = item[1]\n",
    "            new_info['gen_item_list'] = generated_sents[j*20: (j+1)*20]\n",
    "            all_info.append(new_info)\n",
    "\n",
    "# 构造 GT 与评分字典\n",
    "gt = {}\n",
    "ui_scores = {}\n",
    "for i, info in enumerate(all_info):\n",
    "    gt[i] = [int(info['target_item'])]\n",
    "    pred_dict = {}\n",
    "    for j in range(len(info['gen_item_list'])):\n",
    "        try:\n",
    "            pred_dict[int(info['gen_item_list'][j])] = -(j+1)\n",
    "        except:\n",
    "            pass\n",
    "    ui_scores[i] = pred_dict\n",
    "\n",
    "# 定义目标集合 & 计算指标\n",
    "targeted_items = args.data_target[args.split]  # 目标物品列表\n",
    "\n",
    "msg1, res1 = evaluate_all(ui_scores, gt, targeted_items, 1)\n",
    "msg5, res5 = evaluate_all(ui_scores, gt, targeted_items, 5)\n",
    "msg10, res10 = evaluate_all(ui_scores, gt, targeted_items, 10)\n",
    "\n",
    "# print(\"\\nMetrics @1:\", msg1, f\"ER@1: {res1['er']:.4f}\")\n",
    "# print(\"Metrics @5:\", msg5, f\"ER@5: {res5['er']:.4f}\")\n",
    "# print(\"Metrics @10:\", msg10, f\"ER@10: {res10['er']:.4f}\")\n",
    "\n",
    "# 构建保存目录\n",
    "eval_dir = Path(\"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log\") \\\n",
    "           / args.split / eval_date / \"evaluation_logs\"\n",
    "eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 文件名保持与训练一致的前缀，然后替换为 eval_sequential\n",
    "suffix = args.attack_mode\n",
    "mr = args.mr\n",
    "dataset = args.split\n",
    "img_feat = args.image_feature_type\n",
    "reduction = args.reduction_factor\n",
    "epoch = args.epoch\n",
    "base_name = f\"{suffix}_{mr}_VIP5_{dataset}_{img_feat}_{reduction}_{epoch}\"\n",
    "\n",
    "sequential_filename = f\"{base_name}_eval_sequential_{prompt}.txt\"\n",
    "sequential_log_path = eval_dir / sequential_filename\n",
    "\n",
    "# 写入文件，开头加上 Dataset、AttackMode、MaliciousRatio 和 Prompt\n",
    "with open(sequential_log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Sequential Evaluation Results\\n\")\n",
    "    f.write(f\"Dataset: {dataset}\\n\")\n",
    "    f.write(f\"Prompt: {prompt}\\n\")\n",
    "    f.write(f\"AttackMode: {args.attack_mode}\\n\")\n",
    "    f.write(f\"MaliciousRatio: {args.mr}\\n\")\n",
    "    f.write(f\"Prompt: {prompt}\\n\\n\")\n",
    "    f.write(\"=== Metrics @1 ===\\n\")\n",
    "    f.write(msg1 + \"\\n\")\n",
    "    #f.write(f\"ER@1: {res1['er']:.4f}\\n\\n\")\n",
    "    f.write(\"=== Metrics @5 ===\\n\")\n",
    "    f.write(msg5 + \"\\n\")\n",
    "    #f.write(f\"ER@5: {res5['er']:.4f}\\n\\n\")\n",
    "    f.write(\"=== Metrics @10 ===\\n\")\n",
    "    f.write(msg10 + \"\\n\")\n",
    "    #f.write(f\"ER@10: {res10['er']:.4f}\\n\")\n",
    "\n",
    "print(f\"Sequential 结果已保存至: {sequential_log_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vip5_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
