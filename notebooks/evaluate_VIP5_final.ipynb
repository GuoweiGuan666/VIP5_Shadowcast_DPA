{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 1: 环境设置与路径配置\n",
    "\n",
    "功能说明：\n",
    "将项目根目录添加到 Python 模块搜索路径中，确保后续能够正确加载项目内部模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: 环境设置与路径配置\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 将项目根目录添加到 sys.path 中\n",
    "project_path = \"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA\"\n",
    "if project_path not in sys.path:\n",
    "    sys.path.insert(0, project_path)\n",
    "print(\"Project path:\", project_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 2: 导入依赖库与模块\n",
    "\n",
    "功能说明：\n",
    "导入所有需要的第三方库和项目内部模块。注意部分模块（如 P5Tokenizer）在后续 cell 中会用到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有依赖库已导入\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: 导入依赖库与模块\n",
    "import collections\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from packaging import version\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gzip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# 导入项目内部模块\n",
    "from src.param import parse_args\n",
    "from src.utils import LossMeter, load_state_dict, set_global_logging_level\n",
    "from src.dist_utils import reduce_dict\n",
    "from transformers import T5Tokenizer\n",
    "from src.tokenization import P5Tokenizer\n",
    "from src.model import VIP5Tuning\n",
    "from src.trainer_base import TrainerBase\n",
    "\n",
    "# 判断是否使用 native AMP 或 Apex\n",
    "_use_native_amp = False\n",
    "_use_apex = False\n",
    "if version.parse(torch.__version__) < version.parse(\"1.6\"):\n",
    "    from transormers.file_utils import is_apex_available\n",
    "    if is_apex_available():\n",
    "        from apex import amp\n",
    "    _use_apex = True\n",
    "else:\n",
    "    _use_native_amp = True\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "print(\"所有依赖库已导入\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 3: 定义辅助函数\n",
    "\n",
    "功能说明：\n",
    "定义常用的辅助函数，如 pickle、json 的加载函数，以及文件读取函数等，方便后续调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "辅助函数定义完成\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: 定义辅助函数\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_pickle(data, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def ReadLineFromFile(path):\n",
    "    lines = []\n",
    "    with open(path, 'r') as fd:\n",
    "        for line in fd:\n",
    "            lines.append(line.rstrip('\\n'))\n",
    "    return lines\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "print(\"辅助函数定义完成\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 4: 定义 DotDict 类及参数设置\n",
    "\n",
    "功能说明：\n",
    "定义一个 DotDict 类，使得可以通过属性方式访问字典中的值；并设置所有实验参数、随机种子等，保证实验结果可复现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded targeted items: {'beauty': [2], 'clothing': [8], 'sports': [53], 'toys': [62]}\n",
      "当前参数配置：\n",
      "{'distributed': False, 'multiGPU': True, 'fp16': True, 'split': 'toys', 'train': 'toys', 'valid': 'toys', 'test': 'toys', 'batch_size': 16, 'optim': 'adamw', 'warmup_ratio': 0.1, 'lr': 0.001, 'num_workers': 4, 'clip_grad_norm': 5.0, 'losses': 'sequential,direct,explanation', 'backbone': 't5-small', 'data_target': {'beauty': [2], 'clothing': [8], 'sports': [53], 'toys': [62]}, 'image_feature_type': 'vitb32', 'image_feature_size_ratio': 2, 'use_adapter': True, 'reduction_factor': 8, 'use_single_adapter': True, 'use_vis_layer_norm': True, 'add_adapter_cross_attn': True, 'use_lm_head_adapter': True, 'epoch': 20, 'local_rank': 0, 'comment': '', 'train_topk': -1, 'valid_topk': -1, 'dropout': 0.1, 'tokenizer': 'p5', 'max_text_length': 1024, 'gen_max_length': 64, 'do_lower_case': False, 'weight_decay': 0.01, 'adam_eps': 1e-06, 'gradient_accumulation_steps': 1, 'seed': 2022, 'whole_word_embed': True, 'category_embed': True, 'world_size': 4, 'LOSSES_NAME': ['sequential_loss', 'direct_loss', 'explanation_loss', 'total_loss'], 'attack_mode': 'DirectBoostingAttack', 'mr': 0.1, 'date_str': '0430'}\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: 设置参数与随机种子\n",
    "# 功能：构造参数对象、设置随机种子及各项实验参数，保证实验结果可复现。\n",
    "import re\n",
    "\n",
    "class DotDict(dict):\n",
    "    \"\"\"将字典转化为对象，支持通过属性访问\"\"\"\n",
    "    def __init__(self, **kwds):\n",
    "        self.update(kwds)\n",
    "        self.__dict__ = self\n",
    "    def __repr__(self):\n",
    "        # 避免递归调用 __repr__，直接调用 dict 的 __repr__\n",
    "        return dict.__repr__(self)\n",
    "\n",
    "# 构造参数对象\n",
    "args = DotDict()\n",
    "\n",
    "# ----------------- 基本训练参数 -----------------\n",
    "args.distributed = False\n",
    "args.multiGPU = True\n",
    "args.fp16 = True\n",
    "\n",
    "args.split = \"toys\"\n",
    "args.train = args.split\n",
    "args.valid = args.split\n",
    "args.test = args.split\n",
    "args.batch_size = 16\n",
    "args.optim = 'adamw'\n",
    "args.warmup_ratio = 0.1\n",
    "args.lr = 1e-3\n",
    "args.num_workers = 4\n",
    "args.clip_grad_norm = 5.0\n",
    "args.losses = 'sequential,direct,explanation'\n",
    "args.backbone = 't5-small'\n",
    "\n",
    "# 1. 设置数据集与目标物品\n",
    "# args.data_target = {\"beauty\": [2], \"clothing\": [8], \"sports\": [53], \"toys\": [62]}  #TODO: Load targeted items from attack pipeline\n",
    "\n",
    "# 优化：\n",
    "# 1. 设置数据集与目标物品 —— 动态读取 analysis 目录下的 low_pop_items_*.txt\n",
    "import re\n",
    "\n",
    "args.data_target = {}\n",
    "for ds in [\"beauty\", \"clothing\", \"sports\", \"toys\"]:\n",
    "    path = f\"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/analysis/results/{ds}/low_pop_items_{ds}_lowcount_1.txt\"\n",
    "    target_ids = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as fin:\n",
    "        for line in fin:\n",
    "            # 匹配类似 \"Item: B004ZT0SSG (ID: 2), Count: 5\"\n",
    "            m = re.search(r\"\\(ID:\\s*(\\d+)\\)\", line)\n",
    "            if m:\n",
    "                target_ids.append(int(m.group(1)))\n",
    "    if not target_ids:\n",
    "        raise RuntimeError(f\"{ds} 中没有解析到任何 ID，请检查 {path} 的格式。\")\n",
    "    args.data_target[ds] = target_ids\n",
    "\n",
    "print(\"Loaded targeted items:\", args.data_target)\n",
    "\n",
    "\n",
    "# ----------------- 模型及视觉特征参数 -----------------\n",
    "args.image_feature_type = 'vitb32'\n",
    "args.image_feature_size_ratio = 2\n",
    "args.use_adapter = True\n",
    "args.reduction_factor = 8\n",
    "args.use_single_adapter = True\n",
    "args.use_vis_layer_norm = True\n",
    "args.add_adapter_cross_attn = True\n",
    "args.use_lm_head_adapter = True\n",
    "\n",
    "# ----------------- 训练轮数、随机种子等 -----------------\n",
    "args.epoch = 20\n",
    "args.local_rank = 0\n",
    "args.comment = ''\n",
    "args.train_topk = -1\n",
    "args.valid_topk = -1\n",
    "args.dropout = 0.1\n",
    "args.tokenizer = 'p5'\n",
    "args.max_text_length = 1024\n",
    "args.gen_max_length = 64\n",
    "args.do_lower_case = False\n",
    "args.weight_decay = 0.01\n",
    "args.adam_eps = 1e-6\n",
    "args.gradient_accumulation_steps = 1\n",
    "\n",
    "# 设置随机种子\n",
    "args.seed = 2022\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "# ----------------- 启用 Whole Word 和 Category Embedding -----------------\n",
    "args.whole_word_embed = True\n",
    "args.category_embed = True\n",
    "\n",
    "# ----------------- cudnn 及 GPU 参数 -----------------\n",
    "cudnn.benchmark = True\n",
    "ngpus_per_node = torch.cuda.device_count()\n",
    "args.world_size = ngpus_per_node\n",
    "\n",
    "# 设置损失项名称列表\n",
    "LOSSES_NAME = [f'{name}_loss' for name in args.losses.split(',')]\n",
    "LOSSES_NAME.append('total_loss')\n",
    "args.LOSSES_NAME = LOSSES_NAME\n",
    "\n",
    "# ----------------- 攻击模式与恶意比例 -----------------\n",
    "# 请根据当前实验替换下面两行\n",
    "args.attack_mode = \"DirectBoostingAttack\"\n",
    "args.mr = 0.1\n",
    "\n",
    "\n",
    "print(\"当前参数配置：\")\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 5: GPU设置与生成运行名称\n",
    "\n",
    "功能说明：\n",
    "指定使用的 GPU（手动设置），并构造一个运行名称（run_name），便于后续日志及保存结果区分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Launching at GPU 0\n",
      "运行名称: 0430_GPU4_toys_t5-small_sequentialdirectexplanation\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: GPU设置与生成运行名称\n",
    "# 功能：指定 GPU（手动设置），并构造一个运行名称\n",
    "\n",
    "# 手动指定 GPU ID\n",
    "gpu = 0\n",
    "args.gpu = gpu\n",
    "args.rank = gpu\n",
    "print(f'Process Launching at GPU {gpu}')\n",
    "\n",
    "# 设置当前 GPU 设备\n",
    "torch.cuda.set_device(f'cuda:{gpu}')\n",
    "\n",
    "# 构造运行名称\n",
    "comments = []\n",
    "dsets = []\n",
    "if 'toys' in args.train:\n",
    "    dsets.append('toys')\n",
    "if 'beauty' in args.train:\n",
    "    dsets.append('beauty')\n",
    "if 'sports' in args.train:\n",
    "    dsets.append('sports')\n",
    "if 'clothing' in args.train:\n",
    "    dsets.append('clothing')\n",
    "comments.append(''.join(dsets))\n",
    "if args.backbone:\n",
    "    comments.append(args.backbone)\n",
    "comments.append(''.join(args.losses.split(',')))\n",
    "if args.comment != '':\n",
    "    comments.append(args.comment)\n",
    "comment = '_'.join(comments)\n",
    "\n",
    "from datetime import datetime\n",
    "current_time = datetime.now().strftime('%m%d')  # 例如 '0304'\n",
    "\n",
    "if args.local_rank in [0, -1]:\n",
    "    run_name = f'{current_time}_GPU{args.world_size}'\n",
    "    if len(comments) > 0:\n",
    "        run_name += f'_{comment}'\n",
    "    args.run_name = run_name\n",
    "    print(\"运行名称:\", args.run_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 6: 构建模型配置、Tokenizer 与模型\n",
    "\n",
    "功能说明：\n",
    "根据参数构建模型配置（config）、创建 Tokenizer，并加载预训练模型。\n",
    "注意：由于 checkpoint 使用的是 T5Tokenizer，而我们调用 P5Tokenizer，所以会有警告信息，但功能不受影响。\n",
    "另外，为了适配 adapter，需要将 config.d_model 赋值给 adapter_config。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: <class 'src.tokenization.P5Tokenizer'> t5-small\n",
      "Building Model at GPU 0\n",
      "JointEncoder initialized successfully.\n",
      "T5Stack initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VIP5Tuning were not initialized from the model checkpoint at t5-small and are newly initialized: ['decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.visual_embedding.feat_embedding.0.model.0.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.visual_embedding.feat_embedding.0.model.2.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.visual_embedding.feat_embedding.0.model.0.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'output_adapter.adapter.down_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.visual_embedding.feat_embedding.1.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.category_embeddings.weight', 'encoder.visual_embedding.feat_embedding.0.model.2.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'output_adapter.adapter.up_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.whole_word_embeddings.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'output_adapter.adapter.up_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'output_adapter.adapter.down_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_head initialized successfully.\n",
      "OutputParallelAdapterLayer initialized successfully.\n",
      "AdapterConfig: AdapterConfig(add_layer_norm_before_adapter=False, add_layer_norm_after_adapter=False, non_linearity='gelu_new', reduction_factor=8)\n",
      "模型和 Tokenizer 构建完成\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: 构建模型配置、Tokenizer 与模型\n",
    "# 功能：根据参数构建模型配置，创建 Tokenizer，并加载预训练模型\n",
    "import re  # 确保导入 re 模块\n",
    "\n",
    "def create_config(args):\n",
    "    from transformers import T5Config\n",
    "    from adapters import AdapterConfig  # 使用适配器配置\n",
    "\n",
    "    # 从预训练 checkpoint 加载 T5 配置\n",
    "    config = T5Config.from_pretrained(args.backbone)\n",
    "    # 将所有参数写入配置中\n",
    "    for k, v in vars(args).items():\n",
    "        setattr(config, k, v)\n",
    "    config.non_linearity = \"relu\"\n",
    "\n",
    "    # 设置视觉特征参数\n",
    "    image_feature_dim_dict = {\n",
    "        'vitb32': 512,\n",
    "        'vitb16': 512,\n",
    "        'vitl14': 768,\n",
    "        'rn50': 1024,\n",
    "        'rn101': 512\n",
    "    }\n",
    "    config.feat_dim = image_feature_dim_dict[args.image_feature_type]\n",
    "    config.n_vis_tokens = args.image_feature_size_ratio\n",
    "    config.use_vis_layer_norm = args.use_vis_layer_norm\n",
    "    config.reduction_factor = args.reduction_factor\n",
    "\n",
    "    config.use_adapter = args.use_adapter\n",
    "    config.add_adapter_cross_attn = args.add_adapter_cross_attn\n",
    "    config.use_lm_head_adapter = args.use_lm_head_adapter\n",
    "    config.use_single_adapter = args.use_single_adapter\n",
    "\n",
    "    config.dropout_rate = args.dropout\n",
    "    config.dropout = args.dropout\n",
    "    config.attention_dropout = args.dropout\n",
    "    config.activation_dropout = args.dropout\n",
    "\n",
    "    config.losses = args.losses\n",
    "\n",
    "    # 如果使用适配器，则创建适配器配置，并将主配置的 d_model 传给 adapter_config\n",
    "    tasks = re.split(\"[, ]+\", args.losses)\n",
    "    if args.use_adapter:\n",
    "        adapter_config = AdapterConfig()\n",
    "        adapter_config.tasks = tasks\n",
    "        adapter_config.d_model = config.d_model  # 传递隐藏维度\n",
    "        adapter_config.use_single_adapter = args.use_single_adapter\n",
    "        adapter_config.reduction_factor = args.reduction_factor\n",
    "        adapter_config.track_z = False\n",
    "        config.adapter_config = adapter_config\n",
    "    else:\n",
    "        config.adapter_config = None\n",
    "\n",
    "    return config\n",
    "\n",
    "def create_tokenizer(args):\n",
    "    from transformers import T5Tokenizer\n",
    "    # 根据参数决定使用 P5Tokenizer 或 T5Tokenizer\n",
    "    if 'p5' in args.tokenizer:\n",
    "        from src.tokenization import P5Tokenizer\n",
    "        tokenizer_class = P5Tokenizer\n",
    "    else:\n",
    "        tokenizer_class = T5Tokenizer\n",
    "\n",
    "    tokenizer = tokenizer_class.from_pretrained(\n",
    "        args.backbone,\n",
    "        max_length=args.max_text_length,\n",
    "        do_lower_case=args.do_lower_case,\n",
    "    )\n",
    "    print(\"Tokenizer:\", tokenizer_class, args.backbone)\n",
    "    return tokenizer\n",
    "\n",
    "def create_model(model_class, config):\n",
    "    print(f'Building Model at GPU {args.gpu}')\n",
    "    model = model_class.from_pretrained(\n",
    "        args.backbone,\n",
    "        config=config\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 构建配置、Tokenizer 和模型\n",
    "config = create_config(args)\n",
    "if args.tokenizer is None:\n",
    "    args.tokenizer = args.backbone\n",
    "tokenizer = create_tokenizer(args)\n",
    "model_class = VIP5Tuning\n",
    "model = create_model(model_class, config)\n",
    "\n",
    "# 将模型移至指定 GPU\n",
    "model = model.cuda()\n",
    "\n",
    "# 如果使用 P5Tokenizer，则调整模型的词嵌入\n",
    "if 'p5' in args.tokenizer:\n",
    "    model.resize_token_embeddings(tokenizer.vocab_size)\n",
    "model.tokenizer = tokenizer\n",
    "\n",
    "print(\"模型和 Tokenizer 构建完成\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 7: 加载预训练模型权重\n",
    "\n",
    "功能说明：\n",
    "从指定 checkpoint 路径加载预训练模型权重，并打印加载结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'date_str' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 36\u001b[0m\n\u001b[1;32m     22\u001b[0m exp_dir \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mattack_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mmr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# 最终 checkpoint 路径\u001b[39;00m\n\u001b[1;32m     32\u001b[0m args\u001b[38;5;241m.\u001b[39mload \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     35\u001b[0m     args\u001b[38;5;241m.\u001b[39msplit,\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mdate_str\u001b[49m,\n\u001b[1;32m     37\u001b[0m     exp_dir,\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBEST_EVAL_LOSS.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# 调用加载\u001b[39;00m\n\u001b[1;32m     42\u001b[0m load_checkpoint(args\u001b[38;5;241m.\u001b[39mload)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'date_str' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 7: 加载预训练模型权重\n",
    "# 功能：从 checkpoint 加载预训练模型权重，并打印实际加载路径\n",
    "\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "def load_checkpoint(ckpt_path):\n",
    "    # 如果没有以 .pth 结尾，自动补全\n",
    "    if not ckpt_path.endswith('.pth'):\n",
    "        ckpt_path = ckpt_path + '.pth'\n",
    "    print(f\"📥 Loading checkpoint from: {ckpt_path}\")\n",
    "    state_dict = load_state_dict(ckpt_path, 'cpu')\n",
    "    results = model.load_state_dict(state_dict, strict=False)\n",
    "    print(\"ℹ️  load_state_dict 结果：\")\n",
    "    pprint(results)\n",
    "\n",
    "# —— 动态构造 checkpoint 路径 —— #\n",
    "# 假设训练输出目录为 snap/{split}/{date}/{exp_dir}\n",
    "\n",
    "\n",
    "# 构造实验子目录名，与 run_finetune.sh 中保持一致\n",
    "exp_dir = (\n",
    "    f\"{args.attack_mode}_{args.mr}_\"\n",
    "    f\"{args.split}-\"\n",
    "    f\"{args.image_feature_type}-\"\n",
    "    f\"{args.image_feature_size_ratio}-\"\n",
    "    f\"{args.reduction_factor}-\"\n",
    "    f\"{args.epoch}\"\n",
    ")\n",
    "\n",
    "# 最终 checkpoint 路径\n",
    "args.load = os.path.join(\n",
    "    \"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA\",\n",
    "    \"snap\",\n",
    "    args.split,\n",
    "    date_str,\n",
    "    exp_dir,\n",
    "    \"BEST_EVAL_LOSS.pth\"\n",
    ")\n",
    "\n",
    "# 调用加载\n",
    "load_checkpoint(args.load)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 8: 加载数据集及数据映射\n",
    "\n",
    "功能说明：\n",
    "加载数据分割文件（如 rating_splits_augmented.pkl）以及数据映射文件（datamaps.json），用于后续评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data长度: 16759\n",
      "Test data示例: {'reviewerID': 'A5K3CK2PWYQ7O', 'asin': 'B00F4CFEYG', 'reviewerName': 'Ellie \"mittbooks\"', 'helpful': [0, 0], 'reviewText': \"I've found the Melissa & Doug brand to be overall good, although there are occasional negatives.  This is definitely one of the toys we'll mark a &#34;winner.&#34;  The vacuum comes in two pieces that require minimal assembly (the long handle and the base need to be put together - no tools required).  The height is perfect for our two year old who is 3 feet tall.  The top part moves at about a 45 degree angle to facilitate little people pushing the vacuum.  I'm not sure how long the six wooden pieces of &#34;trash&#34; will last.  Although not tiny, they would be easy to lose.  The vacuum does a good job of picking them up easily and there is a small area in the back of the base to take them out again.  There is also a rotating knob on the front of the handle that makes a good clicking noise when it moves.  Our son is truly enjoying this this toy and the overall quality is excellent.  Definitely a good addition to the toy room.\", 'overall': 4.0, 'summary': '... found the Melissa & Doug brand to be overall good, although there are occasional negatives', 'unixReviewTime': 1403913600, 'reviewTime': '06 28, 2014', 'explanation': \"I've found the Melissa & Doug brand to be overall good\", 'feature': 'overall'}\n",
      "用户数量: 19412\n",
      "物品数量: 11924\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: 加载数据集及数据映射\n",
    "# 功能：加载 rating_splits_augmented.pkl 和 datamaps.json 数据文件\n",
    "\n",
    "data_splits = load_pickle(f'../data/{args.split}/rating_splits_augmented.pkl')\n",
    "test_review_data = data_splits['test']\n",
    "print(\"Test data长度:\", len(test_review_data))\n",
    "print(\"Test data示例:\", test_review_data[0])\n",
    "\n",
    "data_maps = load_json(os.path.join('../data', args.split, 'datamaps.json'))\n",
    "print(\"用户数量:\", len(data_maps['user2id']))\n",
    "print(\"物品数量:\", len(data_maps['item2id']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 9: 加载数据生成器与评价指标\n",
    "\n",
    "功能说明：\n",
    "导入数据加载函数和评价指标函数，为后续评估生成数据加载器和计算 BLEU/ROUGE 等指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据加载器与评价指标函数已导入\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: 导入数据加载器与评价指标函数\n",
    "# 功能：导入 get_loader、BLEU、ROUGE 等评价指标函数\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "from src.data import get_loader\n",
    "from evaluate.utils import rouge_score, bleu_score, unique_sentence_percent, root_mean_square_error, mean_absolute_error, feature_detect, feature_matching_ratio, feature_coverage_ratio, feature_diversity\n",
    "from evaluate.metrics4rec import evaluate_all\n",
    "\n",
    "print(\"数据加载器与评价指标函数已导入\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 10: Evaluation - Explanation 任务\n",
    "\n",
    "功能说明：\n",
    "加载 explanation 任务的数据生成器，调用模型生成输出，并计算 BLEU、ROUGE 指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/snap/toys/0430/DirectBoostingAttack_0.1_toys-vitb32-2-8-20/BEST_EVAL_LOSS.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_datum_info\n",
      "Explanation 任务 (Prompt: C-3) 数据量: 646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 646/646 [01:02<00:00, 10.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1  3.6203, BLEU-4  1.2647\n",
      "rouge_1/f_score  6.8512\n",
      "rouge_1/r_score  5.3036\n",
      "rouge_1/p_score 13.1685\n",
      "rouge_2/f_score  1.4750\n",
      "rouge_2/r_score  1.2484\n",
      "rouge_2/p_score  2.4704\n",
      "rouge_l/f_score  5.1943\n",
      "rouge_l/r_score  5.0239\n",
      "rouge_l/p_score 12.7101\n",
      "Explanation 任务 (Prompt: C-3) 评价结果已保存至: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log/toys/0430/evaluation_logs/DirectBoostingAttack_0.1_VIP5_toys_vitb32_8_20_eval_explanation_C-3.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading checkpoint from:\", args.load)\n",
    "\n",
    "# =============================================================================\n",
    "# Cell 10: Evaluation - Explanation 任务（带 Prompt 信息）\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# 如果 args.load 不为空，则从其中提取日期，否则使用当前日期\n",
    "if args.load is not None:\n",
    "    eval_date = Path(args.load).parents[1].name\n",
    "else:\n",
    "    eval_date = datetime.now().strftime(\"%m%d\")\n",
    "\n",
    "# 指定 Explanation 任务的 prompt 及样本数量\n",
    "exp_prompt = 'C-3'  # 可修改为 'C-12', 'C-3' 等所需的 prompt 编号\n",
    "test_task_list = {'explanation': [exp_prompt]}\n",
    "test_sample_numbers = {'sequential': (1, 1), 'direct': (1, 1), 'explanation': 1}\n",
    "\n",
    "# 获取 Explanation 任务的测试数据加载器\n",
    "zeroshot_test_loader = get_loader(\n",
    "    args,\n",
    "    test_task_list,\n",
    "    test_sample_numbers,\n",
    "    split=args.test, \n",
    "    mode='test', \n",
    "    batch_size=args.batch_size,\n",
    "    workers=args.num_workers,\n",
    "    distributed=args.distributed,\n",
    "    data_root=\"../data\",\n",
    "    feature_root=\"../features\"\n",
    ")\n",
    "print(f\"Explanation 任务 (Prompt: {exp_prompt}) 数据量:\", len(zeroshot_test_loader))\n",
    "\n",
    "tokens_predict = []\n",
    "tokens_test = []\n",
    "\n",
    "# 遍历测试数据加载器，调用模型生成预测结果\n",
    "for _, batch in tqdm(enumerate(zeroshot_test_loader), total=len(zeroshot_test_loader), ncols=100):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        tokens_predict.extend(results)\n",
    "        tokens_test.extend(batch['target_text'])\n",
    "\n",
    "# 计算 BLEU 与 ROUGE 指标\n",
    "BLEU1 = bleu_score(tokens_test, tokens_predict, n_gram=1, smooth=False)\n",
    "BLEU4 = bleu_score(tokens_test, tokens_predict, n_gram=4, smooth=False)\n",
    "ROUGE = rouge_score(tokens_test, tokens_predict)\n",
    "\n",
    "print(f'BLEU-1 {BLEU1:7.4f}, BLEU-4 {BLEU4:7.4f}')\n",
    "for k, v in ROUGE.items():\n",
    "    print(f'{k} {v:7.4f}')\n",
    "\n",
    "# 构建保存评估结果的目录和文件名\n",
    "eval_dir = Path(\"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log\") \\\n",
    "           / args.split / eval_date / \"evaluation_logs\"\n",
    "eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 保持与训练时相同的 base_name\n",
    "suffix = args.attack_mode\n",
    "mr = args.mr\n",
    "dataset = args.split\n",
    "img_feat = args.image_feature_type\n",
    "reduction = args.reduction_factor\n",
    "epoch = args.epoch\n",
    "base_name = f\"{suffix}_{mr}_VIP5_{dataset}_{img_feat}_{reduction}_{epoch}\"\n",
    "\n",
    "explanation_filename = f\"{base_name}_eval_explanation_{exp_prompt}.txt\"\n",
    "explanation_log_path = eval_dir / explanation_filename\n",
    "\n",
    "# 保存评估结果，文件头加入 Dataset、AttackMode 和 MaliciousRatio\n",
    "with open(explanation_log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Explanation Evaluation Results\\n\")\n",
    "    f.write(f\"Dataset: {dataset}\\n\")\n",
    "    f.write(f\"AttackMode: {args.attack_mode}\\n\")\n",
    "    f.write(f\"MaliciousRatio: {args.mr}\\n\")\n",
    "    f.write(f\"Prompt: {exp_prompt}\\n\\n\")\n",
    "    f.write(f\"BLEU-1: {BLEU1:7.4f}\\n\")\n",
    "    f.write(f\"BLEU-4: {BLEU4:7.4f}\\n\")\n",
    "    for k, v in ROUGE.items():\n",
    "        f.write(f\"{k}: {v:7.4f}\\n\")\n",
    "\n",
    "print(f\"Explanation 任务 (Prompt: {exp_prompt}) 评价结果已保存至: {explanation_log_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 11: Evaluation - Direct 任务\n",
    "\n",
    "功能说明：\n",
    "加载 direct 任务的测试数据，生成输出并计算评价指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_datum_info\n",
      "攻击模式：DirectBoostingAttack，恶意比例：0.1\n",
      "Direct 任务 (Prompt: B-8) 数据量: 1335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1335/1335 [18:59<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NDCG@1\tRec@1\tHits@1\tPrec@1\tMAP@1\tMRR@1\tER@1\n",
      "0.0395\t0.0395\t0.0395\t0.0395\t0.0395\t0.0395\t0.0000\n",
      "\n",
      "NDCG@5\tRec@5\tHits@5\tPrec@5\tMAP@5\tMRR@5\tER@5\n",
      "0.0727\t0.1050\t0.1050\t0.0210\t0.0621\t0.0621\t0.0000\n",
      "\n",
      "NDCG@10\tRec@10\tHits@10\tPrec@10\tMAP@10\tMRR@10\tER@10\n",
      "0.0928\t0.1677\t0.1677\t0.0168\t0.0703\t0.0703\t0.0005\n",
      "\n",
      "Metrics @1:\n",
      "\n",
      "NDCG@1\tRec@1\tHits@1\tPrec@1\tMAP@1\tMRR@1\tER@1\n",
      "0.0395\t0.0395\t0.0395\t0.0395\t0.0395\t0.0395\t0.0000\n",
      "\n",
      "Metrics @5:\n",
      "\n",
      "NDCG@5\tRec@5\tHits@5\tPrec@5\tMAP@5\tMRR@5\tER@5\n",
      "0.0727\t0.1050\t0.1050\t0.0210\t0.0621\t0.0621\t0.0000\n",
      "\n",
      "Metrics @10:\n",
      "\n",
      "NDCG@10\tRec@10\tHits@10\tPrec@10\tMAP@10\tMRR@10\tER@10\n",
      "0.0928\t0.1677\t0.1677\t0.0168\t0.0703\t0.0703\t0.0005\n",
      "Direct 结果已保存至: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log/toys/0430/evaluation_logs/DirectBoostingAttack_0.1_VIP5_toys_vitb32_8_20_eval_direct_B-8.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 11: Evaluation - Direct 任务（带 Prompt 信息）\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "# 1. 确定 eval_date：若从 checkpoint 恢复则取目录名，否则用当前日期\n",
    "if args.load is not None:\n",
    "    eval_date = Path(args.load).parents[1].name\n",
    "else:\n",
    "    eval_date = datetime.now().strftime(\"%m%d\")\n",
    "\n",
    "# 2. 指定 Direct 任务的 Prompt\n",
    "test_task_list = {'direct': ['B-8']}  # 可选 'B-5' 或 'B-8'\n",
    "prompt = test_task_list['direct'][0]\n",
    "\n",
    "test_sample_numbers = {\n",
    "    'sequential': (1, 1),\n",
    "    'direct': (1, 1),\n",
    "    'explanation': 1\n",
    "}\n",
    "\n",
    "# 3. 获取 Direct 测试 Loader\n",
    "zeroshot_test_loader = get_loader(\n",
    "    args,\n",
    "    test_task_list,\n",
    "    test_sample_numbers,\n",
    "    split=args.test,\n",
    "    mode='test',\n",
    "    batch_size=args.batch_size,\n",
    "    workers=args.num_workers,\n",
    "    distributed=args.distributed,\n",
    "    data_root=\"../data\",\n",
    "    feature_root=\"../features\"\n",
    ")\n",
    "\n",
    "print(f\"攻击模式：{args.attack_mode}，恶意比例：{args.mr}\")\n",
    "print(f\"Direct 任务 (Prompt: {prompt}) 数据量:\", len(zeroshot_test_loader))\n",
    "\n",
    "# 4. 收集所有样本的 GT 与模型输出\n",
    "all_info = []\n",
    "for _, batch in tqdm(enumerate(zeroshot_test_loader), total=len(zeroshot_test_loader)):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        beam_outputs = model.generate(\n",
    "            input_ids=batch['input_ids'].cuda(),\n",
    "            whole_word_ids=batch['whole_word_ids'].cuda(),\n",
    "            category_ids=batch['category_ids'].cuda(),\n",
    "            vis_feats=batch['vis_feats'].cuda(),\n",
    "            task=batch[\"task\"][0],\n",
    "            max_length=50,\n",
    "            num_beams=20,\n",
    "            no_repeat_ngram_size=0,\n",
    "            num_return_sequences=20,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        generated_sents = model.tokenizer.batch_decode(beam_outputs, skip_special_tokens=True)\n",
    "\n",
    "        # for j, (_, tgt_text, _) in enumerate(zip(results, batch['target_text'], batch['source_text'])):\n",
    "        #     all_info.append({\n",
    "        #         'target_item': tgt_text,\n",
    "        #         'gen_item_list': generated_sents[j * 20: (j + 1) * 20]\n",
    "        #     })\n",
    "        for j, item in enumerate(zip(results, batch['target_text'], batch['source_text'])):\n",
    "            new_info = {}\n",
    "            new_info['target_item'] = item[1]\n",
    "            new_info['gen_item_list'] = generated_sents[j*20: (j+1)*20]\n",
    "            all_info.append(new_info)\n",
    "\n",
    "# 5. 构造 GT 与评分字典\n",
    "gt = {}\n",
    "ui_scores = {}\n",
    "for i, info in enumerate(all_info):\n",
    "    gt[i] = [int(info['target_item'])]\n",
    "    pred_dict = {}\n",
    "    for j in range(len(info['gen_item_list'])):\n",
    "        try:\n",
    "            pred_dict[int(info['gen_item_list'][j])] = -(j+1)\n",
    "        except:\n",
    "            pass\n",
    "    ui_scores[i] = pred_dict\n",
    "\n",
    "# 6. 定义用于 ER@K 的目标集合\n",
    "targeted_items = args.data_target[args.split] \n",
    "\n",
    "# 7. 计算指标 + ER@K\n",
    "msg1, res1 = evaluate_all(ui_scores, gt, targeted_items, 1)\n",
    "msg5, res5 = evaluate_all(ui_scores, gt, targeted_items, 5)\n",
    "msg10, res10 = evaluate_all(ui_scores, gt,targeted_items, 10)\n",
    "\n",
    "\n",
    "print(\"\\nMetrics @1:\")\n",
    "print(msg1)\n",
    "# print(f\"ER@1: {res1['er']:.4f}\")\n",
    "print(\"\\nMetrics @5:\")\n",
    "print(msg5)\n",
    "# print(f\"ER@5: {res5['er']:.4f}\")\n",
    "print(\"\\nMetrics @10:\")\n",
    "print(msg10)\n",
    "# print(f\"ER@10: {res10['er']:.4f}\")\n",
    "\n",
    "# 8. 保存结果目录\n",
    "eval_dir = Path(\"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log\") \\\n",
    "           / args.split / eval_date / \"evaluation_logs\"\n",
    "eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 9. 生成与训练时一致的 base_name，并组装文件名（改为 eval_direct）\n",
    "suffix = args.attack_mode          # e.g. DirectBoostingAttack\n",
    "mr = args.mr                       # e.g. 0.1\n",
    "dataset = args.split               # e.g. toys\n",
    "img_feat = args.image_feature_type # e.g. t5-small\n",
    "reduction = args.reduction_factor  # e.g. 8\n",
    "epoch = args.epoch                 # e.g. 20\n",
    "base_name = f\"{suffix}_{mr}_VIP5_{dataset}_{img_feat}_{reduction}_{epoch}\"\n",
    "\n",
    "direct_filename = f\"{base_name}_eval_direct_{prompt}.txt\"\n",
    "direct_log_path = eval_dir / direct_filename\n",
    "\n",
    "# 10. 写入文件，开头加入 Direct Evaluation Results 与 Dataset\n",
    "with open(direct_log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Direct Evaluation Results\\n\")\n",
    "    f.write(f\"Dataset: {dataset}\\n\\n\")\n",
    "    f.write(f\"AttackMode: {args.attack_mode}\\n\")\n",
    "    f.write(f\"MaliciousRatio: {args.mr}\\n\\n\")\n",
    "    f.write(\"=== Metrics @1 ===\\n\")\n",
    "    f.write(msg1 + \"\\n\")\n",
    "    #f.write(f\"ER@1: {res1['er']:.4f}\\n\\n\")\n",
    "    f.write(\"=== Metrics @5 ===\\n\")\n",
    "    f.write(msg5 + \"\\n\")\n",
    "    #f.write(f\"ER@5: {res5['er']:.4f}\\n\\n\")\n",
    "    f.write(\"=== Metrics @10 ===\\n\")\n",
    "    f.write(msg10 + \"\\n\")\n",
    "    #f.write(f\"ER@10: {res10['er']:.4f}\\n\")\n",
    "\n",
    "print(f\"Direct 结果已保存至: {direct_log_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 12: Evaluation - Sequential 任务\n",
    "\n",
    "功能说明：\n",
    "加载 sequential 任务的测试数据，生成输出并计算评价指标，同时对 beam search 结果进行解码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_datum_info\n",
      "攻击模式：DirectBoostingAttack，恶意比例：0.1\n",
      "Sequential 任务 (Prompt: A-3) 数据量: 1335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 1335/1335 [12:17<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NDCG@1\tRec@1\tHits@1\tPrec@1\tMAP@1\tMRR@1\tER@1\n",
      "0.0434\t0.0434\t0.0434\t0.0434\t0.0434\t0.0434\t0.0000\n",
      "\n",
      "NDCG@5\tRec@5\tHits@5\tPrec@5\tMAP@5\tMRR@5\tER@5\n",
      "0.0520\t0.0590\t0.0590\t0.0118\t0.0496\t0.0496\t0.0000\n",
      "\n",
      "NDCG@10\tRec@10\tHits@10\tPrec@10\tMAP@10\tMRR@10\tER@10\n",
      "0.0532\t0.0628\t0.0628\t0.0063\t0.0501\t0.0501\t0.0000\n",
      "Sequential 结果已保存至: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log/toys/0430/evaluation_logs/DirectBoostingAttack_0.1_VIP5_toys_vitb32_8_20_eval_sequential_A-3.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 12: Evaluation - Sequential 任务（带 Prompt 信息）\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# 如果 args.load 不为空，则从 load 路径中提取日期，否则使用当前日期\n",
    "if args.load is not None:\n",
    "    eval_date = Path(args.load).parents[1].name\n",
    "else:\n",
    "    eval_date = datetime.now().strftime(\"%m%d\")\n",
    "\n",
    "# 指定 Sequential 任务的 prompt 及样本数量\n",
    "test_task_list = {'sequential': ['A-3']}\n",
    "prompt = test_task_list['sequential'][0]\n",
    "test_sample_numbers = {'sequential': (1, 1), 'direct': (1, 1), 'explanation': 1}\n",
    "\n",
    "# 获取 Sequential 任务的测试数据加载器\n",
    "zeroshot_test_loader = get_loader(\n",
    "    args,\n",
    "    test_task_list,\n",
    "    test_sample_numbers,\n",
    "    split=args.test,\n",
    "    mode='test',\n",
    "    batch_size=args.batch_size,\n",
    "    workers=args.num_workers,\n",
    "    distributed=args.distributed,\n",
    "    data_root=\"../data\",\n",
    "    feature_root=\"../features\"\n",
    ")\n",
    "\n",
    "print(f\"攻击模式：{args.attack_mode}，恶意比例：{args.mr}\")\n",
    "print(f\"Sequential 任务 (Prompt: {prompt}) 数据量:\", len(zeroshot_test_loader))\n",
    "\n",
    "# 生成候选并收集结果\n",
    "all_info = []\n",
    "for _, batch in tqdm(enumerate(zeroshot_test_loader), total=len(zeroshot_test_loader), ncols=100):\n",
    "    with torch.no_grad():\n",
    "        # 单次生成\n",
    "        results = model.generate_step(batch)\n",
    "        # Beam search 多样本生成\n",
    "        beam_outputs = model.generate(\n",
    "            input_ids=batch['input_ids'].cuda(),\n",
    "            whole_word_ids=batch['whole_word_ids'].cuda(),\n",
    "            category_ids=batch['category_ids'].cuda(),\n",
    "            vis_feats=batch['vis_feats'].cuda(),\n",
    "            task=batch[\"task\"][0],\n",
    "            max_length=50,\n",
    "            num_beams=20,\n",
    "            no_repeat_ngram_size=0,\n",
    "            num_return_sequences=20,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        generated_sents = model.tokenizer.batch_decode(beam_outputs, skip_special_tokens=True)\n",
    "\n",
    "        for j, item in enumerate(zip(results, batch['target_text'], batch['source_text'])):\n",
    "            new_info = {}\n",
    "            new_info['target_item'] = item[1]\n",
    "            new_info['gen_item_list'] = generated_sents[j*20: (j+1)*20]\n",
    "            all_info.append(new_info)\n",
    "\n",
    "# 构造 GT 与评分字典\n",
    "gt = {}\n",
    "ui_scores = {}\n",
    "for i, info in enumerate(all_info):\n",
    "    gt[i] = [int(info['target_item'])]\n",
    "    pred_dict = {}\n",
    "    for j in range(len(info['gen_item_list'])):\n",
    "        try:\n",
    "            pred_dict[int(info['gen_item_list'][j])] = -(j+1)\n",
    "        except:\n",
    "            pass\n",
    "    ui_scores[i] = pred_dict\n",
    "\n",
    "# 定义目标集合 & 计算指标\n",
    "targeted_items = args.data_target[args.split]  # 目标物品列表\n",
    "\n",
    "msg1, res1 = evaluate_all(ui_scores, gt, targeted_items, 1)\n",
    "msg5, res5 = evaluate_all(ui_scores, gt, targeted_items, 5)\n",
    "msg10, res10 = evaluate_all(ui_scores, gt, targeted_items, 10)\n",
    "\n",
    "# print(\"\\nMetrics @1:\", msg1, f\"ER@1: {res1['er']:.4f}\")\n",
    "# print(\"Metrics @5:\", msg5, f\"ER@5: {res5['er']:.4f}\")\n",
    "# print(\"Metrics @10:\", msg10, f\"ER@10: {res10['er']:.4f}\")\n",
    "\n",
    "# 构建保存目录\n",
    "eval_dir = Path(\"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log\") \\\n",
    "           / args.split / eval_date / \"evaluation_logs\"\n",
    "eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 文件名保持与训练一致的前缀，然后替换为 eval_sequential\n",
    "suffix = args.attack_mode\n",
    "mr = args.mr\n",
    "dataset = args.split\n",
    "img_feat = args.image_feature_type\n",
    "reduction = args.reduction_factor\n",
    "epoch = args.epoch\n",
    "base_name = f\"{suffix}_{mr}_VIP5_{dataset}_{img_feat}_{reduction}_{epoch}\"\n",
    "\n",
    "sequential_filename = f\"{base_name}_eval_sequential_{prompt}.txt\"\n",
    "sequential_log_path = eval_dir / sequential_filename\n",
    "\n",
    "# 写入文件，开头加上 Dataset、AttackMode、MaliciousRatio 和 Prompt\n",
    "with open(sequential_log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Sequential Evaluation Results\\n\")\n",
    "    f.write(f\"Dataset: {dataset}\\n\")\n",
    "    f.write(f\"AttackMode: {args.attack_mode}\\n\")\n",
    "    f.write(f\"MaliciousRatio: {args.mr}\\n\")\n",
    "    f.write(f\"Prompt: {prompt}\\n\\n\")\n",
    "    f.write(\"=== Metrics @1 ===\\n\")\n",
    "    f.write(msg1 + \"\\n\")\n",
    "    #f.write(f\"ER@1: {res1['er']:.4f}\\n\\n\")\n",
    "    f.write(\"=== Metrics @5 ===\\n\")\n",
    "    f.write(msg5 + \"\\n\")\n",
    "    #f.write(f\"ER@5: {res5['er']:.4f}\\n\\n\")\n",
    "    f.write(\"=== Metrics @10 ===\\n\")\n",
    "    f.write(msg10 + \"\\n\")\n",
    "    #f.write(f\"ER@10: {res10['er']:.4f}\\n\")\n",
    "\n",
    "print(f\"Sequential 结果已保存至: {sequential_log_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vip5_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
