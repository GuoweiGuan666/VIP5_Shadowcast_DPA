{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 1: ç¯å¢ƒè®¾ç½®ä¸è·¯å¾„é…ç½®\n",
    "\n",
    "åŠŸèƒ½è¯´æ˜ï¼š\n",
    "å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python æ¨¡å—æœç´¢è·¯å¾„ä¸­ï¼Œç¡®ä¿åç»­èƒ½å¤Ÿæ­£ç¡®åŠ è½½é¡¹ç›®å†…éƒ¨æ¨¡å—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ Current working directory: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA\n",
      "â†’ First entries in sys.path: ['/scratch/guowei/Code/VIP5/transformers', '/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/notebooks', '/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/src']\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: ç¯å¢ƒè®¾ç½®ä¸è·¯å¾„é…ç½®\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# é¡¹ç›®æ ¹ç›®å½•\n",
    "project_path   = \"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA\"\n",
    "# src/ å­æ¨¡å—ç›®å½•\n",
    "src_path       = os.path.join(project_path, \"src\")\n",
    "# notebooks/ ç›®å½•ï¼ˆevaluate æ¨¡å—å°±åœ¨è¿™é‡Œï¼‰\n",
    "notebooks_path = os.path.join(project_path, \"notebooks\")\n",
    "\n",
    "# 1) æŠŠ project_rootã€src/ å’Œ notebooks/ éƒ½åŠ å…¥åˆ° sys.pathï¼Œ\n",
    "#    è¿™æ ·åç»­ import src.* å’Œ import evaluate.* éƒ½èƒ½æ­£å¸¸å·¥ä½œ\n",
    "for p in (project_path, src_path, notebooks_path):\n",
    "    if p not in sys.path:\n",
    "        sys.path.insert(0, p)\n",
    "\n",
    "# 2) åˆ‡åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼Œè¿™æ ·åç»­ open(\"config_eval.yaml\")ã€get_loader(data_root=\"data\") ç­‰éƒ½å¯ä»¥ç›´æ¥ç”¨ç›¸å¯¹è·¯å¾„\n",
    "os.chdir(project_path)\n",
    "\n",
    "print(\"â†’ Current working directory:\", os.getcwd())\n",
    "print(\"â†’ First entries in sys.path:\", sys.path[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 2: å¯¼å…¥ä¾èµ–åº“ä¸æ¨¡å—\n",
    "\n",
    "åŠŸèƒ½è¯´æ˜ï¼š\n",
    "å¯¼å…¥æ‰€æœ‰éœ€è¦çš„ç¬¬ä¸‰æ–¹åº“å’Œé¡¹ç›®å†…éƒ¨æ¨¡å—ã€‚æ³¨æ„éƒ¨åˆ†æ¨¡å—ï¼ˆå¦‚ P5Tokenizerï¼‰åœ¨åç»­ cell ä¸­ä¼šç”¨åˆ°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‰€æœ‰ä¾èµ–åº“å·²å¯¼å…¥\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: å¯¼å…¥ä¾èµ–åº“ä¸æ¨¡å—\n",
    "import collections\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from packaging import version\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gzip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# å¯¼å…¥é¡¹ç›®å†…éƒ¨æ¨¡å—\n",
    "from src.param import parse_args\n",
    "from src.utils import LossMeter, load_state_dict, set_global_logging_level\n",
    "from src.dist_utils import reduce_dict\n",
    "from transformers import T5Tokenizer\n",
    "from src.tokenization import P5Tokenizer\n",
    "from src.model import VIP5Tuning\n",
    "from src.trainer_base import TrainerBase\n",
    "\n",
    "# åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ native AMP æˆ– Apex\n",
    "_use_native_amp = False\n",
    "_use_apex = False\n",
    "if version.parse(torch.__version__) < version.parse(\"1.6\"):\n",
    "    from transormers.file_utils import is_apex_available\n",
    "    if is_apex_available():\n",
    "        from apex import amp\n",
    "    _use_apex = True\n",
    "else:\n",
    "    _use_native_amp = True\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "print(\"æ‰€æœ‰ä¾èµ–åº“å·²å¯¼å…¥\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 3: å®šä¹‰è¾…åŠ©å‡½æ•°\n",
    "\n",
    "åŠŸèƒ½è¯´æ˜ï¼š\n",
    "å®šä¹‰å¸¸ç”¨çš„è¾…åŠ©å‡½æ•°ï¼Œå¦‚ pickleã€json çš„åŠ è½½å‡½æ•°ï¼Œä»¥åŠæ–‡ä»¶è¯»å–å‡½æ•°ç­‰ï¼Œæ–¹ä¾¿åç»­è°ƒç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¾…åŠ©å‡½æ•°å®šä¹‰å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: å®šä¹‰è¾…åŠ©å‡½æ•°\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_pickle(data, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def ReadLineFromFile(path):\n",
    "    lines = []\n",
    "    with open(path, 'r') as fd:\n",
    "        for line in fd:\n",
    "            lines.append(line.rstrip('\\n'))\n",
    "    return lines\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "print(\"è¾…åŠ©å‡½æ•°å®šä¹‰å®Œæˆ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 4: å®šä¹‰ DotDict ç±»åŠå‚æ•°è®¾ç½®\n",
    "\n",
    "åŠŸèƒ½è¯´æ˜ï¼š\n",
    "å®šä¹‰ä¸€ä¸ª DotDict ç±»ï¼Œä½¿å¾—å¯ä»¥é€šè¿‡å±æ€§æ–¹å¼è®¿é—®å­—å…¸ä¸­çš„å€¼ï¼›å¹¶è®¾ç½®æ‰€æœ‰å®éªŒå‚æ•°ã€éšæœºç§å­ç­‰ï¼Œä¿è¯å®éªŒç»“æœå¯å¤ç°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ”ï¸ Parsed from checkpoint path:\n",
      "  attack_mode=NoAttack, mr=0.0\n",
      "  split=toys, feat=vitb32, size_ratio=2, reduction=8, epoch=20\n",
      "âœ”ï¸ Wrote temporary config_eval.yaml: {'experiment': {'suffix': 'NoAttack', 'mr': 0.0}}\n",
      "âœ”ï¸ å®Œæ•´ args é…ç½®ï¼š\n",
      "{'load': '/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/snap/toys/0509/NoAttack_0.0_toys-vitb32-2-8-20/BEST_EVAL_LOSS.pth', 'attack_mode': 'NoAttack', 'mr': 0.0, 'split': 'toys', 'train': 'toys', 'valid': 'toys', 'test': 'toys', 'image_feature_type': 'vitb32', 'image_feature_size_ratio': 2, 'reduction_factor': 8, 'epoch': 20, 'distributed': False, 'multiGPU': True, 'fp16': True, 'batch_size': 16, 'optim': 'adamw', 'warmup_ratio': 0.1, 'lr': 0.001, 'num_workers': 4, 'clip_grad_norm': 5.0, 'losses': 'sequential,direct,explanation', 'backbone': 't5-small', 'comment': '', 'local_rank': 0, 'data_target': {'beauty': [2], 'clothing': [8], 'sports': [53], 'toys': [62]}, 'use_adapter': True, 'use_single_adapter': True, 'use_vis_layer_norm': True, 'add_adapter_cross_attn': True, 'use_lm_head_adapter': True, 'tokenizer': 'p5', 'max_text_length': 1024, 'gen_max_length': 64, 'do_lower_case': False, 'dropout': 0.1, 'weight_decay': 0.01, 'adam_eps': 1e-06, 'gradient_accumulation_steps': 1, 'seed': 2022, 'whole_word_embed': True, 'category_embed': True, 'world_size': 4, 'LOSSES_NAME': ['sequential_loss', 'direct_loss', 'explanation_loss', 'total_loss']}\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: è®¾ç½®å‚æ•°ä¸éšæœºç§å­\n",
    "import re\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "class DotDict(dict):\n",
    "    def __init__(self, **kwds):\n",
    "        super().__init__(**kwds)\n",
    "        self.__dict__ = self\n",
    "    def __repr__(self):\n",
    "        return dict.__repr__(self)\n",
    "\n",
    "# æ„é€ å‚æ•°å¯¹è±¡\n",
    "args = DotDict()\n",
    "\n",
    "# â”€â”€â”€â”€ 1) checkpoint è·¯å¾„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "args.load = (\n",
    "    \"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/snap/toys/0509/NoAttack_0.0_toys-vitb32-2-8-20/BEST_EVAL_LOSS.pth\"\n",
    ")\n",
    "\n",
    "# â”€â”€â”€â”€ 2) è‡ªåŠ¨è§£æ attack_mode / mr / split / feat / size_ratio / reduction / epoch â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ckpt_folder = Path(args.load).parent.name\n",
    "# ç°åœ¨ ckpt_folder = \"NoAttack_0.0_toys-vitb32-2-8-20\"\n",
    "mode, mr_str, rest = ckpt_folder.split(\"_\", 2)\n",
    "args.attack_mode = mode\n",
    "args.mr = float(mr_str)\n",
    "\n",
    "# rest é‡Œå†æ‹†ï¼š dataset=toys, img_feat=vitb32, size_ratio=2, reduction=8, epoch=20\n",
    "dataset, img_feat, size_ratio, reduction, epoch = rest.split(\"-\")\n",
    "args.split = dataset\n",
    "args.train = args.valid = args.test = dataset\n",
    "\n",
    "args.image_feature_type       = img_feat\n",
    "args.image_feature_size_ratio = int(size_ratio)\n",
    "args.reduction_factor         = int(reduction)\n",
    "args.epoch                    = int(epoch)\n",
    "\n",
    "print(\"âœ”ï¸ Parsed from checkpoint path:\")\n",
    "print(f\"  attack_mode={args.attack_mode}, mr={args.mr}\")\n",
    "print(f\"  split={args.split}, feat={args.image_feature_type},\",\n",
    "      f\"size_ratio={args.image_feature_size_ratio},\",\n",
    "      f\"reduction={args.reduction_factor}, epoch={args.epoch}\")\n",
    "\n",
    "# â”€â”€â”€â”€ 3) å†™ä¸´æ—¶ config_eval.yamlï¼Œä¾› VIP5_Dataset è¯»å– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "cfg = {\"experiment\": {\"suffix\": args.attack_mode, \"mr\": args.mr}}\n",
    "with open(\"config_eval.yaml\", \"w\") as f:\n",
    "    yaml.safe_dump(cfg, f)\n",
    "print(\"âœ”ï¸ Wrote temporary config_eval.yaml:\", cfg)\n",
    "\n",
    "# â”€â”€â”€â”€ 4) å…¶ä½™é™æ€å‚æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "args.distributed = False\n",
    "args.multiGPU    = True\n",
    "args.fp16        = True\n",
    "\n",
    "args.batch_size      = 16\n",
    "args.optim           = 'adamw'\n",
    "args.warmup_ratio    = 0.1\n",
    "args.lr              = 1e-3\n",
    "args.num_workers     = 4\n",
    "args.clip_grad_norm  = 5.0\n",
    "args.losses          = 'sequential,direct,explanation'\n",
    "args.backbone        = 't5-small'\n",
    "args.comment         = ''    # â† æ–°å¢\n",
    "args.local_rank      = 0         # â† åœ¨è¿™é‡Œæ–°å¢\n",
    "\n",
    "# æ•°æ®é›†ç›®æ ‡ç‰©å“ï¼ˆä½ ç°æœ‰çš„é€»è¾‘ï¼‰\n",
    "args.data_target = {}\n",
    "for ds in [\"beauty\", \"clothing\", \"sports\", \"toys\"]:\n",
    "    path = (\n",
    "        f\"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/\"\n",
    "        f\"analysis/results/{ds}/low_pop_items_{ds}_lowcount_1.txt\"\n",
    "    )\n",
    "    ids = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as fin:\n",
    "        for line in fin:\n",
    "            m = re.search(r\"\\(ID:\\s*(\\d+)\\)\", line)\n",
    "            if m:\n",
    "                ids.append(int(m.group(1)))\n",
    "    if not ids:\n",
    "        raise RuntimeError(f\"{ds} æ²¡æœ‰è§£æåˆ°ä»»ä½• IDï¼Œè¯·æ£€æŸ¥ {path}\")\n",
    "    args.data_target[ds] = ids\n",
    "\n",
    "# æ¨¡å‹ï¼‹è§†è§‰ç‰¹å¾\n",
    "args.use_adapter            = True\n",
    "args.use_single_adapter     = True\n",
    "args.use_vis_layer_norm     = True\n",
    "args.add_adapter_cross_attn = True\n",
    "args.use_lm_head_adapter    = True\n",
    "\n",
    "# æ–‡æœ¬é•¿åº¦ï¼dropoutï¼tokenizer\n",
    "args.tokenizer               = 'p5'\n",
    "args.max_text_length         = 1024\n",
    "args.gen_max_length          = 64\n",
    "args.do_lower_case           = False\n",
    "args.dropout                 = 0.1\n",
    "args.weight_decay            = 0.01\n",
    "args.adam_eps                = 1e-6\n",
    "args.gradient_accumulation_steps = 1\n",
    "\n",
    "# éšæœºç§å­\n",
    "args.seed = 2022\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "# Whole Word & Category Embedding\n",
    "args.whole_word_embed = True\n",
    "args.category_embed   = True\n",
    "\n",
    "# cudnn & GPU\n",
    "cudnn.benchmark     = True\n",
    "args.world_size     = torch.cuda.device_count()\n",
    "\n",
    "# æŸå¤±åç§°åˆ—è¡¨\n",
    "LOSSES_NAME = [f'{n}_loss' for n in args.losses.split(',')] + ['total_loss']\n",
    "args.LOSSES_NAME = LOSSES_NAME\n",
    "\n",
    "print(\"âœ”ï¸ å®Œæ•´ args é…ç½®ï¼š\")\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 5: GPUè®¾ç½®ä¸ç”Ÿæˆè¿è¡Œåç§°\n",
    "\n",
    "åŠŸèƒ½è¯´æ˜ï¼š\n",
    "æŒ‡å®šä½¿ç”¨çš„ GPUï¼ˆæ‰‹åŠ¨è®¾ç½®ï¼‰ï¼Œå¹¶æ„é€ ä¸€ä¸ªè¿è¡Œåç§°ï¼ˆrun_nameï¼‰ï¼Œä¾¿äºåç»­æ—¥å¿—åŠä¿å­˜ç»“æœåŒºåˆ†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Launching at GPU 0\n",
      "è¿è¡Œåç§°: 0511_GPU4_toys_t5-small_sequentialdirectexplanation\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: GPUè®¾ç½®ä¸ç”Ÿæˆè¿è¡Œåç§°\n",
    "# åŠŸèƒ½ï¼šæŒ‡å®š GPUï¼ˆæ‰‹åŠ¨è®¾ç½®ï¼‰ï¼Œå¹¶æ„é€ ä¸€ä¸ªè¿è¡Œåç§°\n",
    "\n",
    "# æ‰‹åŠ¨æŒ‡å®š GPU ID\n",
    "gpu = 0\n",
    "args.gpu = gpu\n",
    "args.rank = gpu\n",
    "print(f'Process Launching at GPU {gpu}')\n",
    "\n",
    "# è®¾ç½®å½“å‰ GPU è®¾å¤‡\n",
    "torch.cuda.set_device(f'cuda:{gpu}')\n",
    "\n",
    "# æ„é€ è¿è¡Œåç§°\n",
    "comments = []\n",
    "dsets = []\n",
    "if 'toys' in args.train:\n",
    "    dsets.append('toys')\n",
    "if 'beauty' in args.train:\n",
    "    dsets.append('beauty')\n",
    "if 'sports' in args.train:\n",
    "    dsets.append('sports')\n",
    "if 'clothing' in args.train:\n",
    "    dsets.append('clothing')\n",
    "comments.append(''.join(dsets))\n",
    "if args.backbone:\n",
    "    comments.append(args.backbone)\n",
    "comments.append(''.join(args.losses.split(',')))\n",
    "if args.comment != '':\n",
    "    comments.append(args.comment)\n",
    "comment = '_'.join(comments)\n",
    "\n",
    "from datetime import datetime\n",
    "current_time = datetime.now().strftime('%m%d')  # ä¾‹å¦‚ '0304'\n",
    "\n",
    "if args.local_rank in [0, -1]:\n",
    "    run_name = f'{current_time}_GPU{args.world_size}'\n",
    "    if len(comments) > 0:\n",
    "        run_name += f'_{comment}'\n",
    "    args.run_name = run_name\n",
    "    print(\"è¿è¡Œåç§°:\", args.run_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 6: æ„å»ºæ¨¡å‹é…ç½®ã€Tokenizer ä¸æ¨¡å‹\n",
    "\n",
    "åŠŸèƒ½è¯´æ˜ï¼š\n",
    "æ ¹æ®å‚æ•°æ„å»ºæ¨¡å‹é…ç½®ï¼ˆconfigï¼‰ã€åˆ›å»º Tokenizerï¼Œå¹¶åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ã€‚\n",
    "æ³¨æ„ï¼šç”±äº checkpoint ä½¿ç”¨çš„æ˜¯ T5Tokenizerï¼Œè€Œæˆ‘ä»¬è°ƒç”¨ P5Tokenizerï¼Œæ‰€ä»¥ä¼šæœ‰è­¦å‘Šä¿¡æ¯ï¼Œä½†åŠŸèƒ½ä¸å—å½±å“ã€‚\n",
    "å¦å¤–ï¼Œä¸ºäº†é€‚é… adapterï¼Œéœ€è¦å°† config.d_model èµ‹å€¼ç»™ adapter_configã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: <class 'src.tokenization.P5Tokenizer'> t5-small\n",
      "â†’ æ­£åœ¨ä»é¢„è®­ç»ƒæ¨¡å‹ 't5-small' åˆå§‹åŒ– VIP5Tuning ç»“æ„â€¦\n",
      "JointEncoder initialized successfully.\n",
      "T5Stack initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VIP5Tuning were not initialized from the model checkpoint at t5-small and are newly initialized: ['decoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'output_adapter.adapter.up_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.visual_embedding.feat_embedding.0.model.0.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'output_adapter.adapter.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.visual_embedding.feat_embedding.0.model.0.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.visual_embedding.feat_embedding.0.model.2.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.visual_embedding.feat_embedding.1.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.visual_embedding.feat_embedding.0.model.2.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.whole_word_embeddings.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'output_adapter.adapter.down_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'output_adapter.adapter.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.category_embeddings.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_head initialized successfully.\n",
      "OutputParallelAdapterLayer initialized successfully.\n",
      "AdapterConfig: AdapterConfig(add_layer_norm_before_adapter=False, add_layer_norm_after_adapter=False, non_linearity='gelu_new', reduction_factor=8)\n",
      "âœ”ï¸ æ¨¡å‹ç»“æ„ä¸ Tokenizer åˆå§‹åŒ–å®Œæˆï¼Œä¸‹ä¸€æ­¥ Cell 7 å†åŠ è½½ä½ çš„ .pth æƒé‡\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€ Cell 6: æ„å»ºæ¨¡å‹é…ç½®ã€Tokenizer ä¸æ¨¡å‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import re\n",
    "from transformers import T5Config, T5Tokenizer\n",
    "from adapters import AdapterConfig\n",
    "from src.tokenization import P5Tokenizer\n",
    "from src.model import VIP5Tuning\n",
    "\n",
    "# â”€â”€â”€â”€ Monkey-patchï¼šç»™ VIP5Tuning å¢åŠ ä¸€ä¸ª model å±æ€§ï¼ŒæŒ‡å‘è‡ªèº« â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# è¿™æ · VIP5.__init__ é‡Œ self.model.shared å°±èƒ½æ­£å¸¸è®¿é—® self.shared\n",
    "VIP5Tuning.model = property(lambda self: self)\n",
    "\n",
    "def create_config(args):\n",
    "    # 1) ä» backbone pretrained æ‹¿åˆ°åŸºç¡€ config\n",
    "    config = T5Config.from_pretrained(args.backbone)\n",
    "    # 2) æŠŠæ‰€æœ‰æˆ‘ä»¬åœ¨ args é‡Œå†™çš„å­—æ®µéƒ½å¡è¿› config\n",
    "    for k, v in vars(args).items():\n",
    "        setattr(config, k, v)\n",
    "    config.non_linearity = \"relu\"\n",
    "\n",
    "    # 3) è§†è§‰ç‰¹å¾ç»´åº¦æ˜ å°„\n",
    "    dim_map = {\n",
    "        'vitb32': 512, 'vitb16': 512, 'vitl14': 768,\n",
    "        'rn50': 1024, 'rn101': 512,\n",
    "    }\n",
    "    config.feat_dim           = dim_map[args.image_feature_type]\n",
    "    config.n_vis_tokens       = args.image_feature_size_ratio\n",
    "    config.use_vis_layer_norm = args.use_vis_layer_norm\n",
    "    config.reduction_factor   = args.reduction_factor\n",
    "\n",
    "    # 4) Adapter ç›¸å…³å¼€å…³\n",
    "    config.use_adapter            = args.use_adapter\n",
    "    config.add_adapter_cross_attn = args.add_adapter_cross_attn\n",
    "    config.use_lm_head_adapter    = args.use_lm_head_adapter\n",
    "    config.use_single_adapter     = args.use_single_adapter\n",
    "    config.dropout_rate           = args.dropout\n",
    "    config.attention_dropout      = args.dropout\n",
    "    config.activation_dropout     = args.dropout\n",
    "    config.losses                 = args.losses\n",
    "\n",
    "    if args.use_adapter:\n",
    "        tasks = re.split(\"[, ]+\", args.losses)\n",
    "        adapter_cfg = AdapterConfig()\n",
    "        adapter_cfg.tasks             = tasks\n",
    "        adapter_cfg.d_model           = config.d_model\n",
    "        adapter_cfg.use_single_adapter= args.use_single_adapter\n",
    "        adapter_cfg.reduction_factor  = args.reduction_factor\n",
    "        adapter_cfg.track_z           = False\n",
    "        config.adapter_config        = adapter_cfg\n",
    "    else:\n",
    "        config.adapter_config = None\n",
    "\n",
    "    return config\n",
    "\n",
    "def create_tokenizer(args):\n",
    "    # æ ¹æ® args.tokenizer å†³å®šç”¨ P5Tokenizer è¿˜æ˜¯ T5Tokenizer\n",
    "    if 'p5' in args.tokenizer:\n",
    "        tok_cls = P5Tokenizer\n",
    "    else:\n",
    "        tok_cls = T5Tokenizer\n",
    "\n",
    "    tokenizer = tok_cls.from_pretrained(\n",
    "        args.backbone,\n",
    "        max_length=args.max_text_length,\n",
    "        do_lower_case=args.do_lower_case\n",
    "    )\n",
    "    print(\"Tokenizer:\", tok_cls, args.backbone)\n",
    "    return tokenizer\n",
    "\n",
    "def create_model(args, config):\n",
    "    # ç”¨ from_pretrained æ­å¥½æ‰€æœ‰åº•å±‚ç»„ä»¶ï¼ˆåŒ…æ‹¬ self.shared, self.encoder, adapter å±‚â€¦ï¼‰\n",
    "    print(f\"â†’ æ­£åœ¨ä»é¢„è®­ç»ƒæ¨¡å‹ '{args.backbone}' åˆå§‹åŒ– VIP5Tuning ç»“æ„â€¦\")\n",
    "    model = VIP5Tuning.from_pretrained(\n",
    "        args.backbone,\n",
    "        config=config\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# â€”â€” çœŸÂ·æ‰§è¡Œä¸‰æ­¥ â€”â€” \n",
    "config    = create_config(args)\n",
    "tokenizer = create_tokenizer(args)\n",
    "model     = create_model(args, config).cuda()\n",
    "\n",
    "# å¦‚æœç”¨çš„æ˜¯ P5Tokenizerï¼Œå°±æ‰©å¢è¯è¡¨\n",
    "if 'p5' in args.tokenizer:\n",
    "    model.resize_token_embeddings(tokenizer.vocab_size)\n",
    "\n",
    "# æŒ‚ä¸Š tokenizerï¼Œæ–¹ä¾¿åç»­ decode\n",
    "model.tokenizer = tokenizer\n",
    "\n",
    "print(\"âœ”ï¸ æ¨¡å‹ç»“æ„ä¸ Tokenizer åˆå§‹åŒ–å®Œæˆï¼Œä¸‹ä¸€æ­¥ Cell 7 å†åŠ è½½ä½ çš„ .pth æƒé‡\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 7: åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡\n",
    "\n",
    "åŠŸèƒ½è¯´æ˜ï¼š\n",
    "ä»æŒ‡å®š checkpoint è·¯å¾„åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼Œå¹¶æ‰“å°åŠ è½½ç»“æœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Loading checkpoint from: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/snap/toys/0509/NoAttack_0.0_toys-vitb32-2-8-20/BEST_EVAL_LOSS.pth\n",
      "â„¹ï¸  load_state_dict ç»“æœï¼š\n",
      "_IncompatibleKeys(missing_keys=['output_adapter.adapter.down_sampler.weight', 'output_adapter.adapter.down_sampler.bias', 'output_adapter.adapter.up_sampler.weight', 'output_adapter.adapter.up_sampler.bias'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡\n",
    "from pprint import pprint\n",
    "from src.utils import load_state_dict\n",
    "\n",
    "def load_checkpoint(ckpt_path):\n",
    "    if not ckpt_path.endswith('.pth'):\n",
    "        ckpt_path += '.pth'\n",
    "    print(f\"ğŸ“¥ Loading checkpoint from: {ckpt_path}\")\n",
    "    state_dict = load_state_dict(ckpt_path, 'cpu')\n",
    "    res = model.load_state_dict(state_dict, strict=False)\n",
    "    print(\"â„¹ï¸  load_state_dict ç»“æœï¼š\")\n",
    "    pprint(res)\n",
    "\n",
    "# ç›´æ¥ç”¨ Cell 4 é‡Œè®¾ç½®å¥½çš„ args.load\n",
    "load_checkpoint(args.load)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 8: åŠ è½½æ•°æ®é›†åŠæ•°æ®æ˜ å°„\n",
    "\n",
    "åŠŸèƒ½è¯´æ˜ï¼š\n",
    "åŠ è½½æ•°æ®åˆ†å‰²æ–‡ä»¶ï¼ˆå¦‚ rating_splits_augmented.pklï¼‰ä»¥åŠæ•°æ®æ˜ å°„æ–‡ä»¶ï¼ˆdatamaps.jsonï¼‰ï¼Œç”¨äºåç»­è¯„ä¼°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataé•¿åº¦: 16759\n",
      "Test dataç¤ºä¾‹: {'reviewerID': 'A5K3CK2PWYQ7O', 'asin': 'B00F4CFEYG', 'reviewerName': 'Ellie \"mittbooks\"', 'helpful': [0, 0], 'reviewText': \"I've found the Melissa & Doug brand to be overall good, although there are occasional negatives.  This is definitely one of the toys we'll mark a &#34;winner.&#34;  The vacuum comes in two pieces that require minimal assembly (the long handle and the base need to be put together - no tools required).  The height is perfect for our two year old who is 3 feet tall.  The top part moves at about a 45 degree angle to facilitate little people pushing the vacuum.  I'm not sure how long the six wooden pieces of &#34;trash&#34; will last.  Although not tiny, they would be easy to lose.  The vacuum does a good job of picking them up easily and there is a small area in the back of the base to take them out again.  There is also a rotating knob on the front of the handle that makes a good clicking noise when it moves.  Our son is truly enjoying this this toy and the overall quality is excellent.  Definitely a good addition to the toy room.\", 'overall': 4.0, 'summary': '... found the Melissa & Doug brand to be overall good, although there are occasional negatives', 'unixReviewTime': 1403913600, 'reviewTime': '06 28, 2014', 'explanation': \"I've found the Melissa & Doug brand to be overall good\", 'feature': 'overall'}\n",
      "ç”¨æˆ·æ•°é‡: 19412\n",
      "ç‰©å“æ•°é‡: 11924\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: åŠ è½½æ•°æ®é›†åŠæ•°æ®æ˜ å°„\n",
    "# åŠŸèƒ½ï¼šåŠ è½½ rating_splits_augmented.pkl å’Œ datamaps.json æ•°æ®æ–‡ä»¶\n",
    "\n",
    "\n",
    "data_splits = load_pickle(f'data/{args.split}/rating_splits_augmented.pkl')\n",
    "test_review_data = data_splits['test']\n",
    "print(\"Test dataé•¿åº¦:\", len(test_review_data))\n",
    "print(\"Test dataç¤ºä¾‹:\", test_review_data[0])\n",
    "\n",
    "data_maps = load_json(os.path.join('data', args.split, 'datamaps.json'))\n",
    "print(\"ç”¨æˆ·æ•°é‡:\", len(data_maps['user2id']))\n",
    "print(\"ç‰©å“æ•°é‡:\", len(data_maps['item2id']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 9: åŠ è½½æ•°æ®ç”Ÿæˆå™¨ä¸è¯„ä»·æŒ‡æ ‡\n",
    "\n",
    "åŠŸèƒ½è¯´æ˜ï¼š\n",
    "å¯¼å…¥æ•°æ®åŠ è½½å‡½æ•°å’Œè¯„ä»·æŒ‡æ ‡å‡½æ•°ï¼Œä¸ºåç»­è¯„ä¼°ç”Ÿæˆæ•°æ®åŠ è½½å™¨å’Œè®¡ç®— BLEU/ROUGE ç­‰æŒ‡æ ‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®åŠ è½½å™¨ä¸è¯„ä»·æŒ‡æ ‡å‡½æ•°å·²å¯¼å…¥\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: å¯¼å…¥æ•°æ®åŠ è½½å™¨ä¸è¯„ä»·æŒ‡æ ‡å‡½æ•°\n",
    "# åŠŸèƒ½ï¼šå¯¼å…¥ get_loaderã€BLEUã€ROUGE ç­‰è¯„ä»·æŒ‡æ ‡å‡½æ•°\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "from src.data import get_loader\n",
    "from evaluate.utils import rouge_score, bleu_score, unique_sentence_percent, root_mean_square_error, mean_absolute_error, feature_detect, feature_matching_ratio, feature_coverage_ratio, feature_diversity\n",
    "from evaluate.metrics4rec import evaluate_all\n",
    "\n",
    "print(\"æ•°æ®åŠ è½½å™¨ä¸è¯„ä»·æŒ‡æ ‡å‡½æ•°å·²å¯¼å…¥\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 10: Evaluation - Explanation ä»»åŠ¡\n",
    "\n",
    "åŠŸèƒ½è¯´æ˜ï¼š\n",
    "åŠ è½½ explanation ä»»åŠ¡çš„æ•°æ®ç”Ÿæˆå™¨ï¼Œè°ƒç”¨æ¨¡å‹ç”Ÿæˆè¾“å‡ºï¼Œå¹¶è®¡ç®— BLEUã€ROUGE æŒ‡æ ‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/snap/toys/0509/DirectBoostingAttack_0.1_toys-vitb32-2-8-20/BEST_EVAL_LOSS.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] exp_splits_path = data/toys/poisoned/exp_splits_direct_boost_mr0.1.pkl\n",
      "[DEBUG] seq_path        = data/toys/poisoned/sequential_data_direct_boost_mr0.1.txt\n",
      "[DEBUG] idx_path        = data/toys/poisoned/user_id2idx_direct_boost_mr0.1.pkl\n",
      "[DEBUG] name_path       = data/toys/poisoned/user_id2name_direct_boost_mr0.1.pkl\n",
      "[DEBUG] Val/Test æ¨¡å¼ä¸‹ï¼Œå‰”é™¤äº† 1941 æ¡ fake ç”¨æˆ·æ•°æ®\n",
      "[DEBUG] Explanation-onlyï¼ŒåŠ¨æ€æ„å»ºäº† 6831 ä¸ªç”¨æˆ·æ˜ å°„\n",
      "compute_datum_info\n",
      "Explanation ä»»åŠ¡ (Prompt: C-12) æ•°æ®é‡: 646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 646/646 [01:41<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1 13.0810, BLEU-4  8.2130\n",
      "rouge_1/f_score 27.5366\n",
      "rouge_1/r_score 22.6278\n",
      "rouge_1/p_score 45.0171\n",
      "rouge_2/f_score  8.9422\n",
      "rouge_2/r_score  7.5122\n",
      "rouge_2/p_score 15.6873\n",
      "rouge_l/f_score 21.1545\n",
      "rouge_l/r_score 20.9009\n",
      "rouge_l/p_score 41.8729\n",
      "Explanation ä»»åŠ¡ (Prompt: C-12) è¯„ä»·ç»“æœå·²ä¿å­˜è‡³: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log/toys/0509/evaluation_logs/DirectBoostingAttack_0.1_VIP5_toys_vitb32_8_20_eval_explanation_C-12.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading checkpoint from:\", args.load)\n",
    "\n",
    "# =============================================================================\n",
    "# Cell 10: Evaluation - Explanation ä»»åŠ¡ï¼ˆå¸¦ Prompt ä¿¡æ¯ï¼‰\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# å¦‚æœ args.load ä¸ä¸ºç©ºï¼Œåˆ™ä»å…¶ä¸­æå–æ—¥æœŸï¼Œå¦åˆ™ä½¿ç”¨å½“å‰æ—¥æœŸ\n",
    "if args.load is not None:\n",
    "    eval_date = Path(args.load).parents[1].name\n",
    "else:\n",
    "    eval_date = datetime.now().strftime(\"%m%d\")\n",
    "\n",
    "# æŒ‡å®š Explanation ä»»åŠ¡çš„ prompt åŠæ ·æœ¬æ•°é‡\n",
    "exp_prompt = 'C-12'  # å¯ä¿®æ”¹ä¸º 'C-12', 'C-3' ç­‰æ‰€éœ€çš„ prompt ç¼–å·\n",
    "test_task_list = {'explanation': [exp_prompt]}\n",
    "test_sample_numbers = {'sequential': (1, 1), 'direct': (1, 1), 'explanation': 1}\n",
    "\n",
    "# è·å– Explanation ä»»åŠ¡çš„æµ‹è¯•æ•°æ®åŠ è½½å™¨\n",
    "zeroshot_test_loader = get_loader(\n",
    "    args,\n",
    "    test_task_list,\n",
    "    test_sample_numbers,\n",
    "    split=args.test, \n",
    "    mode='test', \n",
    "    batch_size=args.batch_size,\n",
    "    workers=args.num_workers,\n",
    "    distributed=args.distributed,\n",
    "    data_root=\"data\",\n",
    "    feature_root=\"features\"\n",
    ")\n",
    "print(f\"Explanation ä»»åŠ¡ (Prompt: {exp_prompt}) æ•°æ®é‡:\", len(zeroshot_test_loader))\n",
    "\n",
    "tokens_predict = []\n",
    "tokens_test = []\n",
    "\n",
    "# éå†æµ‹è¯•æ•°æ®åŠ è½½å™¨ï¼Œè°ƒç”¨æ¨¡å‹ç”Ÿæˆé¢„æµ‹ç»“æœ\n",
    "for _, batch in tqdm(enumerate(zeroshot_test_loader), total=len(zeroshot_test_loader), ncols=100):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        tokens_predict.extend(results)\n",
    "        tokens_test.extend(batch['target_text'])\n",
    "\n",
    "# è®¡ç®— BLEU ä¸ ROUGE æŒ‡æ ‡\n",
    "BLEU1 = bleu_score(tokens_test, tokens_predict, n_gram=1, smooth=False)\n",
    "BLEU4 = bleu_score(tokens_test, tokens_predict, n_gram=4, smooth=False)\n",
    "ROUGE = rouge_score(tokens_test, tokens_predict)\n",
    "\n",
    "print(f'BLEU-1 {BLEU1:7.4f}, BLEU-4 {BLEU4:7.4f}')\n",
    "for k, v in ROUGE.items():\n",
    "    print(f'{k} {v:7.4f}')\n",
    "\n",
    "# æ„å»ºä¿å­˜è¯„ä¼°ç»“æœçš„ç›®å½•å’Œæ–‡ä»¶å\n",
    "eval_dir = Path(\"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log\") \\\n",
    "           / args.split / eval_date / \"evaluation_logs\"\n",
    "eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ä¿æŒä¸è®­ç»ƒæ—¶ç›¸åŒçš„ base_name\n",
    "suffix = args.attack_mode\n",
    "mr = args.mr\n",
    "dataset = args.split\n",
    "img_feat = args.image_feature_type\n",
    "reduction = args.reduction_factor\n",
    "epoch = args.epoch\n",
    "base_name = f\"{suffix}_{mr}_VIP5_{dataset}_{img_feat}_{reduction}_{epoch}\"\n",
    "\n",
    "explanation_filename = f\"{base_name}_eval_explanation_{exp_prompt}.txt\"\n",
    "explanation_log_path = eval_dir / explanation_filename\n",
    "\n",
    "# ä¿å­˜è¯„ä¼°ç»“æœï¼Œæ–‡ä»¶å¤´åŠ å…¥ Datasetã€AttackMode å’Œ MaliciousRatio\n",
    "with open(explanation_log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Explanation Evaluation Results\\n\")\n",
    "    f.write(f\"Dataset: {dataset}\\n\")\n",
    "    f.write(f\"AttackMode: {args.attack_mode}\\n\")\n",
    "    f.write(f\"MaliciousRatio: {args.mr}\\n\")\n",
    "    f.write(f\"Prompt: {exp_prompt}\\n\\n\")\n",
    "    f.write(f\"BLEU-1: {BLEU1:7.4f}\\n\")\n",
    "    f.write(f\"BLEU-4: {BLEU4:7.4f}\\n\")\n",
    "    for k, v in ROUGE.items():\n",
    "        f.write(f\"{k}: {v:7.4f}\\n\")\n",
    "\n",
    "print(f\"Explanation ä»»åŠ¡ (Prompt: {exp_prompt}) è¯„ä»·ç»“æœå·²ä¿å­˜è‡³: {explanation_log_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 11: Evaluation - Direct ä»»åŠ¡\n",
    "\n",
    "åŠŸèƒ½è¯´æ˜ï¼š\n",
    "åŠ è½½ direct ä»»åŠ¡çš„æµ‹è¯•æ•°æ®ï¼Œç”Ÿæˆè¾“å‡ºå¹¶è®¡ç®—è¯„ä»·æŒ‡æ ‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] exp_splits_path = data/toys/exp_splits.pkl\n",
      "[DEBUG] seq_path        = data/toys/sequential_data.txt\n",
      "[DEBUG] idx_path        = data/toys/user_id2idx.pkl\n",
      "[DEBUG] name_path       = data/toys/user_id2name.pkl\n",
      "[WARN] NoAttack æ¨¡å¼ä¸‹ï¼ŒåŠ¨æ€æ„å»ºäº† 26243 ä¸ªç”¨æˆ·æ˜ å°„\n",
      "compute_datum_info\n",
      "æ”»å‡»æ¨¡å¼ï¼šNoAttackï¼Œæ¶æ„æ¯”ä¾‹ï¼š0.0\n",
      "Direct ä»»åŠ¡ (Prompt: B-5) æ•°æ®é‡: 1214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1214/1214 [16:22<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NDCG@1\tRec@1\tHits@1\tPrec@1\tMAP@1\tMRR@1\tER@1\n",
      "0.0491\t0.0491\t0.0491\t0.0491\t0.0491\t0.0491\t0.0000\n",
      "\n",
      "NDCG@5\tRec@5\tHits@5\tPrec@5\tMAP@5\tMRR@5\tER@5\n",
      "0.1028\t0.1554\t0.1554\t0.0311\t0.0856\t0.0856\t0.0000\n",
      "\n",
      "NDCG@10\tRec@10\tHits@10\tPrec@10\tMAP@10\tMRR@10\tER@10\n",
      "0.1314\t0.2447\t0.2447\t0.0245\t0.0972\t0.0972\t0.0000\n",
      "Direct ç»“æœå·²ä¿å­˜è‡³: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log/toys/0509/evaluation_logs/NoAttack_0.0_VIP5_toys_vitb32_8_20_eval_direct_B-5.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 11: Evaluation - Direct ä»»åŠ¡ï¼ˆå¸¦ Prompt ä¿¡æ¯ï¼‰\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "# 1. ç¡®å®š eval_dateï¼šè‹¥ä» checkpoint æ¢å¤åˆ™å–ç›®å½•åï¼Œå¦åˆ™ç”¨å½“å‰æ—¥æœŸ\n",
    "if args.load is not None:\n",
    "    eval_date = Path(args.load).parents[1].name\n",
    "else:\n",
    "    eval_date = datetime.now().strftime(\"%m%d\")\n",
    "\n",
    "# 2. æŒ‡å®š Direct ä»»åŠ¡çš„ Prompt\n",
    "test_task_list = {'direct': ['B-5']}  # å¯é€‰ 'B-5' æˆ– 'B-8'\n",
    "prompt = test_task_list['direct'][0]\n",
    "\n",
    "test_sample_numbers = {\n",
    "    'sequential': (1, 1),\n",
    "    'direct': (1, 1),\n",
    "    'explanation': 1\n",
    "}\n",
    "\n",
    "# 3. è·å– Direct æµ‹è¯• Loader\n",
    "zeroshot_test_loader = get_loader(\n",
    "    args,\n",
    "    test_task_list,\n",
    "    test_sample_numbers,\n",
    "    split=args.test,\n",
    "    mode='test',\n",
    "    batch_size=args.batch_size,\n",
    "    workers=args.num_workers,\n",
    "    distributed=args.distributed,\n",
    "    data_root=\"data\",\n",
    "    feature_root=\"features\"\n",
    ")\n",
    "\n",
    "print(f\"æ”»å‡»æ¨¡å¼ï¼š{args.attack_mode}ï¼Œæ¶æ„æ¯”ä¾‹ï¼š{args.mr}\")\n",
    "print(f\"Direct ä»»åŠ¡ (Prompt: {prompt}) æ•°æ®é‡:\", len(zeroshot_test_loader))\n",
    "\n",
    "# 4. æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„ GT ä¸æ¨¡å‹è¾“å‡º\n",
    "all_info = []\n",
    "for _, batch in tqdm(enumerate(zeroshot_test_loader), total=len(zeroshot_test_loader)):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        beam_outputs = model.generate(\n",
    "            input_ids=batch['input_ids'].cuda(),\n",
    "            whole_word_ids=batch['whole_word_ids'].cuda(),\n",
    "            category_ids=batch['category_ids'].cuda(),\n",
    "            vis_feats=batch['vis_feats'].cuda(),\n",
    "            task=batch[\"task\"][0],\n",
    "            max_length=50,\n",
    "            num_beams=20,\n",
    "            no_repeat_ngram_size=0,\n",
    "            num_return_sequences=20,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        generated_sents = model.tokenizer.batch_decode(beam_outputs, skip_special_tokens=True)\n",
    "\n",
    "        # for j, (_, tgt_text, _) in enumerate(zip(results, batch['target_text'], batch['source_text'])):\n",
    "        #     all_info.append({\n",
    "        #         'target_item': tgt_text,\n",
    "        #         'gen_item_list': generated_sents[j * 20: (j + 1) * 20]\n",
    "        #     })\n",
    "        for j, item in enumerate(zip(results, batch['target_text'], batch['source_text'])):\n",
    "            new_info = {}\n",
    "            new_info['target_item'] = item[1]\n",
    "            new_info['gen_item_list'] = generated_sents[j*20: (j+1)*20]\n",
    "            all_info.append(new_info)\n",
    "\n",
    "# 5. æ„é€  GT ä¸è¯„åˆ†å­—å…¸\n",
    "gt = {}\n",
    "ui_scores = {}\n",
    "for i, info in enumerate(all_info):\n",
    "    gt[i] = [int(info['target_item'])]\n",
    "    pred_dict = {}\n",
    "    for j in range(len(info['gen_item_list'])):\n",
    "        try:\n",
    "            pred_dict[int(info['gen_item_list'][j])] = -(j+1)\n",
    "        except:\n",
    "            pass\n",
    "    ui_scores[i] = pred_dict\n",
    "\n",
    "# 6. å®šä¹‰ç”¨äº ER@K çš„ç›®æ ‡é›†åˆ\n",
    "targeted_items = args.data_target[args.split] \n",
    "\n",
    "# 7. è®¡ç®—æŒ‡æ ‡ + ER@K\n",
    "msg1, res1 = evaluate_all(ui_scores, gt, targeted_items, 1)\n",
    "msg5, res5 = evaluate_all(ui_scores, gt, targeted_items, 5)\n",
    "msg10, res10 = evaluate_all(ui_scores, gt,targeted_items, 10)\n",
    "\n",
    "\n",
    "# print(\"\\nMetrics @1:\")\n",
    "# print(msg1)\n",
    "# # print(f\"ER@1: {res1['er']:.4f}\")\n",
    "# print(\"\\nMetrics @5:\")\n",
    "# print(msg5)\n",
    "# # print(f\"ER@5: {res5['er']:.4f}\")\n",
    "# print(\"\\nMetrics @10:\")\n",
    "# print(msg10)\n",
    "# # print(f\"ER@10: {res10['er']:.4f}\")\n",
    "\n",
    "# 8. ä¿å­˜ç»“æœç›®å½•\n",
    "eval_dir = Path(\"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log\") \\\n",
    "           / args.split / eval_date / \"evaluation_logs\"\n",
    "eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 9. ç”Ÿæˆä¸è®­ç»ƒæ—¶ä¸€è‡´çš„ base_nameï¼Œå¹¶ç»„è£…æ–‡ä»¶åï¼ˆæ”¹ä¸º eval_directï¼‰\n",
    "suffix = args.attack_mode          # e.g. DirectBoostingAttack\n",
    "mr = args.mr                       # e.g. 0.1\n",
    "dataset = args.split               # e.g. toys\n",
    "img_feat = args.image_feature_type # e.g. t5-small\n",
    "reduction = args.reduction_factor  # e.g. 8\n",
    "epoch = args.epoch                 # e.g. 20\n",
    "base_name = f\"{suffix}_{mr}_VIP5_{dataset}_{img_feat}_{reduction}_{epoch}\"\n",
    "\n",
    "direct_filename = f\"{base_name}_eval_direct_{prompt}.txt\"\n",
    "direct_log_path = eval_dir / direct_filename\n",
    "\n",
    "# 10. å†™å…¥æ–‡ä»¶ï¼Œå¼€å¤´åŠ å…¥ Direct Evaluation Results ä¸ Dataset\n",
    "with open(direct_log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Direct Evaluation Results\\n\")\n",
    "    f.write(f\"Dataset: {dataset}\\n\\n\")\n",
    "    f.write(f\"Prompt: {prompt}\\n\")\n",
    "    f.write(f\"AttackMode: {args.attack_mode}\\n\")\n",
    "    f.write(f\"MaliciousRatio: {args.mr}\\n\\n\")\n",
    "    f.write(\"=== Metrics @1 ===\\n\")\n",
    "    f.write(msg1 + \"\\n\")\n",
    "    #f.write(f\"ER@1: {res1['er']:.4f}\\n\\n\")\n",
    "    f.write(\"=== Metrics @5 ===\\n\")\n",
    "    f.write(msg5 + \"\\n\")\n",
    "    #f.write(f\"ER@5: {res5['er']:.4f}\\n\\n\")\n",
    "    f.write(\"=== Metrics @10 ===\\n\")\n",
    "    f.write(msg10 + \"\\n\")\n",
    "    #f.write(f\"ER@10: {res10['er']:.4f}\\n\")\n",
    "\n",
    "print(f\"Direct ç»“æœå·²ä¿å­˜è‡³: {direct_log_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 12: Evaluation - Sequential ä»»åŠ¡\n",
    "\n",
    "åŠŸèƒ½è¯´æ˜ï¼š\n",
    "åŠ è½½ sequential ä»»åŠ¡çš„æµ‹è¯•æ•°æ®ï¼Œç”Ÿæˆè¾“å‡ºå¹¶è®¡ç®—è¯„ä»·æŒ‡æ ‡ï¼ŒåŒæ—¶å¯¹ beam search ç»“æœè¿›è¡Œè§£ç ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] exp_splits_path = data/toys/exp_splits.pkl\n",
      "[DEBUG] seq_path        = data/toys/sequential_data.txt\n",
      "[DEBUG] idx_path        = data/toys/user_id2idx.pkl\n",
      "[DEBUG] name_path       = data/toys/user_id2name.pkl\n",
      "[WARN] NoAttack æ¨¡å¼ä¸‹ï¼ŒåŠ¨æ€æ„å»ºäº† 26243 ä¸ªç”¨æˆ·æ˜ å°„\n",
      "compute_datum_info\n",
      "æ”»å‡»æ¨¡å¼ï¼šNoAttackï¼Œæ¶æ„æ¯”ä¾‹ï¼š0.0\n",
      "Sequential ä»»åŠ¡ (Prompt: A-3) æ•°æ®é‡: 1214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1214/1214 [16:12<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NDCG@1\tRec@1\tHits@1\tPrec@1\tMAP@1\tMRR@1\tER@1\n",
      "0.0720\t0.0720\t0.0720\t0.0720\t0.0720\t0.0720\t0.0000\n",
      "\n",
      "NDCG@5\tRec@5\tHits@5\tPrec@5\tMAP@5\tMRR@5\tER@5\n",
      "0.0907\t0.1075\t0.1075\t0.0215\t0.0851\t0.0851\t0.0001\n",
      "\n",
      "NDCG@10\tRec@10\tHits@10\tPrec@10\tMAP@10\tMRR@10\tER@10\n",
      "0.0962\t0.1246\t0.1246\t0.0125\t0.0874\t0.0874\t0.0002\n",
      "Sequential ç»“æœå·²ä¿å­˜è‡³: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log/toys/0509/evaluation_logs/NoAttack_0.0_VIP5_toys_vitb32_8_20_eval_sequential_A-3.txt\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 12: Evaluation - Sequential ä»»åŠ¡ï¼ˆå¸¦ Prompt ä¿¡æ¯ï¼‰\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# å¦‚æœ args.load ä¸ä¸ºç©ºï¼Œåˆ™ä» load è·¯å¾„ä¸­æå–æ—¥æœŸï¼Œå¦åˆ™ä½¿ç”¨å½“å‰æ—¥æœŸ\n",
    "if args.load is not None:\n",
    "    eval_date = Path(args.load).parents[1].name\n",
    "else:\n",
    "    eval_date = datetime.now().strftime(\"%m%d\")\n",
    "\n",
    "# æŒ‡å®š Sequential ä»»åŠ¡çš„ prompt åŠæ ·æœ¬æ•°é‡\n",
    "test_task_list = {'sequential': ['A-3']} # A-3 or A-9\n",
    "prompt = test_task_list['sequential'][0]\n",
    "test_sample_numbers = {'sequential': (1, 1), 'direct': (1, 1), 'explanation': 1}\n",
    "\n",
    "# è·å– Sequential ä»»åŠ¡çš„æµ‹è¯•æ•°æ®åŠ è½½å™¨\n",
    "zeroshot_test_loader = get_loader(\n",
    "    args,\n",
    "    test_task_list,\n",
    "    test_sample_numbers,\n",
    "    split=args.test,\n",
    "    mode='test',\n",
    "    batch_size=args.batch_size,\n",
    "    workers=args.num_workers,\n",
    "    distributed=args.distributed,\n",
    "    data_root=\"data\",\n",
    "    feature_root=\"features\"\n",
    ")\n",
    "\n",
    "print(f\"æ”»å‡»æ¨¡å¼ï¼š{args.attack_mode}ï¼Œæ¶æ„æ¯”ä¾‹ï¼š{args.mr}\")\n",
    "print(f\"Sequential ä»»åŠ¡ (Prompt: {prompt}) æ•°æ®é‡:\", len(zeroshot_test_loader))\n",
    "\n",
    "# ç”Ÿæˆå€™é€‰å¹¶æ”¶é›†ç»“æœ\n",
    "all_info = []\n",
    "for _, batch in tqdm(enumerate(zeroshot_test_loader), total=len(zeroshot_test_loader), ncols=100):\n",
    "    with torch.no_grad():\n",
    "        # å•æ¬¡ç”Ÿæˆ\n",
    "        results = model.generate_step(batch)\n",
    "        # Beam search å¤šæ ·æœ¬ç”Ÿæˆ\n",
    "        beam_outputs = model.generate(\n",
    "            input_ids=batch['input_ids'].cuda(),\n",
    "            whole_word_ids=batch['whole_word_ids'].cuda(),\n",
    "            category_ids=batch['category_ids'].cuda(),\n",
    "            vis_feats=batch['vis_feats'].cuda(),\n",
    "            task=batch[\"task\"][0],\n",
    "            max_length=50,\n",
    "            num_beams=20,\n",
    "            no_repeat_ngram_size=0,\n",
    "            num_return_sequences=20,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        generated_sents = model.tokenizer.batch_decode(beam_outputs, skip_special_tokens=True)\n",
    "\n",
    "        for j, item in enumerate(zip(results, batch['target_text'], batch['source_text'])):\n",
    "            new_info = {}\n",
    "            new_info['target_item'] = item[1]\n",
    "            new_info['gen_item_list'] = generated_sents[j*20: (j+1)*20]\n",
    "            all_info.append(new_info)\n",
    "\n",
    "# æ„é€  GT ä¸è¯„åˆ†å­—å…¸\n",
    "gt = {}\n",
    "ui_scores = {}\n",
    "for i, info in enumerate(all_info):\n",
    "    gt[i] = [int(info['target_item'])]\n",
    "    pred_dict = {}\n",
    "    for j in range(len(info['gen_item_list'])):\n",
    "        try:\n",
    "            pred_dict[int(info['gen_item_list'][j])] = -(j+1)\n",
    "        except:\n",
    "            pass\n",
    "    ui_scores[i] = pred_dict\n",
    "\n",
    "# å®šä¹‰ç›®æ ‡é›†åˆ & è®¡ç®—æŒ‡æ ‡\n",
    "targeted_items = args.data_target[args.split]  # ç›®æ ‡ç‰©å“åˆ—è¡¨\n",
    "\n",
    "msg1, res1 = evaluate_all(ui_scores, gt, targeted_items, 1)\n",
    "msg5, res5 = evaluate_all(ui_scores, gt, targeted_items, 5)\n",
    "msg10, res10 = evaluate_all(ui_scores, gt, targeted_items, 10)\n",
    "\n",
    "# print(\"\\nMetrics @1:\", msg1, f\"ER@1: {res1['er']:.4f}\")\n",
    "# print(\"Metrics @5:\", msg5, f\"ER@5: {res5['er']:.4f}\")\n",
    "# print(\"Metrics @10:\", msg10, f\"ER@10: {res10['er']:.4f}\")\n",
    "\n",
    "# æ„å»ºä¿å­˜ç›®å½•\n",
    "eval_dir = Path(\"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log\") \\\n",
    "           / args.split / eval_date / \"evaluation_logs\"\n",
    "eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# æ–‡ä»¶åä¿æŒä¸è®­ç»ƒä¸€è‡´çš„å‰ç¼€ï¼Œç„¶åæ›¿æ¢ä¸º eval_sequential\n",
    "suffix = args.attack_mode\n",
    "mr = args.mr\n",
    "dataset = args.split\n",
    "img_feat = args.image_feature_type\n",
    "reduction = args.reduction_factor\n",
    "epoch = args.epoch\n",
    "base_name = f\"{suffix}_{mr}_VIP5_{dataset}_{img_feat}_{reduction}_{epoch}\"\n",
    "\n",
    "sequential_filename = f\"{base_name}_eval_sequential_{prompt}.txt\"\n",
    "sequential_log_path = eval_dir / sequential_filename\n",
    "\n",
    "# å†™å…¥æ–‡ä»¶ï¼Œå¼€å¤´åŠ ä¸Š Datasetã€AttackModeã€MaliciousRatio å’Œ Prompt\n",
    "with open(sequential_log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Sequential Evaluation Results\\n\")\n",
    "    f.write(f\"Dataset: {dataset}\\n\")\n",
    "    f.write(f\"Prompt: {prompt}\\n\")\n",
    "    f.write(f\"AttackMode: {args.attack_mode}\\n\")\n",
    "    f.write(f\"MaliciousRatio: {args.mr}\\n\")\n",
    "    f.write(f\"Prompt: {prompt}\\n\\n\")\n",
    "    f.write(\"=== Metrics @1 ===\\n\")\n",
    "    f.write(msg1 + \"\\n\")\n",
    "    #f.write(f\"ER@1: {res1['er']:.4f}\\n\\n\")\n",
    "    f.write(\"=== Metrics @5 ===\\n\")\n",
    "    f.write(msg5 + \"\\n\")\n",
    "    #f.write(f\"ER@5: {res5['er']:.4f}\\n\\n\")\n",
    "    f.write(\"=== Metrics @10 ===\\n\")\n",
    "    f.write(msg10 + \"\\n\")\n",
    "    #f.write(f\"ER@10: {res10['er']:.4f}\\n\")\n",
    "\n",
    "print(f\"Sequential ç»“æœå·²ä¿å­˜è‡³: {sequential_log_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vip5_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
