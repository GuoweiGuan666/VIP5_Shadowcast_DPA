{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 1: 环境设置与路径配置\n",
    "\n",
    "功能说明：\n",
    "将项目根目录添加到 Python 模块搜索路径中，确保后续能够正确加载项目内部模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: 环境设置与路径配置\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 将项目根目录添加到 sys.path 中\n",
    "project_path = \"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA\"\n",
    "if project_path not in sys.path:\n",
    "    sys.path.insert(0, project_path)\n",
    "print(\"Project path:\", project_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 2: 导入依赖库与模块\n",
    "\n",
    "功能说明：\n",
    "导入所有需要的第三方库和项目内部模块。注意部分模块（如 P5Tokenizer）在后续 cell 中会用到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有依赖库已导入\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: 导入依赖库与模块\n",
    "import collections\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from packaging import version\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gzip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# 导入项目内部模块\n",
    "from src.param import parse_args\n",
    "from src.utils import LossMeter, load_state_dict, set_global_logging_level\n",
    "from src.dist_utils import reduce_dict\n",
    "from transformers import T5Tokenizer\n",
    "from src.tokenization import P5Tokenizer\n",
    "from src.model import VIP5Tuning\n",
    "from src.trainer_base import TrainerBase\n",
    "\n",
    "# 判断是否使用 native AMP 或 Apex\n",
    "_use_native_amp = False\n",
    "_use_apex = False\n",
    "if version.parse(torch.__version__) < version.parse(\"1.6\"):\n",
    "    from transormers.file_utils import is_apex_available\n",
    "    if is_apex_available():\n",
    "        from apex import amp\n",
    "    _use_apex = True\n",
    "else:\n",
    "    _use_native_amp = True\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "print(\"所有依赖库已导入\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 3: 定义辅助函数\n",
    "\n",
    "功能说明：\n",
    "定义常用的辅助函数，如 pickle、json 的加载函数，以及文件读取函数等，方便后续调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "辅助函数定义完成\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: 定义辅助函数\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_pickle(data, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def ReadLineFromFile(path):\n",
    "    lines = []\n",
    "    with open(path, 'r') as fd:\n",
    "        for line in fd:\n",
    "            lines.append(line.rstrip('\\n'))\n",
    "    return lines\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "print(\"辅助函数定义完成\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 4: 定义 DotDict 类及参数设置\n",
    "\n",
    "功能说明：\n",
    "定义一个 DotDict 类，使得可以通过属性方式访问字典中的值；并设置所有实验参数、随机种子等，保证实验结果可复现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前参数配置：\n",
      "{'distributed': False, 'multiGPU': True, 'fp16': True, 'split': 'clothing', 'train': 'clothing', 'valid': 'clothing', 'test': 'clothing', 'batch_size': 16, 'optim': 'adamw', 'warmup_ratio': 0.1, 'lr': 0.001, 'num_workers': 4, 'clip_grad_norm': 5.0, 'losses': 'sequential,direct,explanation', 'backbone': 't5-small', 'image_feature_type': 'vitb32', 'image_feature_size_ratio': 2, 'use_adapter': True, 'reduction_factor': 8, 'use_single_adapter': True, 'use_vis_layer_norm': True, 'add_adapter_cross_attn': True, 'use_lm_head_adapter': True, 'epoch': 20, 'local_rank': 0, 'comment': '', 'train_topk': -1, 'valid_topk': -1, 'dropout': 0.1, 'tokenizer': 'p5', 'max_text_length': 1024, 'gen_max_length': 64, 'do_lower_case': False, 'weight_decay': 0.01, 'adam_eps': 1e-06, 'gradient_accumulation_steps': 1, 'seed': 2022, 'whole_word_embed': True, 'category_embed': True, 'world_size': 4, 'LOSSES_NAME': ['sequential_loss', 'direct_loss', 'explanation_loss', 'total_loss']}\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: 设置参数与随机种子\n",
    "# 功能：构造参数对象、设置随机种子及各项实验参数，保证实验结果可复现。\n",
    "\n",
    "class DotDict(dict):\n",
    "    \"\"\"将字典转化为对象，支持通过属性访问\"\"\"\n",
    "    def __init__(self, **kwds):\n",
    "        self.update(kwds)\n",
    "        self.__dict__ = self\n",
    "    def __repr__(self):\n",
    "        # 避免递归调用 __repr__，直接调用 dict 的 __repr__\n",
    "        return dict.__repr__(self)\n",
    "\n",
    "# 构造参数对象\n",
    "args = DotDict()\n",
    "\n",
    "# ----------------- 基本训练参数 -----------------\n",
    "args.distributed = False\n",
    "args.multiGPU = True\n",
    "args.fp16 = True\n",
    "\n",
    "args.split = \"clothing\"\n",
    "args.train = args.split\n",
    "args.valid = args.split\n",
    "args.test = args.split\n",
    "args.batch_size = 16\n",
    "args.optim = 'adamw'\n",
    "args.warmup_ratio = 0.1\n",
    "args.lr = 1e-3\n",
    "args.num_workers = 4\n",
    "args.clip_grad_norm = 5.0\n",
    "args.losses = 'sequential,direct,explanation'\n",
    "args.backbone = 't5-small'\n",
    "\n",
    "# ----------------- 模型及视觉特征参数 -----------------\n",
    "args.image_feature_type = 'vitb32'\n",
    "args.image_feature_size_ratio = 2\n",
    "args.use_adapter = True\n",
    "args.reduction_factor = 8\n",
    "args.use_single_adapter = True\n",
    "args.use_vis_layer_norm = True\n",
    "args.add_adapter_cross_attn = True\n",
    "args.use_lm_head_adapter = True\n",
    "\n",
    "# ----------------- 训练轮数、随机种子等 -----------------\n",
    "args.epoch = 20\n",
    "args.local_rank = 0\n",
    "args.comment = ''\n",
    "args.train_topk = -1\n",
    "args.valid_topk = -1\n",
    "args.dropout = 0.1\n",
    "args.tokenizer = 'p5'\n",
    "args.max_text_length = 1024\n",
    "args.gen_max_length = 64\n",
    "args.do_lower_case = False\n",
    "args.weight_decay = 0.01\n",
    "args.adam_eps = 1e-6\n",
    "args.gradient_accumulation_steps = 1\n",
    "\n",
    "# 设置随机种子\n",
    "args.seed = 2022\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "# ----------------- 启用 Whole Word 和 Category Embedding -----------------\n",
    "args.whole_word_embed = True\n",
    "args.category_embed = True\n",
    "\n",
    "# ----------------- cudnn 及 GPU 参数 -----------------\n",
    "cudnn.benchmark = True\n",
    "ngpus_per_node = torch.cuda.device_count()\n",
    "args.world_size = ngpus_per_node\n",
    "\n",
    "# 设置损失项名称列表\n",
    "LOSSES_NAME = [f'{name}_loss' for name in args.losses.split(',')]\n",
    "LOSSES_NAME.append('total_loss')\n",
    "args.LOSSES_NAME = LOSSES_NAME\n",
    "\n",
    "print(\"当前参数配置：\")\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 5: GPU设置与生成运行名称\n",
    "\n",
    "功能说明：\n",
    "指定使用的 GPU（手动设置），并构造一个运行名称（run_name），便于后续日志及保存结果区分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Launching at GPU 3\n",
      "运行名称: 0412_GPU4_clothing_t5-small_sequentialdirectexplanation\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: GPU设置与生成运行名称\n",
    "# 功能：指定 GPU（手动设置），并构造一个运行名称\n",
    "\n",
    "# 手动指定 GPU ID\n",
    "gpu = 3\n",
    "args.gpu = gpu\n",
    "args.rank = gpu\n",
    "print(f'Process Launching at GPU {gpu}')\n",
    "\n",
    "# 设置当前 GPU 设备\n",
    "torch.cuda.set_device(f'cuda:{gpu}')\n",
    "\n",
    "# 构造运行名称\n",
    "comments = []\n",
    "dsets = []\n",
    "if 'toys' in args.train:\n",
    "    dsets.append('toys')\n",
    "if 'beauty' in args.train:\n",
    "    dsets.append('beauty')\n",
    "if 'sports' in args.train:\n",
    "    dsets.append('sports')\n",
    "if 'clothing' in args.train:\n",
    "    dsets.append('clothing')\n",
    "comments.append(''.join(dsets))\n",
    "if args.backbone:\n",
    "    comments.append(args.backbone)\n",
    "comments.append(''.join(args.losses.split(',')))\n",
    "if args.comment != '':\n",
    "    comments.append(args.comment)\n",
    "comment = '_'.join(comments)\n",
    "\n",
    "from datetime import datetime\n",
    "current_time = datetime.now().strftime('%m%d')  # 例如 '0304'\n",
    "\n",
    "if args.local_rank in [0, -1]:\n",
    "    run_name = f'{current_time}_GPU{args.world_size}'\n",
    "    if len(comments) > 0:\n",
    "        run_name += f'_{comment}'\n",
    "    args.run_name = run_name\n",
    "    print(\"运行名称:\", args.run_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 6: 构建模型配置、Tokenizer 与模型\n",
    "\n",
    "功能说明：\n",
    "根据参数构建模型配置（config）、创建 Tokenizer，并加载预训练模型。\n",
    "注意：由于 checkpoint 使用的是 T5Tokenizer，而我们调用 P5Tokenizer，所以会有警告信息，但功能不受影响。\n",
    "另外，为了适配 adapter，需要将 config.d_model 赋值给 adapter_config。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: <class 'src.tokenization.P5Tokenizer'> t5-small\n",
      "Building Model at GPU 3\n",
      "JointEncoder initialized successfully.\n",
      "T5Stack initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VIP5Tuning were not initialized from the model checkpoint at t5-small and are newly initialized: ['decoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.visual_embedding.feat_embedding.0.model.0.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.whole_word_embeddings.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'output_adapter.adapter.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'output_adapter.adapter.down_sampler.weight', 'encoder.visual_embedding.feat_embedding.0.model.0.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.category_embeddings.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'encoder.visual_embedding.feat_embedding.0.model.2.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'output_adapter.adapter.up_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.visual_embedding.feat_embedding.0.model.2.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.visual_embedding.feat_embedding.1.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'output_adapter.adapter.down_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_head initialized successfully.\n",
      "OutputParallelAdapterLayer initialized successfully.\n",
      "AdapterConfig: AdapterConfig(add_layer_norm_before_adapter=False, add_layer_norm_after_adapter=False, non_linearity='gelu_new', reduction_factor=8)\n",
      "模型和 Tokenizer 构建完成\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: 构建模型配置、Tokenizer 与模型\n",
    "# 功能：根据参数构建模型配置，创建 Tokenizer，并加载预训练模型\n",
    "import re  # 确保导入 re 模块\n",
    "\n",
    "def create_config(args):\n",
    "    from transformers import T5Config\n",
    "    from adapters import AdapterConfig  # 使用适配器配置\n",
    "\n",
    "    # 从预训练 checkpoint 加载 T5 配置\n",
    "    config = T5Config.from_pretrained(args.backbone)\n",
    "    # 将所有参数写入配置中\n",
    "    for k, v in vars(args).items():\n",
    "        setattr(config, k, v)\n",
    "    config.non_linearity = \"relu\"\n",
    "\n",
    "    # 设置视觉特征参数\n",
    "    image_feature_dim_dict = {\n",
    "        'vitb32': 512,\n",
    "        'vitb16': 512,\n",
    "        'vitl14': 768,\n",
    "        'rn50': 1024,\n",
    "        'rn101': 512\n",
    "    }\n",
    "    config.feat_dim = image_feature_dim_dict[args.image_feature_type]\n",
    "    config.n_vis_tokens = args.image_feature_size_ratio\n",
    "    config.use_vis_layer_norm = args.use_vis_layer_norm\n",
    "    config.reduction_factor = args.reduction_factor\n",
    "\n",
    "    config.use_adapter = args.use_adapter\n",
    "    config.add_adapter_cross_attn = args.add_adapter_cross_attn\n",
    "    config.use_lm_head_adapter = args.use_lm_head_adapter\n",
    "    config.use_single_adapter = args.use_single_adapter\n",
    "\n",
    "    config.dropout_rate = args.dropout\n",
    "    config.dropout = args.dropout\n",
    "    config.attention_dropout = args.dropout\n",
    "    config.activation_dropout = args.dropout\n",
    "\n",
    "    config.losses = args.losses\n",
    "\n",
    "    # 如果使用适配器，则创建适配器配置，并将主配置的 d_model 传给 adapter_config\n",
    "    tasks = re.split(\"[, ]+\", args.losses)\n",
    "    if args.use_adapter:\n",
    "        adapter_config = AdapterConfig()\n",
    "        adapter_config.tasks = tasks\n",
    "        adapter_config.d_model = config.d_model  # 传递隐藏维度\n",
    "        adapter_config.use_single_adapter = args.use_single_adapter\n",
    "        adapter_config.reduction_factor = args.reduction_factor\n",
    "        adapter_config.track_z = False\n",
    "        config.adapter_config = adapter_config\n",
    "    else:\n",
    "        config.adapter_config = None\n",
    "\n",
    "    return config\n",
    "\n",
    "def create_tokenizer(args):\n",
    "    from transformers import T5Tokenizer\n",
    "    # 根据参数决定使用 P5Tokenizer 或 T5Tokenizer\n",
    "    if 'p5' in args.tokenizer:\n",
    "        from src.tokenization import P5Tokenizer\n",
    "        tokenizer_class = P5Tokenizer\n",
    "    else:\n",
    "        tokenizer_class = T5Tokenizer\n",
    "\n",
    "    tokenizer = tokenizer_class.from_pretrained(\n",
    "        args.backbone,\n",
    "        max_length=args.max_text_length,\n",
    "        do_lower_case=args.do_lower_case,\n",
    "    )\n",
    "    print(\"Tokenizer:\", tokenizer_class, args.backbone)\n",
    "    return tokenizer\n",
    "\n",
    "def create_model(model_class, config):\n",
    "    print(f'Building Model at GPU {args.gpu}')\n",
    "    model = model_class.from_pretrained(\n",
    "        args.backbone,\n",
    "        config=config\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 构建配置、Tokenizer 和模型\n",
    "config = create_config(args)\n",
    "if args.tokenizer is None:\n",
    "    args.tokenizer = args.backbone\n",
    "tokenizer = create_tokenizer(args)\n",
    "model_class = VIP5Tuning\n",
    "model = create_model(model_class, config)\n",
    "\n",
    "# 将模型移至指定 GPU\n",
    "model = model.cuda()\n",
    "\n",
    "# 如果使用 P5Tokenizer，则调整模型的词嵌入\n",
    "if 'p5' in args.tokenizer:\n",
    "    model.resize_token_embeddings(tokenizer.vocab_size)\n",
    "model.tokenizer = tokenizer\n",
    "\n",
    "print(\"模型和 Tokenizer 构建完成\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 7: 加载预训练模型权重\n",
    "\n",
    "功能说明：\n",
    "从指定 checkpoint 路径加载预训练模型权重，并打印加载结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['output_adapter.adapter.down_sampler.weight', 'output_adapter.adapter.down_sampler.bias', 'output_adapter.adapter.up_sampler.weight', 'output_adapter.adapter.up_sampler.bias'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: 加载预训练模型权重\n",
    "# 功能：从 checkpoint 加载预训练模型权重\n",
    "from pprint import pprint\n",
    "\n",
    "def load_checkpoint(ckpt_path):\n",
    "    state_dict = load_state_dict(ckpt_path, 'cpu')\n",
    "    results = model.load_state_dict(state_dict, strict=False)\n",
    "    pprint(results)\n",
    "\n",
    "# 指定 checkpoint 路径（需根据实际路径修改）\n",
    "args.load = \"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/snap/clothing/0411/clothing-vitb32-2-8-20-NoAttack/BEST_EVAL_LOSS.pth\"\n",
    "ckpt_path = args.load\n",
    "load_checkpoint(ckpt_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 8: 加载数据集及数据映射\n",
    "\n",
    "功能说明：\n",
    "加载数据分割文件（如 rating_splits_augmented.pkl）以及数据映射文件（datamaps.json），用于后续评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data长度: 27867\n",
      "Test data示例: {'reviewerID': 'AQVU2X4NK5V31', 'asin': 'B004L4B7HG', 'reviewerName': 'Michael Duffy', 'helpful': [0, 3], 'reviewText': 'I loved the look of this shoe.  certainly better constructed than the Minimus but a different fit completely.  harder to break in.  another dust collector in my closet.', 'overall': 3.0, 'summary': 'Merrell Trail Glove Running', 'unixReviewTime': 1368144000, 'reviewTime': '05 10, 2013', 'explanation': 'certainly better constructed than the Minimus but a different fit completely', 'feature': 'fit'}\n",
      "用户数量: 39387\n",
      "物品数量: 23033\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: 加载数据集及数据映射\n",
    "# 功能：加载 rating_splits_augmented.pkl 和 datamaps.json 数据文件\n",
    "\n",
    "data_splits = load_pickle(f'../data/{args.split}/rating_splits_augmented.pkl')\n",
    "test_review_data = data_splits['test']\n",
    "print(\"Test data长度:\", len(test_review_data))\n",
    "print(\"Test data示例:\", test_review_data[0])\n",
    "\n",
    "data_maps = load_json(os.path.join('../data', args.split, 'datamaps.json'))\n",
    "print(\"用户数量:\", len(data_maps['user2id']))\n",
    "print(\"物品数量:\", len(data_maps['item2id']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 9: 加载数据生成器与评价指标\n",
    "\n",
    "功能说明：\n",
    "导入数据加载函数和评价指标函数，为后续评估生成数据加载器和计算 BLEU/ROUGE 等指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据加载器与评价指标函数已导入\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: 导入数据加载器与评价指标函数\n",
    "# 功能：导入 get_loader、BLEU、ROUGE 等评价指标函数\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "from src.data import get_loader\n",
    "from evaluate.utils import rouge_score, bleu_score, unique_sentence_percent, root_mean_square_error, mean_absolute_error, feature_detect, feature_matching_ratio, feature_coverage_ratio, feature_diversity\n",
    "from evaluate.metrics4rec import evaluate_all\n",
    "\n",
    "print(\"数据加载器与评价指标函数已导入\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 10: Evaluation - Explanation 任务\n",
    "\n",
    "功能说明：\n",
    "加载 explanation 任务的数据生成器，调用模型生成输出，并计算 BLEU、ROUGE 指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sources:  ['clothing']\n",
      "compute_datum_info\n",
      "Explanation 任务 (Prompt: C-3) 数据量: 1120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 1120/1120 [01:39<00:00, 11.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1  7.2843\n",
      "BLEU-4  2.2932\n",
      "rouge_1/f_score  6.0033\n",
      "rouge_1/r_score  4.5774\n",
      "rouge_1/p_score 11.5490\n",
      "rouge_2/f_score  0.8001\n",
      "rouge_2/r_score  0.6465\n",
      "rouge_2/p_score  1.5125\n",
      "rouge_l/f_score  4.6208\n",
      "rouge_l/r_score  4.4421\n",
      "rouge_l/p_score 11.3287\n",
      "Explanation 任务 (Prompt: C-3) 评价结果已保存至: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log/clothing/0411/evaluation_logs/VIP5_clothing_vitb32_8_20_evaluation_explanation_C-3.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 10: Evaluation - Explanation 任务（带 Prompt 信息）\n",
    "# 功能说明：\n",
    "#   1. 加载指定 prompt（例如 'C-12'）下的 Explanation 任务测试数据；\n",
    "#   2. 调用模型生成预测结果，并计算 BLEU（1-gram 和 4-gram）与 ROUGE 指标；\n",
    "#   3. 在保存评估结果文件时，文件名和文件内容中都会包含当前使用的 prompt 信息，\n",
    "#      便于后续对比不同 prompt 的评估效果。\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# 如果 args.load 不为空，则从其中提取日期，否则使用当前日期\n",
    "if args.load is not None:\n",
    "    # 假定 args.load 形如 \".../snap/<split>/<日期>/<exp_name>/BEST_EVAL_LOSS.pth\"\n",
    "    eval_date = Path(args.load).parents[1].name\n",
    "else:\n",
    "    eval_date = datetime.now().strftime(\"%m%d\")\n",
    "\n",
    "# 指定 Explanation 任务的 prompt 及样本数量\n",
    "exp_prompt = 'C-3'  # 可修改为 'C-12', 'C-3' 等所需的 prompt 编号\n",
    "test_task_list = {'explanation': [exp_prompt]}\n",
    "test_sample_numbers = {'sequential': (1, 1), 'direct': (1, 1), 'explanation': 1}\n",
    "\n",
    "# 获取 Explanation 任务的测试数据加载器\n",
    "zeroshot_test_loader = get_loader(\n",
    "    args,\n",
    "    test_task_list,\n",
    "    test_sample_numbers,\n",
    "    split=args.test, \n",
    "    mode='test', \n",
    "    batch_size=args.batch_size,\n",
    "    workers=args.num_workers,\n",
    "    distributed=args.distributed,\n",
    "    data_root=\"../data\",        # 显式指定数据目录\n",
    "    feature_root=\"../features\"  # 显式指定视觉特征目录\n",
    ")\n",
    "print(f\"Explanation 任务 (Prompt: {exp_prompt}) 数据量:\", len(zeroshot_test_loader))\n",
    "\n",
    "tokens_predict = []\n",
    "tokens_test = []\n",
    "\n",
    "# 遍历测试数据加载器，调用模型生成预测结果\n",
    "for i, batch in tqdm(enumerate(zeroshot_test_loader), total=len(zeroshot_test_loader), ncols=100):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        tokens_predict.extend(results)\n",
    "        tokens_test.extend(batch['target_text'])\n",
    "\n",
    "# 计算 BLEU 与 ROUGE 指标\n",
    "BLEU1 = bleu_score(tokens_test, tokens_predict, n_gram=1, smooth=False)\n",
    "print(f'BLEU-1 {BLEU1:7.4f}')\n",
    "BLEU4 = bleu_score(tokens_test, tokens_predict, n_gram=4, smooth=False)\n",
    "print(f'BLEU-4 {BLEU4:7.4f}')\n",
    "\n",
    "ROUGE = rouge_score(tokens_test, tokens_predict)\n",
    "for k, v in ROUGE.items():\n",
    "    print(f'{k} {v:7.4f}')\n",
    "\n",
    "# 构建保存评估结果的目录和文件名，文件名中包含当前使用的 prompt 信息\n",
    "eval_dir = f\"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log/{args.split}/{eval_date}/evaluation_logs\"\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "\n",
    "explanation_filename = (\n",
    "    f\"VIP5_{args.split}_{args.image_feature_type}_\"\n",
    "    f\"{args.reduction_factor}_{args.epoch}_evaluation_explanation_{exp_prompt}.txt\"\n",
    ")\n",
    "explanation_log_path = os.path.join(eval_dir, explanation_filename)\n",
    "\n",
    "# 保存评估结果，文件内容中也包含 prompt 信息\n",
    "with open(explanation_log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Explanation Evaluation Results\\n\")\n",
    "    f.write(f\"Prompt: {exp_prompt}\\n\")\n",
    "    f.write(f\"BLEU-1: {BLEU1:7.4f}\\n\")\n",
    "    f.write(f\"BLEU-4: {BLEU4:7.4f}\\n\")\n",
    "    for k, v in ROUGE.items():\n",
    "        f.write(f\"{k}: {v:7.4f}\\n\")\n",
    "\n",
    "print(f\"Explanation 任务 (Prompt: {exp_prompt}) 评价结果已保存至: {explanation_log_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 11: Evaluation - Direct 任务\n",
    "\n",
    "功能说明：\n",
    "加载 direct 任务的测试数据，生成输出并计算评价指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sources:  ['clothing']\n",
      "compute_datum_info\n",
      "Direct 任务 (Prompt: B-5) 数据量: 2462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2462/2462 [35:37<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NDCG@1\tRec@1\tHits@1\tPrec@1\tMAP@1\tMRR@1\n",
      "0.0473\t0.0473\t0.0473\t0.0473\t0.0473\t0.0473\n",
      "\n",
      "NDCG@5\tRec@5\tHits@5\tPrec@5\tMAP@5\tMRR@5\n",
      "0.0874\t0.1265\t0.1265\t0.0253\t0.0745\t0.0745\n",
      "\n",
      "NDCG@10\tRec@10\tHits@10\tPrec@10\tMAP@10\tMRR@10\n",
      "0.1097\t0.1963\t0.1963\t0.0196\t0.0836\t0.0836\n",
      "\n",
      "Evaluation Metrics at top-5:\n",
      "\n",
      "NDCG@5\tRec@5\tHits@5\tPrec@5\tMAP@5\tMRR@5\n",
      "0.0874\t0.1265\t0.1265\t0.0253\t0.0745\t0.0745\n",
      "\n",
      "Evaluation Metrics at top-10:\n",
      "\n",
      "NDCG@10\tRec@10\tHits@10\tPrec@10\tMAP@10\tMRR@10\n",
      "0.1097\t0.1963\t0.1963\t0.0196\t0.0836\t0.0836\n",
      "Direct 任务 (Prompt: B-5) 评价结果已保存至: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log/clothing/0411/evaluation_logs/VIP5_clothing_vitb32_8_20_evaluation_direct_B-5.txt\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 11: Evaluation - Direct 任务（带 Prompt 信息）\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# 如果 args.load 不为空，则从其中提取日期，否则使用当前日期\n",
    "if args.load is not None:\n",
    "    eval_date = Path(args.load).parents[1].name\n",
    "else:\n",
    "    eval_date = datetime.now().strftime(\"%m%d\")\n",
    "\n",
    "# 指定 Direct 任务的测试任务与 Prompt\n",
    "test_task_list = {'direct': ['B-5']}  # 例：可选 'B-5' 或 'B-8'\n",
    "prompt = test_task_list['direct'][0]  # 获取当前使用的 Prompt\n",
    "\n",
    "test_sample_numbers = {\n",
    "    'sequential': (1, 1),\n",
    "    'direct': (1, 1),\n",
    "    'explanation': 1\n",
    "}\n",
    "\n",
    "# 获取 Direct 任务的测试数据加载器\n",
    "zeroshot_test_loader = get_loader(\n",
    "    args,\n",
    "    test_task_list,\n",
    "    test_sample_numbers,\n",
    "    split=args.test,\n",
    "    mode='test',\n",
    "    batch_size=args.batch_size,\n",
    "    workers=args.num_workers,\n",
    "    distributed=args.distributed,\n",
    "    data_root=\"../data\",        # 显式指定数据目录\n",
    "    feature_root=\"../features\"  # 显式指定视觉特征目录\n",
    ")\n",
    "\n",
    "print(f\"Direct 任务 (Prompt: {prompt}) 数据量:\", len(zeroshot_test_loader))\n",
    "\n",
    "all_info = []\n",
    "for i, batch in tqdm(enumerate(zeroshot_test_loader), total=len(zeroshot_test_loader)):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        beam_outputs = model.generate(\n",
    "            input_ids=batch['input_ids'].to('cuda'),\n",
    "            whole_word_ids=batch['whole_word_ids'].to('cuda'),\n",
    "            category_ids=batch['category_ids'].to('cuda'),\n",
    "            vis_feats=batch['vis_feats'].to('cuda'),\n",
    "            task=batch[\"task\"][0],\n",
    "            max_length=50,\n",
    "            num_beams=20,\n",
    "            no_repeat_ngram_size=0,\n",
    "            num_return_sequences=20,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        generated_sents = model.tokenizer.batch_decode(beam_outputs, skip_special_tokens=True)\n",
    "\n",
    "        # 遍历当前批次中每个样本（假设每个样本生成20个候选）\n",
    "        for j, (_, tgt_text, _) in enumerate(zip(results, batch['target_text'], batch['source_text'])):\n",
    "            new_info = {}\n",
    "            new_info['target_item'] = tgt_text\n",
    "            new_info['gen_item_list'] = generated_sents[j * 20: (j + 1) * 20]\n",
    "            all_info.append(new_info)\n",
    "\n",
    "# 构造 ground truth 和预测得分字典\n",
    "gt = {}\n",
    "ui_scores = {}\n",
    "for i, info in enumerate(all_info):\n",
    "    gt[i] = [int(info['target_item'])]\n",
    "    pred_dict = {}\n",
    "    for j, pred in enumerate(info['gen_item_list']):\n",
    "        try:\n",
    "            pred_dict[int(pred)] = -(j + 1)\n",
    "        except Exception:\n",
    "            pass\n",
    "    ui_scores[i] = pred_dict\n",
    "\n",
    "# 计算推荐指标\n",
    "msg_top1, res_top1 = evaluate_all(ui_scores, gt, 1)\n",
    "msg_top5, res_top5 = evaluate_all(ui_scores, gt, 5)\n",
    "msg_top10, res_top10 = evaluate_all(ui_scores, gt, 10)\n",
    "\n",
    "print(\"\\nEvaluation Metrics at top-5:\")\n",
    "print(msg_top5)\n",
    "print(\"\\nEvaluation Metrics at top-10:\")\n",
    "print(msg_top10)\n",
    "\n",
    "# 保存 Direct 任务评价结果到文件，文件名包含 Prompt\n",
    "eval_dir = f\"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log/{args.split}/{eval_date}/evaluation_logs\"\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "\n",
    "direct_filename = (\n",
    "    f\"VIP5_{args.split}_\"\n",
    "    f\"{args.image_feature_type}_\"\n",
    "    f\"{args.reduction_factor}_\"\n",
    "    f\"{args.epoch}_evaluation_direct_{prompt}.txt\"\n",
    ")\n",
    "direct_log_path = os.path.join(eval_dir, direct_filename)\n",
    "\n",
    "with open(direct_log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Direct Evaluation Results\\n\")\n",
    "    f.write(f\"Prompt: {prompt}\\n\")\n",
    "    f.write(\"Evaluation Metrics at top-5:\\n\")\n",
    "    f.write(msg_top5 + \"\\n\")\n",
    "    f.write(\"Evaluation Metrics at top-10:\\n\")\n",
    "    f.write(msg_top10 + \"\\n\")\n",
    "\n",
    "print(f\"Direct 任务 (Prompt: {prompt}) 评价结果已保存至: {direct_log_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 12: Evaluation - Sequential 任务\n",
    "\n",
    "功能说明：\n",
    "加载 sequential 任务的测试数据，生成输出并计算评价指标，同时对 beam search 结果进行解码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sources:  ['beauty']\n",
      "compute_datum_info\n",
      "Sequential 任务 (Prompt: A-3) 数据量: 1398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1398/1398 [13:39<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NDCG@1\tRec@1\tHits@1\tPrec@1\tMAP@1\tMRR@1\n",
      "0.0328\t0.0328\t0.0328\t0.0328\t0.0328\t0.0328\n",
      "\n",
      "NDCG@5\tRec@5\tHits@5\tPrec@5\tMAP@5\tMRR@5\n",
      "0.0465\t0.0597\t0.0597\t0.0119\t0.0422\t0.0422\n",
      "\n",
      "NDCG@10\tRec@10\tHits@10\tPrec@10\tMAP@10\tMRR@10\n",
      "0.0508\t0.0730\t0.0730\t0.0073\t0.0440\t0.0440\n",
      "\n",
      "Evaluation Metrics at top-1:\n",
      "\n",
      "NDCG@1\tRec@1\tHits@1\tPrec@1\tMAP@1\tMRR@1\n",
      "0.0328\t0.0328\t0.0328\t0.0328\t0.0328\t0.0328\n",
      "\n",
      "Evaluation Metrics at top-5:\n",
      "\n",
      "NDCG@5\tRec@5\tHits@5\tPrec@5\tMAP@5\tMRR@5\n",
      "0.0465\t0.0597\t0.0597\t0.0119\t0.0422\t0.0422\n",
      "\n",
      "Evaluation Metrics at top-10:\n",
      "\n",
      "NDCG@10\tRec@10\tHits@10\tPrec@10\tMAP@10\tMRR@10\n",
      "0.0508\t0.0730\t0.0730\t0.0073\t0.0440\t0.0440\n",
      "Sequential 任务 (Prompt: A-3) 评价结果已保存至: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log/beauty/0410/evaluation_logs/VIP5_beauty_vitb32_8_20_evaluation_sequential_A-3.txt\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# 如果 args.load 不为空，则从其中提取日期，否则使用当前日期\n",
    "if args.load is not None:\n",
    "    eval_date = Path(args.load).parents[1].name\n",
    "else:\n",
    "    eval_date = datetime.now().strftime(\"%m%d\")\n",
    "\n",
    "# 指定 Sequential 任务的测试任务与 Prompt\n",
    "test_task_list = {'sequential': ['A-3']}  # 例：可选 'A-9', 'A-3' 等\n",
    "prompt = test_task_list['sequential'][0]  # 获取当前使用的 Prompt\n",
    "\n",
    "test_sample_numbers = {\n",
    "    'sequential': (1, 1),\n",
    "    'direct': (1, 1),\n",
    "    'explanation': 1\n",
    "}\n",
    "\n",
    "# 获取 Sequential 任务的测试数据加载器\n",
    "zeroshot_test_loader = get_loader(\n",
    "    args,\n",
    "    test_task_list,\n",
    "    test_sample_numbers,\n",
    "    split=args.test,\n",
    "    mode='test',\n",
    "    batch_size=args.batch_size,\n",
    "    workers=args.num_workers,\n",
    "    distributed=args.distributed,\n",
    "    data_root=\"../data\",         # 显式指定数据所在目录\n",
    "    feature_root=\"../features\"   # 显式指定视觉特征目录\n",
    ")\n",
    "\n",
    "print(f\"Sequential 任务 (Prompt: {prompt}) 数据量:\", len(zeroshot_test_loader))\n",
    "\n",
    "all_info = []\n",
    "for i, batch in tqdm(enumerate(zeroshot_test_loader), total=len(zeroshot_test_loader)):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        beam_outputs = model.generate(\n",
    "            input_ids=batch['input_ids'].to('cuda'),\n",
    "            whole_word_ids=batch['whole_word_ids'].to('cuda'),\n",
    "            category_ids=batch['category_ids'].to('cuda'),\n",
    "            vis_feats=batch['vis_feats'].to('cuda'),\n",
    "            task=batch[\"task\"][0],\n",
    "            max_length=50,\n",
    "            num_beams=20,\n",
    "            no_repeat_ngram_size=0,\n",
    "            num_return_sequences=20,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        generated_sents = model.tokenizer.batch_decode(beam_outputs, skip_special_tokens=True)\n",
    "\n",
    "        # 遍历当前批次中每个样本（假设每个样本生成20个候选）\n",
    "        for j in range(len(batch['target_text'])):\n",
    "            new_info = {}\n",
    "            new_info['target_item'] = batch['target_text'][j]\n",
    "            new_info['gen_item_list'] = generated_sents[j * 20: (j + 1) * 20]\n",
    "            all_info.append(new_info)\n",
    "\n",
    "# 构造 ground truth 和预测得分字典\n",
    "gt = {}\n",
    "ui_scores = {}\n",
    "for i, info in enumerate(all_info):\n",
    "    gt[i] = [int(info['target_item'])]\n",
    "    pred_dict = {}\n",
    "    for j, pred in enumerate(info['gen_item_list']):\n",
    "        try:\n",
    "            pred_dict[int(pred)] = -(j + 1)\n",
    "        except Exception:\n",
    "            pass\n",
    "    ui_scores[i] = pred_dict\n",
    "\n",
    "# 计算推荐指标\n",
    "msg_top1, res_top1 = evaluate_all(ui_scores, gt, 1)\n",
    "msg_top5, res_top5 = evaluate_all(ui_scores, gt, 5)\n",
    "msg_top10, res_top10 = evaluate_all(ui_scores, gt, 10)\n",
    "\n",
    "print(\"\\nEvaluation Metrics at top-1:\")\n",
    "print(msg_top1)\n",
    "print(\"\\nEvaluation Metrics at top-5:\")\n",
    "print(msg_top5)\n",
    "print(\"\\nEvaluation Metrics at top-10:\")\n",
    "print(msg_top10)\n",
    "\n",
    "# 保存 Sequential 任务评价结果到文件，文件名包含 Prompt\n",
    "eval_dir = f\"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log/{args.split}/{eval_date}/evaluation_logs\"\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "\n",
    "sequential_filename = (\n",
    "    f\"VIP5_{args.split}_\"\n",
    "    f\"{args.image_feature_type}_\"\n",
    "    f\"{args.reduction_factor}_\"\n",
    "    f\"{args.epoch}_evaluation_sequential_{prompt}.txt\"\n",
    ")\n",
    "sequential_log_path = os.path.join(eval_dir, sequential_filename)\n",
    "\n",
    "with open(sequential_log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Sequential Evaluation Results\\n\")\n",
    "    f.write(f\"Prompt: {prompt}\\n\")\n",
    "    f.write(\"Evaluation Metrics at top-1:\\n\")\n",
    "    f.write(msg_top1 + \"\\n\")\n",
    "    f.write(\"Evaluation Metrics at top-5:\\n\")\n",
    "    f.write(msg_top5 + \"\\n\")\n",
    "    f.write(\"Evaluation Metrics at top-10:\\n\")\n",
    "    f.write(msg_top10 + \"\\n\")\n",
    "\n",
    "print(f\"Sequential 任务 (Prompt: {prompt}) 评价结果已保存至: {sequential_log_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vip5_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
