{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 1: 环境设置与路径配置\n",
    "\n",
    "功能说明：\n",
    "将项目根目录添加到 Python 模块搜索路径中，确保后续能够正确加载项目内部模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Current working directory: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA\n",
      "→ First entries in sys.path: ['/scratch/guowei/Code/VIP5/transformers', '/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/notebooks', '/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/src']\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: 环境设置与路径配置\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 项目根目录\n",
    "project_path   = \"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA\"\n",
    "# src/ 子模块目录\n",
    "src_path       = os.path.join(project_path, \"src\")\n",
    "# notebooks/ 目录（evaluate 模块就在这里）\n",
    "notebooks_path = os.path.join(project_path, \"notebooks\")\n",
    "\n",
    "# 1) 把 project_root、src/ 和 notebooks/ 都加入到 sys.path，\n",
    "#    这样后续 import src.* 和 import evaluate.* 都能正常工作\n",
    "for p in (project_path, src_path, notebooks_path):\n",
    "    if p not in sys.path:\n",
    "        sys.path.insert(0, p)\n",
    "\n",
    "# 2) 切到项目根目录，这样后续 open(\"config_eval.yaml\")、get_loader(data_root=\"data\") 等都可以直接用相对路径\n",
    "os.chdir(project_path)\n",
    "\n",
    "print(\"→ Current working directory:\", os.getcwd())\n",
    "print(\"→ First entries in sys.path:\", sys.path[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 2: 导入依赖库与模块\n",
    "\n",
    "功能说明：\n",
    "导入所有需要的第三方库和项目内部模块。注意部分模块（如 P5Tokenizer）在后续 cell 中会用到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有依赖库已导入\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: 导入依赖库与模块\n",
    "import collections\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from packaging import version\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gzip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# 导入项目内部模块\n",
    "from src.param import parse_args\n",
    "from src.utils import LossMeter, load_state_dict, set_global_logging_level\n",
    "from src.dist_utils import reduce_dict\n",
    "from transformers import T5Tokenizer\n",
    "from src.tokenization import P5Tokenizer\n",
    "from src.model import VIP5Tuning\n",
    "from src.trainer_base import TrainerBase\n",
    "\n",
    "# 判断是否使用 native AMP 或 Apex\n",
    "_use_native_amp = False\n",
    "_use_apex = False\n",
    "if version.parse(torch.__version__) < version.parse(\"1.6\"):\n",
    "    from transormers.file_utils import is_apex_available\n",
    "    if is_apex_available():\n",
    "        from apex import amp\n",
    "    _use_apex = True\n",
    "else:\n",
    "    _use_native_amp = True\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "print(\"所有依赖库已导入\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 3: 定义辅助函数\n",
    "\n",
    "功能说明：\n",
    "定义常用的辅助函数，如 pickle、json 的加载函数，以及文件读取函数等，方便后续调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "辅助函数定义完成\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: 定义辅助函数\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_pickle(data, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def ReadLineFromFile(path):\n",
    "    lines = []\n",
    "    with open(path, 'r') as fd:\n",
    "        for line in fd:\n",
    "            lines.append(line.rstrip('\\n'))\n",
    "    return lines\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "print(\"辅助函数定义完成\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 4: 定义 DotDict 类及参数设置\n",
    "\n",
    "功能说明：\n",
    "定义一个 DotDict 类，使得可以通过属性方式访问字典中的值；并设置所有实验参数、随机种子等，保证实验结果可复现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Parsed from checkpoint path:\n",
      "  attack_mode=NoAttack, mr=0.0\n",
      "  split=toys, feat=vitb32, size_ratio=2, reduction=8, epoch=20\n",
      "✔️ Wrote temporary config_eval.yaml: {'experiment': {'suffix': 'NoAttack', 'mr': 0.0}}\n",
      "✔️ 完整 args 配置：\n",
      "{'load': '/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/snap/toys/0509/NoAttack_0.0_toys-vitb32-2-8-20/BEST_EVAL_LOSS.pth', 'attack_mode': 'NoAttack', 'mr': 0.0, 'split': 'toys', 'train': 'toys', 'valid': 'toys', 'test': 'toys', 'image_feature_type': 'vitb32', 'image_feature_size_ratio': 2, 'reduction_factor': 8, 'epoch': 20, 'distributed': False, 'multiGPU': True, 'fp16': True, 'batch_size': 16, 'optim': 'adamw', 'warmup_ratio': 0.1, 'lr': 0.001, 'num_workers': 4, 'clip_grad_norm': 5.0, 'losses': 'sequential,direct,explanation', 'backbone': 't5-small', 'comment': '', 'local_rank': 0, 'data_target': {'beauty': [2], 'clothing': [8], 'sports': [53], 'toys': [62]}, 'use_adapter': True, 'use_single_adapter': True, 'use_vis_layer_norm': True, 'add_adapter_cross_attn': True, 'use_lm_head_adapter': True, 'tokenizer': 'p5', 'max_text_length': 1024, 'gen_max_length': 64, 'do_lower_case': False, 'dropout': 0.1, 'weight_decay': 0.01, 'adam_eps': 1e-06, 'gradient_accumulation_steps': 1, 'seed': 2022, 'whole_word_embed': True, 'category_embed': True, 'world_size': 4, 'LOSSES_NAME': ['sequential_loss', 'direct_loss', 'explanation_loss', 'total_loss']}\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: 设置参数与随机种子\n",
    "import re\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "class DotDict(dict):\n",
    "    def __init__(self, **kwds):\n",
    "        super().__init__(**kwds)\n",
    "        self.__dict__ = self\n",
    "    def __repr__(self):\n",
    "        return dict.__repr__(self)\n",
    "\n",
    "# 构造参数对象\n",
    "args = DotDict()\n",
    "\n",
    "# ──── 1) checkpoint 路径 ────────────────────────────────────────────────\n",
    "args.load = (\n",
    "    \"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/snap/toys/0509/NoAttack_0.0_toys-vitb32-2-8-20/BEST_EVAL_LOSS.pth\"\n",
    ")\n",
    "\n",
    "# ──── 2) 自动解析 attack_mode / mr / split / feat / size_ratio / reduction / epoch ─────────\n",
    "ckpt_folder = Path(args.load).parent.name\n",
    "# 现在 ckpt_folder = \"NoAttack_0.0_toys-vitb32-2-8-20\"\n",
    "mode, mr_str, rest = ckpt_folder.split(\"_\", 2)\n",
    "args.attack_mode = mode\n",
    "args.mr = float(mr_str)\n",
    "\n",
    "# rest 里再拆： dataset=toys, img_feat=vitb32, size_ratio=2, reduction=8, epoch=20\n",
    "dataset, img_feat, size_ratio, reduction, epoch = rest.split(\"-\")\n",
    "args.split = dataset\n",
    "args.train = args.valid = args.test = dataset\n",
    "\n",
    "args.image_feature_type       = img_feat\n",
    "args.image_feature_size_ratio = int(size_ratio)\n",
    "args.reduction_factor         = int(reduction)\n",
    "args.epoch                    = int(epoch)\n",
    "\n",
    "print(\"✔️ Parsed from checkpoint path:\")\n",
    "print(f\"  attack_mode={args.attack_mode}, mr={args.mr}\")\n",
    "print(f\"  split={args.split}, feat={args.image_feature_type},\",\n",
    "      f\"size_ratio={args.image_feature_size_ratio},\",\n",
    "      f\"reduction={args.reduction_factor}, epoch={args.epoch}\")\n",
    "\n",
    "# ──── 3) 写临时 config_eval.yaml，供 VIP5_Dataset 读取 ──────────────────────────────\n",
    "cfg = {\"experiment\": {\"suffix\": args.attack_mode, \"mr\": args.mr}}\n",
    "with open(\"config_eval.yaml\", \"w\") as f:\n",
    "    yaml.safe_dump(cfg, f)\n",
    "print(\"✔️ Wrote temporary config_eval.yaml:\", cfg)\n",
    "\n",
    "# ──── 4) 其余静态参数 ────────────────────────────────────────────────\n",
    "args.distributed = False\n",
    "args.multiGPU    = True\n",
    "args.fp16        = True\n",
    "\n",
    "args.batch_size      = 16\n",
    "args.optim           = 'adamw'\n",
    "args.warmup_ratio    = 0.1\n",
    "args.lr              = 1e-3\n",
    "args.num_workers     = 4\n",
    "args.clip_grad_norm  = 5.0\n",
    "args.losses          = 'sequential,direct,explanation'\n",
    "args.backbone        = 't5-small'\n",
    "args.comment         = ''    # ← 新增\n",
    "args.local_rank      = 0         # ← 在这里新增\n",
    "\n",
    "# 数据集目标物品（你现有的逻辑）\n",
    "args.data_target = {}\n",
    "for ds in [\"beauty\", \"clothing\", \"sports\", \"toys\"]:\n",
    "    path = (\n",
    "        f\"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/\"\n",
    "        f\"analysis/results/{ds}/low_pop_items_{ds}_lowcount_1.txt\"\n",
    "    )\n",
    "    ids = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as fin:\n",
    "        for line in fin:\n",
    "            m = re.search(r\"\\(ID:\\s*(\\d+)\\)\", line)\n",
    "            if m:\n",
    "                ids.append(int(m.group(1)))\n",
    "    if not ids:\n",
    "        raise RuntimeError(f\"{ds} 没有解析到任何 ID，请检查 {path}\")\n",
    "    args.data_target[ds] = ids\n",
    "\n",
    "# 模型＋视觉特征\n",
    "args.use_adapter            = True\n",
    "args.use_single_adapter     = True\n",
    "args.use_vis_layer_norm     = True\n",
    "args.add_adapter_cross_attn = True\n",
    "args.use_lm_head_adapter    = True\n",
    "\n",
    "# 文本长度／dropout／tokenizer\n",
    "args.tokenizer               = 'p5'\n",
    "args.max_text_length         = 1024\n",
    "args.gen_max_length          = 64\n",
    "args.do_lower_case           = False\n",
    "args.dropout                 = 0.1\n",
    "args.weight_decay            = 0.01\n",
    "args.adam_eps                = 1e-6\n",
    "args.gradient_accumulation_steps = 1\n",
    "\n",
    "# 随机种子\n",
    "args.seed = 2022\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "# Whole Word & Category Embedding\n",
    "args.whole_word_embed = True\n",
    "args.category_embed   = True\n",
    "\n",
    "# cudnn & GPU\n",
    "cudnn.benchmark     = True\n",
    "args.world_size     = torch.cuda.device_count()\n",
    "\n",
    "# 损失名称列表\n",
    "LOSSES_NAME = [f'{n}_loss' for n in args.losses.split(',')] + ['total_loss']\n",
    "args.LOSSES_NAME = LOSSES_NAME\n",
    "\n",
    "print(\"✔️ 完整 args 配置：\")\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 5: GPU设置与生成运行名称\n",
    "\n",
    "功能说明：\n",
    "指定使用的 GPU（手动设置），并构造一个运行名称（run_name），便于后续日志及保存结果区分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Launching at GPU 0\n",
      "运行名称: 0509_GPU4_toys_t5-small_sequentialdirectexplanation\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: GPU设置与生成运行名称\n",
    "# 功能：指定 GPU（手动设置），并构造一个运行名称\n",
    "\n",
    "# 手动指定 GPU ID\n",
    "gpu = 0\n",
    "args.gpu = gpu\n",
    "args.rank = gpu\n",
    "print(f'Process Launching at GPU {gpu}')\n",
    "\n",
    "# 设置当前 GPU 设备\n",
    "torch.cuda.set_device(f'cuda:{gpu}')\n",
    "\n",
    "# 构造运行名称\n",
    "comments = []\n",
    "dsets = []\n",
    "if 'toys' in args.train:\n",
    "    dsets.append('toys')\n",
    "if 'beauty' in args.train:\n",
    "    dsets.append('beauty')\n",
    "if 'sports' in args.train:\n",
    "    dsets.append('sports')\n",
    "if 'clothing' in args.train:\n",
    "    dsets.append('clothing')\n",
    "comments.append(''.join(dsets))\n",
    "if args.backbone:\n",
    "    comments.append(args.backbone)\n",
    "comments.append(''.join(args.losses.split(',')))\n",
    "if args.comment != '':\n",
    "    comments.append(args.comment)\n",
    "comment = '_'.join(comments)\n",
    "\n",
    "from datetime import datetime\n",
    "current_time = datetime.now().strftime('%m%d')  # 例如 '0304'\n",
    "\n",
    "if args.local_rank in [0, -1]:\n",
    "    run_name = f'{current_time}_GPU{args.world_size}'\n",
    "    if len(comments) > 0:\n",
    "        run_name += f'_{comment}'\n",
    "    args.run_name = run_name\n",
    "    print(\"运行名称:\", args.run_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 6: 构建模型配置、Tokenizer 与模型\n",
    "\n",
    "功能说明：\n",
    "根据参数构建模型配置（config）、创建 Tokenizer，并加载预训练模型。\n",
    "注意：由于 checkpoint 使用的是 T5Tokenizer，而我们调用 P5Tokenizer，所以会有警告信息，但功能不受影响。\n",
    "另外，为了适配 adapter，需要将 config.d_model 赋值给 adapter_config。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: <class 'src.tokenization.P5Tokenizer'> t5-small\n",
      "→ 正在从预训练模型 't5-small' 初始化 VIP5Tuning 结构…\n",
      "JointEncoder initialized successfully.\n",
      "T5Stack initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VIP5Tuning were not initialized from the model checkpoint at t5-small and are newly initialized: ['decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.visual_embedding.feat_embedding.0.model.2.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'output_adapter.adapter.up_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.visual_embedding.feat_embedding.1.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'output_adapter.adapter.down_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.visual_embedding.feat_embedding.0.model.0.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'output_adapter.adapter.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.up_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.2.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.5.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'output_adapter.adapter.up_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.visual_embedding.feat_embedding.0.model.0.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.whole_word_embeddings.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.0.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.4.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.explanation.up_sampler.bias', 'encoder.block.4.layer.0.attn_adapter.adapters.explanation.up_sampler.weight', 'encoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.1.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'decoder.block.0.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.down_sampler.weight', 'decoder.block.3.layer.0.attn_adapter.adapters.sequential.up_sampler.weight', 'encoder.block.2.layer.1.ff_adapter.adapters.direct.up_sampler.weight', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.4.layer.0.attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.4.layer.2.ff_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.3.layer.2.ff_adapter.adapters.direct.down_sampler.weight', 'decoder.block.5.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.1.layer.1.ff_adapter.adapters.direct.down_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'decoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'decoder.block.5.layer.2.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.2.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.4.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.direct.up_sampler.weight', 'encoder.block.1.layer.0.attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.3.layer.0.attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.4.layer.1.ff_adapter.adapters.sequential.down_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.0.layer.1.ff_adapter.adapters.explanation.down_sampler.bias', 'encoder.block.5.layer.0.attn_adapter.adapters.sequential.down_sampler.weight', 'encoder.block.5.layer.1.ff_adapter.adapters.explanation.down_sampler.weight', 'encoder.category_embeddings.weight', 'decoder.block.4.layer.1.enc_attn_adapter.adapters.explanation.down_sampler.weight', 'encoder.block.3.layer.1.ff_adapter.adapters.direct.up_sampler.bias', 'decoder.block.4.layer.2.ff_adapter.adapters.explanation.up_sampler.weight', 'decoder.block.0.layer.0.attn_adapter.adapters.explanation.down_sampler.bias', 'decoder.block.3.layer.1.enc_attn_adapter.adapters.sequential.down_sampler.bias', 'encoder.block.0.layer.0.attn_adapter.adapters.direct.up_sampler.bias', 'encoder.block.2.layer.0.attn_adapter.adapters.direct.down_sampler.bias', 'encoder.block.1.layer.0.attn_adapter.adapters.direct.down_sampler.weight', 'encoder.block.1.layer.1.ff_adapter.adapters.explanation.up_sampler.bias', 'decoder.block.1.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'decoder.block.2.layer.2.ff_adapter.adapters.sequential.up_sampler.weight', 'encoder.visual_embedding.feat_embedding.0.model.2.bias', 'decoder.block.2.layer.1.enc_attn_adapter.adapters.sequential.up_sampler.bias', 'encoder.block.3.layer.1.ff_adapter.adapters.explanation.down_sampler.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_head initialized successfully.\n",
      "OutputParallelAdapterLayer initialized successfully.\n",
      "AdapterConfig: AdapterConfig(add_layer_norm_before_adapter=False, add_layer_norm_after_adapter=False, non_linearity='gelu_new', reduction_factor=8)\n",
      "✔️ 模型结构与 Tokenizer 初始化完成，下一步 Cell 7 再加载你的 .pth 权重\n"
     ]
    }
   ],
   "source": [
    "# ──── Cell 6: 构建模型配置、Tokenizer 与模型 ────────────────────────\n",
    "import re\n",
    "from transformers import T5Config, T5Tokenizer\n",
    "from adapters import AdapterConfig\n",
    "from src.tokenization import P5Tokenizer\n",
    "from src.model import VIP5Tuning\n",
    "\n",
    "# ──── Monkey-patch：给 VIP5Tuning 增加一个 model 属性，指向自身 ─────────────\n",
    "# 这样 VIP5.__init__ 里 self.model.shared 就能正常访问 self.shared\n",
    "VIP5Tuning.model = property(lambda self: self)\n",
    "\n",
    "def create_config(args):\n",
    "    # 1) 从 backbone pretrained 拿到基础 config\n",
    "    config = T5Config.from_pretrained(args.backbone)\n",
    "    # 2) 把所有我们在 args 里写的字段都塞进 config\n",
    "    for k, v in vars(args).items():\n",
    "        setattr(config, k, v)\n",
    "    config.non_linearity = \"relu\"\n",
    "\n",
    "    # 3) 视觉特征维度映射\n",
    "    dim_map = {\n",
    "        'vitb32': 512, 'vitb16': 512, 'vitl14': 768,\n",
    "        'rn50': 1024, 'rn101': 512,\n",
    "    }\n",
    "    config.feat_dim           = dim_map[args.image_feature_type]\n",
    "    config.n_vis_tokens       = args.image_feature_size_ratio\n",
    "    config.use_vis_layer_norm = args.use_vis_layer_norm\n",
    "    config.reduction_factor   = args.reduction_factor\n",
    "\n",
    "    # 4) Adapter 相关开关\n",
    "    config.use_adapter            = args.use_adapter\n",
    "    config.add_adapter_cross_attn = args.add_adapter_cross_attn\n",
    "    config.use_lm_head_adapter    = args.use_lm_head_adapter\n",
    "    config.use_single_adapter     = args.use_single_adapter\n",
    "    config.dropout_rate           = args.dropout\n",
    "    config.attention_dropout      = args.dropout\n",
    "    config.activation_dropout     = args.dropout\n",
    "    config.losses                 = args.losses\n",
    "\n",
    "    if args.use_adapter:\n",
    "        tasks = re.split(\"[, ]+\", args.losses)\n",
    "        adapter_cfg = AdapterConfig()\n",
    "        adapter_cfg.tasks             = tasks\n",
    "        adapter_cfg.d_model           = config.d_model\n",
    "        adapter_cfg.use_single_adapter= args.use_single_adapter\n",
    "        adapter_cfg.reduction_factor  = args.reduction_factor\n",
    "        adapter_cfg.track_z           = False\n",
    "        config.adapter_config        = adapter_cfg\n",
    "    else:\n",
    "        config.adapter_config = None\n",
    "\n",
    "    return config\n",
    "\n",
    "def create_tokenizer(args):\n",
    "    # 根据 args.tokenizer 决定用 P5Tokenizer 还是 T5Tokenizer\n",
    "    if 'p5' in args.tokenizer:\n",
    "        tok_cls = P5Tokenizer\n",
    "    else:\n",
    "        tok_cls = T5Tokenizer\n",
    "\n",
    "    tokenizer = tok_cls.from_pretrained(\n",
    "        args.backbone,\n",
    "        max_length=args.max_text_length,\n",
    "        do_lower_case=args.do_lower_case\n",
    "    )\n",
    "    print(\"Tokenizer:\", tok_cls, args.backbone)\n",
    "    return tokenizer\n",
    "\n",
    "def create_model(args, config):\n",
    "    # 用 from_pretrained 搭好所有底层组件（包括 self.shared, self.encoder, adapter 层…）\n",
    "    print(f\"→ 正在从预训练模型 '{args.backbone}' 初始化 VIP5Tuning 结构…\")\n",
    "    model = VIP5Tuning.from_pretrained(\n",
    "        args.backbone,\n",
    "        config=config\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# —— 真·执行三步 —— \n",
    "config    = create_config(args)\n",
    "tokenizer = create_tokenizer(args)\n",
    "model     = create_model(args, config).cuda()\n",
    "\n",
    "# 如果用的是 P5Tokenizer，就扩增词表\n",
    "if 'p5' in args.tokenizer:\n",
    "    model.resize_token_embeddings(tokenizer.vocab_size)\n",
    "\n",
    "# 挂上 tokenizer，方便后续 decode\n",
    "model.tokenizer = tokenizer\n",
    "\n",
    "print(\"✔️ 模型结构与 Tokenizer 初始化完成，下一步 Cell 7 再加载你的 .pth 权重\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 7: 加载预训练模型权重\n",
    "\n",
    "功能说明：\n",
    "从指定 checkpoint 路径加载预训练模型权重，并打印加载结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading checkpoint from: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/snap/toys/0509/NoAttack_0.0_toys-vitb32-2-8-20/BEST_EVAL_LOSS.pth\n",
      "ℹ️  load_state_dict 结果：\n",
      "_IncompatibleKeys(missing_keys=['output_adapter.adapter.down_sampler.weight', 'output_adapter.adapter.down_sampler.bias', 'output_adapter.adapter.up_sampler.weight', 'output_adapter.adapter.up_sampler.bias'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: 加载预训练模型权重\n",
    "from pprint import pprint\n",
    "from src.utils import load_state_dict\n",
    "\n",
    "def load_checkpoint(ckpt_path):\n",
    "    if not ckpt_path.endswith('.pth'):\n",
    "        ckpt_path += '.pth'\n",
    "    print(f\"📥 Loading checkpoint from: {ckpt_path}\")\n",
    "    state_dict = load_state_dict(ckpt_path, 'cpu')\n",
    "    res = model.load_state_dict(state_dict, strict=False)\n",
    "    print(\"ℹ️  load_state_dict 结果：\")\n",
    "    pprint(res)\n",
    "\n",
    "# 直接用 Cell 4 里设置好的 args.load\n",
    "load_checkpoint(args.load)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 8: 加载数据集及数据映射\n",
    "\n",
    "功能说明：\n",
    "加载数据分割文件（如 rating_splits_augmented.pkl）以及数据映射文件（datamaps.json），用于后续评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data长度: 16759\n",
      "Test data示例: {'reviewerID': 'A5K3CK2PWYQ7O', 'asin': 'B00F4CFEYG', 'reviewerName': 'Ellie \"mittbooks\"', 'helpful': [0, 0], 'reviewText': \"I've found the Melissa & Doug brand to be overall good, although there are occasional negatives.  This is definitely one of the toys we'll mark a &#34;winner.&#34;  The vacuum comes in two pieces that require minimal assembly (the long handle and the base need to be put together - no tools required).  The height is perfect for our two year old who is 3 feet tall.  The top part moves at about a 45 degree angle to facilitate little people pushing the vacuum.  I'm not sure how long the six wooden pieces of &#34;trash&#34; will last.  Although not tiny, they would be easy to lose.  The vacuum does a good job of picking them up easily and there is a small area in the back of the base to take them out again.  There is also a rotating knob on the front of the handle that makes a good clicking noise when it moves.  Our son is truly enjoying this this toy and the overall quality is excellent.  Definitely a good addition to the toy room.\", 'overall': 4.0, 'summary': '... found the Melissa & Doug brand to be overall good, although there are occasional negatives', 'unixReviewTime': 1403913600, 'reviewTime': '06 28, 2014', 'explanation': \"I've found the Melissa & Doug brand to be overall good\", 'feature': 'overall'}\n",
      "用户数量: 19412\n",
      "物品数量: 11924\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: 加载数据集及数据映射\n",
    "# 功能：加载 rating_splits_augmented.pkl 和 datamaps.json 数据文件\n",
    "\n",
    "\n",
    "data_splits = load_pickle(f'data/{args.split}/rating_splits_augmented.pkl')\n",
    "test_review_data = data_splits['test']\n",
    "print(\"Test data长度:\", len(test_review_data))\n",
    "print(\"Test data示例:\", test_review_data[0])\n",
    "\n",
    "data_maps = load_json(os.path.join('data', args.split, 'datamaps.json'))\n",
    "print(\"用户数量:\", len(data_maps['user2id']))\n",
    "print(\"物品数量:\", len(data_maps['item2id']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 9: 加载数据生成器与评价指标\n",
    "\n",
    "功能说明：\n",
    "导入数据加载函数和评价指标函数，为后续评估生成数据加载器和计算 BLEU/ROUGE 等指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据加载器与评价指标函数已导入\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: 导入数据加载器与评价指标函数\n",
    "# 功能：导入 get_loader、BLEU、ROUGE 等评价指标函数\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "from src.data import get_loader\n",
    "from evaluate.utils import rouge_score, bleu_score, unique_sentence_percent, root_mean_square_error, mean_absolute_error, feature_detect, feature_matching_ratio, feature_coverage_ratio, feature_diversity\n",
    "from evaluate.metrics4rec import evaluate_all\n",
    "\n",
    "print(\"数据加载器与评价指标函数已导入\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 10: Evaluation - Explanation 任务\n",
    "\n",
    "功能说明：\n",
    "加载 explanation 任务的数据生成器，调用模型生成输出，并计算 BLEU、ROUGE 指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/snap/toys/0509/NoAttack_0.0_toys-vitb32-2-8-20/BEST_EVAL_LOSS.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] exp_splits_path = data/toys/exp_splits.pkl\n",
      "[DEBUG] seq_path        = data/toys/sequential_data.txt\n",
      "[DEBUG] idx_path        = data/toys/user_id2idx.pkl\n",
      "[DEBUG] name_path       = data/toys/user_id2name.pkl\n",
      "[DEBUG] Explanation-only，动态构建了 6831 个用户映射\n",
      "compute_datum_info\n",
      "Explanation 任务 (Prompt: C-3) 数据量: 646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 646/646 [01:28<00:00,  7.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1  4.3561, BLEU-4  1.6555\n",
      "rouge_1/f_score  8.2143\n",
      "rouge_1/r_score  6.4110\n",
      "rouge_1/p_score 15.0156\n",
      "rouge_2/f_score  1.8224\n",
      "rouge_2/r_score  1.4683\n",
      "rouge_2/p_score  3.2528\n",
      "rouge_l/f_score  6.1977\n",
      "rouge_l/r_score  5.9713\n",
      "rouge_l/p_score 14.2932\n",
      "Explanation 任务 (Prompt: C-3) 评价结果已保存至: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log/toys/0509/evaluation_logs/NoAttack_0.0_VIP5_toys_vitb32_8_20_eval_explanation_C-3.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading checkpoint from:\", args.load)\n",
    "\n",
    "# =============================================================================\n",
    "# Cell 10: Evaluation - Explanation 任务（带 Prompt 信息）\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# 如果 args.load 不为空，则从其中提取日期，否则使用当前日期\n",
    "if args.load is not None:\n",
    "    eval_date = Path(args.load).parents[1].name\n",
    "else:\n",
    "    eval_date = datetime.now().strftime(\"%m%d\")\n",
    "\n",
    "# 指定 Explanation 任务的 prompt 及样本数量\n",
    "exp_prompt = 'C-3'  # 可修改为 'C-12', 'C-3' 等所需的 prompt 编号\n",
    "test_task_list = {'explanation': [exp_prompt]}\n",
    "test_sample_numbers = {'sequential': (1, 1), 'direct': (1, 1), 'explanation': 1}\n",
    "\n",
    "# 获取 Explanation 任务的测试数据加载器\n",
    "zeroshot_test_loader = get_loader(\n",
    "    args,\n",
    "    test_task_list,\n",
    "    test_sample_numbers,\n",
    "    split=args.test, \n",
    "    mode='test', \n",
    "    batch_size=args.batch_size,\n",
    "    workers=args.num_workers,\n",
    "    distributed=args.distributed,\n",
    "    data_root=\"data\",\n",
    "    feature_root=\"features\"\n",
    ")\n",
    "print(f\"Explanation 任务 (Prompt: {exp_prompt}) 数据量:\", len(zeroshot_test_loader))\n",
    "\n",
    "tokens_predict = []\n",
    "tokens_test = []\n",
    "\n",
    "# 遍历测试数据加载器，调用模型生成预测结果\n",
    "for _, batch in tqdm(enumerate(zeroshot_test_loader), total=len(zeroshot_test_loader), ncols=100):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        tokens_predict.extend(results)\n",
    "        tokens_test.extend(batch['target_text'])\n",
    "\n",
    "# 计算 BLEU 与 ROUGE 指标\n",
    "BLEU1 = bleu_score(tokens_test, tokens_predict, n_gram=1, smooth=False)\n",
    "BLEU4 = bleu_score(tokens_test, tokens_predict, n_gram=4, smooth=False)\n",
    "ROUGE = rouge_score(tokens_test, tokens_predict)\n",
    "\n",
    "print(f'BLEU-1 {BLEU1:7.4f}, BLEU-4 {BLEU4:7.4f}')\n",
    "for k, v in ROUGE.items():\n",
    "    print(f'{k} {v:7.4f}')\n",
    "\n",
    "# 构建保存评估结果的目录和文件名\n",
    "eval_dir = Path(\"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log\") \\\n",
    "           / args.split / eval_date / \"evaluation_logs\"\n",
    "eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 保持与训练时相同的 base_name\n",
    "suffix = args.attack_mode\n",
    "mr = args.mr\n",
    "dataset = args.split\n",
    "img_feat = args.image_feature_type\n",
    "reduction = args.reduction_factor\n",
    "epoch = args.epoch\n",
    "base_name = f\"{suffix}_{mr}_VIP5_{dataset}_{img_feat}_{reduction}_{epoch}\"\n",
    "\n",
    "explanation_filename = f\"{base_name}_eval_explanation_{exp_prompt}.txt\"\n",
    "explanation_log_path = eval_dir / explanation_filename\n",
    "\n",
    "# 保存评估结果，文件头加入 Dataset、AttackMode 和 MaliciousRatio\n",
    "with open(explanation_log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Explanation Evaluation Results\\n\")\n",
    "    f.write(f\"Dataset: {dataset}\\n\")\n",
    "    f.write(f\"AttackMode: {args.attack_mode}\\n\")\n",
    "    f.write(f\"MaliciousRatio: {args.mr}\\n\")\n",
    "    f.write(f\"Prompt: {exp_prompt}\\n\\n\")\n",
    "    f.write(f\"BLEU-1: {BLEU1:7.4f}\\n\")\n",
    "    f.write(f\"BLEU-4: {BLEU4:7.4f}\\n\")\n",
    "    for k, v in ROUGE.items():\n",
    "        f.write(f\"{k}: {v:7.4f}\\n\")\n",
    "\n",
    "print(f\"Explanation 任务 (Prompt: {exp_prompt}) 评价结果已保存至: {explanation_log_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 11: Evaluation - Direct 任务\n",
    "\n",
    "功能说明：\n",
    "加载 direct 任务的测试数据，生成输出并计算评价指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] exp_splits_path = data/toys/exp_splits.pkl\n",
      "[DEBUG] seq_path        = data/toys/sequential_data.txt\n",
      "[DEBUG] idx_path        = data/toys/user_id2idx.pkl\n",
      "[DEBUG] name_path       = data/toys/user_id2name.pkl\n",
      "[WARN] NoAttack 模式下，动态构建了 26243 个用户映射\n",
      "compute_datum_info\n",
      "攻击模式：NoAttack，恶意比例：0.0\n",
      "Direct 任务 (Prompt: B-5) 数据量: 1214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 840/1214 [12:01<04:57,  1.26it/s]ERROR: Unexpected segmentation fault encountered in worker.\n",
      " 69%|██████▉   | 840/1214 [12:01<05:21,  1.16it/s]\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x73f07fe36430>\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1442, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/selectors.py\", line 426, in select\n",
      "    key = self._key_from_fd(fd)\n",
      "  File \"/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/selectors.py\", line 286, in _key_from_fd\n",
      "    return self._fd_to_key[fd]\n",
      "  File \"/scratch/guanguowei/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 1522205) is killed by signal: Segmentation fault. \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     49\u001b[0m     results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate_step(batch)\n\u001b[0;32m---> 50\u001b[0m     beam_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhole_word_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhole_word_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategory_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvis_feats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvis_feats\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     generated_sents \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(beam_outputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# for j, (_, tgt_text, _) in enumerate(zip(results, batch['target_text'], batch['source_text'])):\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m#     all_info.append({\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m#         'target_item': tgt_text,\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m#         'gen_item_list': generated_sents[j * 20: (j + 1) * 20]\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m#     })\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/transformers/generation/utils.py:1252\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1244\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1245\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1246\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration results, please set `padding_side=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` when initializing the tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1247\u001b[0m         )\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;66;03m# and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1252\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/transformers/generation/utils.py:617\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[1;32m    615\u001b[0m encoder_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    616\u001b[0m encoder_kwargs[model_input_name] \u001b[38;5;241m=\u001b[39m inputs_tensor\n\u001b[0;32m--> 617\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]: ModelOutput \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoder_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/MyWork/VIP5_Shadowcast_DPA/notebooks/modeling_vip5.py:873\u001b[0m, in \u001b[0;36mJointEncoder.forward\u001b[0;34m(self, input_ids, whole_word_ids, category_ids, vis_feats, attention_mask, inputs_embeds, head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, task)\u001b[0m\n\u001b[1;32m    870\u001b[0m position_bias \u001b[38;5;241m=\u001b[39m position_bias \u001b[38;5;241m+\u001b[39m extended_attention_mask\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (layer_module, past_key_value) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock, past_key_values)):\n\u001b[0;32m--> 873\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)\u001b[39;00m\n\u001b[1;32m    889\u001b[0m     hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m layer_outputs[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vip5_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/MyWork/VIP5_Shadowcast_DPA/notebooks/modeling_vip5.py:524\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, head_mask, past_key_value, use_cache, output_attentions, return_dict, task)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;66;03m# Apply Feed Forward layer\u001b[39;00m\n\u001b[1;32m    523\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m](hidden_states, task)\n\u001b[0;32m--> 524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(hidden_states)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    525\u001b[0m     clamp_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(hidden_states\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmax \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m    526\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(hidden_states, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mclamp_value, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mclamp_value)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 11: Evaluation - Direct 任务（带 Prompt 信息）\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "# 1. 确定 eval_date：若从 checkpoint 恢复则取目录名，否则用当前日期\n",
    "if args.load is not None:\n",
    "    eval_date = Path(args.load).parents[1].name\n",
    "else:\n",
    "    eval_date = datetime.now().strftime(\"%m%d\")\n",
    "\n",
    "# 2. 指定 Direct 任务的 Prompt\n",
    "test_task_list = {'direct': ['B-5']}  # 可选 'B-5' 或 'B-8'\n",
    "prompt = test_task_list['direct'][0]\n",
    "\n",
    "test_sample_numbers = {\n",
    "    'sequential': (1, 1),\n",
    "    'direct': (1, 1),\n",
    "    'explanation': 1\n",
    "}\n",
    "\n",
    "# 3. 获取 Direct 测试 Loader\n",
    "zeroshot_test_loader = get_loader(\n",
    "    args,\n",
    "    test_task_list,\n",
    "    test_sample_numbers,\n",
    "    split=args.test,\n",
    "    mode='test',\n",
    "    batch_size=args.batch_size,\n",
    "    workers=args.num_workers,\n",
    "    distributed=args.distributed,\n",
    "    data_root=\"data\",\n",
    "    feature_root=\"features\"\n",
    ")\n",
    "\n",
    "print(f\"攻击模式：{args.attack_mode}，恶意比例：{args.mr}\")\n",
    "print(f\"Direct 任务 (Prompt: {prompt}) 数据量:\", len(zeroshot_test_loader))\n",
    "\n",
    "# 4. 收集所有样本的 GT 与模型输出\n",
    "all_info = []\n",
    "for _, batch in tqdm(enumerate(zeroshot_test_loader), total=len(zeroshot_test_loader)):\n",
    "    with torch.no_grad():\n",
    "        results = model.generate_step(batch)\n",
    "        beam_outputs = model.generate(\n",
    "            input_ids=batch['input_ids'].cuda(),\n",
    "            whole_word_ids=batch['whole_word_ids'].cuda(),\n",
    "            category_ids=batch['category_ids'].cuda(),\n",
    "            vis_feats=batch['vis_feats'].cuda(),\n",
    "            task=batch[\"task\"][0],\n",
    "            max_length=50,\n",
    "            num_beams=20,\n",
    "            no_repeat_ngram_size=0,\n",
    "            num_return_sequences=20,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        generated_sents = model.tokenizer.batch_decode(beam_outputs, skip_special_tokens=True)\n",
    "\n",
    "        # for j, (_, tgt_text, _) in enumerate(zip(results, batch['target_text'], batch['source_text'])):\n",
    "        #     all_info.append({\n",
    "        #         'target_item': tgt_text,\n",
    "        #         'gen_item_list': generated_sents[j * 20: (j + 1) * 20]\n",
    "        #     })\n",
    "        for j, item in enumerate(zip(results, batch['target_text'], batch['source_text'])):\n",
    "            new_info = {}\n",
    "            new_info['target_item'] = item[1]\n",
    "            new_info['gen_item_list'] = generated_sents[j*20: (j+1)*20]\n",
    "            all_info.append(new_info)\n",
    "\n",
    "# 5. 构造 GT 与评分字典\n",
    "gt = {}\n",
    "ui_scores = {}\n",
    "for i, info in enumerate(all_info):\n",
    "    gt[i] = [int(info['target_item'])]\n",
    "    pred_dict = {}\n",
    "    for j in range(len(info['gen_item_list'])):\n",
    "        try:\n",
    "            pred_dict[int(info['gen_item_list'][j])] = -(j+1)\n",
    "        except:\n",
    "            pass\n",
    "    ui_scores[i] = pred_dict\n",
    "\n",
    "# 6. 定义用于 ER@K 的目标集合\n",
    "targeted_items = args.data_target[args.split] \n",
    "\n",
    "# 7. 计算指标 + ER@K\n",
    "msg1, res1 = evaluate_all(ui_scores, gt, targeted_items, 1)\n",
    "msg5, res5 = evaluate_all(ui_scores, gt, targeted_items, 5)\n",
    "msg10, res10 = evaluate_all(ui_scores, gt,targeted_items, 10)\n",
    "\n",
    "\n",
    "# print(\"\\nMetrics @1:\")\n",
    "# print(msg1)\n",
    "# # print(f\"ER@1: {res1['er']:.4f}\")\n",
    "# print(\"\\nMetrics @5:\")\n",
    "# print(msg5)\n",
    "# # print(f\"ER@5: {res5['er']:.4f}\")\n",
    "# print(\"\\nMetrics @10:\")\n",
    "# print(msg10)\n",
    "# # print(f\"ER@10: {res10['er']:.4f}\")\n",
    "\n",
    "# 8. 保存结果目录\n",
    "eval_dir = Path(\"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log\") \\\n",
    "           / args.split / eval_date / \"evaluation_logs\"\n",
    "eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 9. 生成与训练时一致的 base_name，并组装文件名（改为 eval_direct）\n",
    "suffix = args.attack_mode          # e.g. DirectBoostingAttack\n",
    "mr = args.mr                       # e.g. 0.1\n",
    "dataset = args.split               # e.g. toys\n",
    "img_feat = args.image_feature_type # e.g. t5-small\n",
    "reduction = args.reduction_factor  # e.g. 8\n",
    "epoch = args.epoch                 # e.g. 20\n",
    "base_name = f\"{suffix}_{mr}_VIP5_{dataset}_{img_feat}_{reduction}_{epoch}\"\n",
    "\n",
    "direct_filename = f\"{base_name}_eval_direct_{prompt}.txt\"\n",
    "direct_log_path = eval_dir / direct_filename\n",
    "\n",
    "# 10. 写入文件，开头加入 Direct Evaluation Results 与 Dataset\n",
    "with open(direct_log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Direct Evaluation Results\\n\")\n",
    "    f.write(f\"Dataset: {dataset}\\n\\n\")\n",
    "    f.write(f\"Prompt: {prompt}\\n\")\n",
    "    f.write(f\"AttackMode: {args.attack_mode}\\n\")\n",
    "    f.write(f\"MaliciousRatio: {args.mr}\\n\\n\")\n",
    "    f.write(\"=== Metrics @1 ===\\n\")\n",
    "    f.write(msg1 + \"\\n\")\n",
    "    #f.write(f\"ER@1: {res1['er']:.4f}\\n\\n\")\n",
    "    f.write(\"=== Metrics @5 ===\\n\")\n",
    "    f.write(msg5 + \"\\n\")\n",
    "    #f.write(f\"ER@5: {res5['er']:.4f}\\n\\n\")\n",
    "    f.write(\"=== Metrics @10 ===\\n\")\n",
    "    f.write(msg10 + \"\\n\")\n",
    "    #f.write(f\"ER@10: {res10['er']:.4f}\\n\")\n",
    "\n",
    "print(f\"Direct 结果已保存至: {direct_log_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 12: Evaluation - Sequential 任务\n",
    "\n",
    "功能说明：\n",
    "加载 sequential 任务的测试数据，生成输出并计算评价指标，同时对 beam search 结果进行解码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'P5Tokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] exp_splits_path = data/toys/exp_splits.pkl\n",
      "[DEBUG] seq_path        = data/toys/sequential_data.txt\n",
      "[DEBUG] idx_path        = data/toys/user_id2idx.pkl\n",
      "[DEBUG] name_path       = data/toys/user_id2name.pkl\n",
      "[WARN] NoAttack 模式下，动态构建了 26243 个用户映射\n",
      "compute_datum_info\n",
      "攻击模式：NoAttack，恶意比例：0.0\n",
      "Sequential 任务 (Prompt: A-9) 数据量: 1214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 1214/1214 [13:12<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NDCG@1\tRec@1\tHits@1\tPrec@1\tMAP@1\tMRR@1\tER@1\n",
      "0.0718\t0.0718\t0.0718\t0.0718\t0.0718\t0.0718\t0.0000\n",
      "\n",
      "NDCG@5\tRec@5\tHits@5\tPrec@5\tMAP@5\tMRR@5\tER@5\n",
      "0.0903\t0.1070\t0.1070\t0.0214\t0.0847\t0.0847\t0.0000\n",
      "\n",
      "NDCG@10\tRec@10\tHits@10\tPrec@10\tMAP@10\tMRR@10\tER@10\n",
      "0.0960\t0.1247\t0.1247\t0.0125\t0.0871\t0.0871\t0.0002\n",
      "Sequential 结果已保存至: /scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log/toys/0509/evaluation_logs/NoAttack_0.0_VIP5_toys_vitb32_8_20_eval_sequential_A-9.txt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 12: Evaluation - Sequential 任务（带 Prompt 信息）\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# 如果 args.load 不为空，则从 load 路径中提取日期，否则使用当前日期\n",
    "if args.load is not None:\n",
    "    eval_date = Path(args.load).parents[1].name\n",
    "else:\n",
    "    eval_date = datetime.now().strftime(\"%m%d\")\n",
    "\n",
    "# 指定 Sequential 任务的 prompt 及样本数量\n",
    "test_task_list = {'sequential': ['A-9']} # A-3 or A-9\n",
    "prompt = test_task_list['sequential'][0]\n",
    "test_sample_numbers = {'sequential': (1, 1), 'direct': (1, 1), 'explanation': 1}\n",
    "\n",
    "# 获取 Sequential 任务的测试数据加载器\n",
    "zeroshot_test_loader = get_loader(\n",
    "    args,\n",
    "    test_task_list,\n",
    "    test_sample_numbers,\n",
    "    split=args.test,\n",
    "    mode='test',\n",
    "    batch_size=args.batch_size,\n",
    "    workers=args.num_workers,\n",
    "    distributed=args.distributed,\n",
    "    data_root=\"data\",\n",
    "    feature_root=\"features\"\n",
    ")\n",
    "\n",
    "print(f\"攻击模式：{args.attack_mode}，恶意比例：{args.mr}\")\n",
    "print(f\"Sequential 任务 (Prompt: {prompt}) 数据量:\", len(zeroshot_test_loader))\n",
    "\n",
    "# 生成候选并收集结果\n",
    "all_info = []\n",
    "for _, batch in tqdm(enumerate(zeroshot_test_loader), total=len(zeroshot_test_loader), ncols=100):\n",
    "    with torch.no_grad():\n",
    "        # 单次生成\n",
    "        results = model.generate_step(batch)\n",
    "        # Beam search 多样本生成\n",
    "        beam_outputs = model.generate(\n",
    "            input_ids=batch['input_ids'].cuda(),\n",
    "            whole_word_ids=batch['whole_word_ids'].cuda(),\n",
    "            category_ids=batch['category_ids'].cuda(),\n",
    "            vis_feats=batch['vis_feats'].cuda(),\n",
    "            task=batch[\"task\"][0],\n",
    "            max_length=50,\n",
    "            num_beams=20,\n",
    "            no_repeat_ngram_size=0,\n",
    "            num_return_sequences=20,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        generated_sents = model.tokenizer.batch_decode(beam_outputs, skip_special_tokens=True)\n",
    "\n",
    "        for j, item in enumerate(zip(results, batch['target_text'], batch['source_text'])):\n",
    "            new_info = {}\n",
    "            new_info['target_item'] = item[1]\n",
    "            new_info['gen_item_list'] = generated_sents[j*20: (j+1)*20]\n",
    "            all_info.append(new_info)\n",
    "\n",
    "# 构造 GT 与评分字典\n",
    "gt = {}\n",
    "ui_scores = {}\n",
    "for i, info in enumerate(all_info):\n",
    "    gt[i] = [int(info['target_item'])]\n",
    "    pred_dict = {}\n",
    "    for j in range(len(info['gen_item_list'])):\n",
    "        try:\n",
    "            pred_dict[int(info['gen_item_list'][j])] = -(j+1)\n",
    "        except:\n",
    "            pass\n",
    "    ui_scores[i] = pred_dict\n",
    "\n",
    "# 定义目标集合 & 计算指标\n",
    "targeted_items = args.data_target[args.split]  # 目标物品列表\n",
    "\n",
    "msg1, res1 = evaluate_all(ui_scores, gt, targeted_items, 1)\n",
    "msg5, res5 = evaluate_all(ui_scores, gt, targeted_items, 5)\n",
    "msg10, res10 = evaluate_all(ui_scores, gt, targeted_items, 10)\n",
    "\n",
    "# print(\"\\nMetrics @1:\", msg1, f\"ER@1: {res1['er']:.4f}\")\n",
    "# print(\"Metrics @5:\", msg5, f\"ER@5: {res5['er']:.4f}\")\n",
    "# print(\"Metrics @10:\", msg10, f\"ER@10: {res10['er']:.4f}\")\n",
    "\n",
    "# 构建保存目录\n",
    "eval_dir = Path(\"/scratch/guanguowei/Code/MyWork/VIP5_Shadowcast_DPA/log\") \\\n",
    "           / args.split / eval_date / \"evaluation_logs\"\n",
    "eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 文件名保持与训练一致的前缀，然后替换为 eval_sequential\n",
    "suffix = args.attack_mode\n",
    "mr = args.mr\n",
    "dataset = args.split\n",
    "img_feat = args.image_feature_type\n",
    "reduction = args.reduction_factor\n",
    "epoch = args.epoch\n",
    "base_name = f\"{suffix}_{mr}_VIP5_{dataset}_{img_feat}_{reduction}_{epoch}\"\n",
    "\n",
    "sequential_filename = f\"{base_name}_eval_sequential_{prompt}.txt\"\n",
    "sequential_log_path = eval_dir / sequential_filename\n",
    "\n",
    "# 写入文件，开头加上 Dataset、AttackMode、MaliciousRatio 和 Prompt\n",
    "with open(sequential_log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Sequential Evaluation Results\\n\")\n",
    "    f.write(f\"Dataset: {dataset}\\n\")\n",
    "    f.write(f\"Prompt: {prompt}\\n\")\n",
    "    f.write(f\"AttackMode: {args.attack_mode}\\n\")\n",
    "    f.write(f\"MaliciousRatio: {args.mr}\\n\")\n",
    "    f.write(f\"Prompt: {prompt}\\n\\n\")\n",
    "    f.write(\"=== Metrics @1 ===\\n\")\n",
    "    f.write(msg1 + \"\\n\")\n",
    "    #f.write(f\"ER@1: {res1['er']:.4f}\\n\\n\")\n",
    "    f.write(\"=== Metrics @5 ===\\n\")\n",
    "    f.write(msg5 + \"\\n\")\n",
    "    #f.write(f\"ER@5: {res5['er']:.4f}\\n\\n\")\n",
    "    f.write(\"=== Metrics @10 ===\\n\")\n",
    "    f.write(msg10 + \"\\n\")\n",
    "    #f.write(f\"ER@10: {res10['er']:.4f}\\n\")\n",
    "\n",
    "print(f\"Sequential 结果已保存至: {sequential_log_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vip5_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
